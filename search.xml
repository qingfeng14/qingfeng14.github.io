<?xml version="1.0" encoding="utf-8"?>
<search>
  
    
    <entry>
      <title><![CDATA[Golang 性能测试, 单元测试]]></title>
      <url>%2Farticle%2Fgolang%2Fgolang%E6%80%A7%E8%83%BD%26%E5%8D%95%E5%85%83%E6%B5%8B%E8%AF%95.html</url>
      <content type="text"><![CDATA[Golang 测试总结 go test的初始化以及退出前的clear — TestMain 程序的初始化工作可以通过init()函数完成,Go的init有以下特点: 先于main函数执行 自动调用 导入其他包时先执行import包的init函数,多次导入时只导入一次 同一个包可以有多个init 测试程序中有时候可能需要完成一些setup 或者 teardown操作, go 1.4 之后加入了TestMain方法帮助我们完成这个任务. 当包含TestMain函数时, go test会执行该函数而不是直接运行测试. It is sometimes necessary for a test program to do extra setup or teardown before or after testing. It is also sometimes necessary for a test to control which code runs on the main thread. To support these and other cases, if a test file contains a function: 1func TestMain(m *testing.M) then the generated test will call TestMain(m) instead of running the tests directly. TestMain runs in the main goroutine and can do whatever setup and teardown is necessary around a call to m.Run. It should then call os.Exit with the result of m.Run. When TestMain is called, flag.Parse has not been run. If TestMain depends on command-line flags, including those of the testing package, it should call flag.Parse explicitly. A simple implementation of TestMain is: 123456func TestMain(m *testing.M) &#123; setup() code := m.Run() shutdown() os.Exit(code)&#125; Go Coverage test 覆盖率测试 12PKGS=`go list github.com/xxx/xxx/... | grep -v /vendor`go test -cover -p 1 -timeout=20m $PKGS 123456# 测试所有包的覆盖率go test -cover -timeout=20m -coverpkg=tracing/... tracing/tests # -coverprofile=c.out# 测试特定包的覆盖率go test -cover -timeout=20m -coverpkg=tracing/util tracing/tests -coverprofile=c.out# 查看覆盖率报告go tool cover -html=c.out -o=tag.html Reference TestMain—What is it Good For?]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Hexo配置问题-Pandoc]]></title>
      <url>%2Farticle%2FHexo%E9%85%8D%E7%BD%AE%E9%97%AE%E9%A2%98-Pandoc.html</url>
      <content type="text"><![CDATA[Pandoc 的问题 Error: spawn pandoc ENOENT 12345678910events.js:183 throw er; // Unhandled 'error' event ^Error: spawn pandoc ENOENT at _errnoException (util.js:1022:11) at Process.ChildProcess._handle.onexit (internal/child_process.js:190:19) at onErrorNT (internal/child_process.js:372:16) at _combinedTickCallback (internal/process/next_tick.js:138:11) at process._tickCallback (internal/process/next_tick.js:180:9) 解决方案: 应该是未找到pandoc的原因,解决方案是设置相关PATH, 或者安装pandoc Error: pandoc exited with code 2: –smart/-S has been removed. Use +smart or -smart extension instead. 123456789Error: pandoc exited with code 2: --smart/-S has been removed. Use +smart or -smart extension instead.For example: pandoc -f markdown+smart -t markdown-smart.Try pandoc --help for more information. at ChildProcess.&lt;anonymous&gt; (/home/arxan/go_project/src/blog_hexo/node_modules/hexo-renderer-pandoc/index.js:73:20) at emitTwo (events.js:126:13) at ChildProcess.emit (events.js:214:7) at maybeClose (internal/child_process.js:925:16) at Process.ChildProcess._handle.onexit (internal/child_process.js:209:5) Pandoc 2.0 移除了 –smart/-S 参数, 需要修改为 pandoc -f markdown+smart -t markdown-smart. 或者重新安装1.x版本的pandoc 修改hexo-renderer-pandoc@0.1.1参数的方法为: 进入node_modules/hexo-renderer-pandoc/index.js line43, change 1var args = [ '-f', 'markdown', '-t', 'html', math, '--smart'] to 1var args = [ '-f', 'markdown+smart', '-t', 'html', math]]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[linux or Mac 命令行获取本机IP地址]]></title>
      <url>%2Farticle%2Flinux-Mac%E5%91%BD%E4%BB%A4%E8%A1%8C%E8%8E%B7%E5%8F%96%E6%9C%AC%E6%9C%BAIP%E5%9C%B0%E5%9D%80.html</url>
      <content type="text"><![CDATA[Linux/Mac 获取本机的IP地址 1ifconfig | grep -A 1 'eth0' | tail -n 1 | awk '&#123;print $2&#125;' 其中eth0需要替换为对应的网卡. Linux/Mac 获取本机的公网IP 通过访问公网IP检测的网站即可. 12curl ifconfig.mecurl httpbin.org/ip]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Centrally Banked Cryptocurrencies 书面翻译]]></title>
      <url>%2Farticle%2FRSCoin%E8%AE%BA%E6%96%87%E7%BF%BB%E8%AF%91.html</url>
      <content type="text"><![CDATA[摘要：从比特币开始的现代数字加密货币，构建了一个基于去中心化的基于区块链的交易分类账，通过工作量证明机制维护交易账和货币发行。这样的去中心化机制有好处，比如独立于国家的政治控制，然而也有计算开销和可扩展性的巨大限制。为此，我们引入了一种现代数字加密货币框架——RSCoin，在这个框架中，央行完全控制货币的发行，但是依赖于一系列分散式权威节点(mintettes)来防止双花。尽管货币的政策是中央集权的，RSCoin仍然提供了高透明性和可审计性保证。我们从理论和实验上证明了这种中等程度的中心化的好处，比如消除不必要的hash以及可扩展的用于避免双花攻击的系统。 一. 引言 2009年推出的比特币，以及许多由此启发的类似的加密货币(如:莱特币和瑞波币)，已经取得了巨大的成功：财务上，2015年11月，比特币的市值达到48亿美元，30个加密货币市值超过100万美元。在知名度上，比特币以及被越来越多的人接受为一种支付方式了，比如有15万商人使用CoinBase或者Bitpay 作为支付网关提供商。 最近，摩根大通和纳斯达克等全球主要金融机构已经宣布了区块链技术的开发计划。政府机构现在也意识到了加密货币的潜在影响力：欧洲央行认为加密货币影响货币政策和价格稳定；美联储关注数字货币提供“更快、更安全、更高效的支付系统”；英国财政部发誓在这个领域支持创新。这并不令人惊讶，因为当前央行使用的财务结算系统（如CHAPS, TARGET2, 和 Fedwire）仍然相对比较昂贵，并且至少在后台，延迟较高并且在创新方面停滞不前。 尽管现有的加密货币取得了一些成功，他们也有许多局限性。可以说最大的问题是低扩展性：比特币（qazwsx2目前为止使用量对多的）每秒最多处理7笔交易，并在提高这一速度上面临着重大的挑战，然而PayPal每秒可以处理100笔以上交易，Visa可以每秒平均处理2000至7000笔交易。缺乏可扩展性是因为其依赖于广播和需要大量的计算力]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[计算机网络管理-课程设计选题辅导]]></title>
      <url>%2Farticle%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E7%AE%A1%E7%90%86-%E8%AF%BE%E7%A8%8B%E8%AE%BE%E8%AE%A1%E9%80%89%E9%A2%98%E8%BE%85%E5%AF%BC.html</url>
      <content type="text"><![CDATA[课程选题辅导 IPV4 地址空间测绘 多尺度，多维度，多状态，多视图 希尔伯特曲线， IPv4 地址 网络空间是几维的 可做的工作： 测量，细化工作（区域，服务器等等），某个城市，某个省份，whois，等等 设计可视化的方法，网络空间地图的绘制方法，反应更多的信息 网络空间万物分类 网络空间万物： 网络空间中，使用网络手段能够探测和感知的实体 感知到的是一个链接网络的设备（ip，端口） 网络空间方法：ping，trace，Telnet 网络空间图谱 虚实：有的对应实体，有的纯软件 明暗：有的开放，有的内网 ISP:互联网提供服务商 ICP:互联网内容提供商 政治经济社会文化生态 异构网络测量节点管理 网络测量需要协同 全球互联网的拓扑结构：准确性—-多点探测 测量数据需要融合：internet measurement 寻址测量资源：looking-glass，planetLab， 测量木马 搜索和拼接测量数据 基于位置的路由 路由表 互联网ip地址分配碎片化 网络工程：网络是怎么建成的，运转的 体系结构 拓扑和路由 可靠性，性能，安全，可管理 技术选型’ 金盾工程 全球互联网：peer, c/s AS 路由劫持 OSPF,BGP]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Massive-Data-Process-Lab3-K-means-Clustering-of-Netflix-Data]]></title>
      <url>%2Farticle%2FMassive-Data-Process-Lab3-K-means-Clustering-of-Netflix-Data.html</url>
      <content type="text"><![CDATA[实验目标 使用K-means聚类方法对Netflix电影进行聚类.输入电影集合以及每部电影的用户评价列表，输出400个相关电影集合. 实验步骤 数据预处理，将电影评论信息进行整合，一个电影输出一行 movieid, rating list Canopy Selection Mark Data set by Canopy K-means Iteration Viewer 具体实现 Data Prep 原始数据是一个Movie的信息一个文件，为了处理，需要将其转换为一个Movie 一行的形式 Map阶段： 输入文件是大量小文件，因此直接采用WholeFileInputFormat， 即Map的输入value为整个文件的内容，从中解析出MovieID, 以及对应的用户评论列表即可，注意这里需要对用户评论列表按照userID进行排序，以提高后面几步计算距离时的效率 1234567891011String content = new String(value.getBytes(),0, value.getLength());String [] lines = content.split("\n");Text movie = new Text(lines[0].replace(":", ""));List&lt;RatingPair&gt; user_list = new ArrayList&lt;&gt;();StringBuffer rating_list = new StringBuffer();for(int i = 1; i &lt; lines.length; ++i) &#123; String []info = lines[i].split(","); user_list.add(new RatingPair(info[0], info[1]));&#125;// 按用户id排序Collections.sort(user_list); 不需要reduce函数。 Canopy Selection 在这个步骤中，使用Simple Distance 来选择Canopy Centers, 该距离函数是两个电影的相同用户评论数.由于在第一步中已经对用户评论列表进行了按用户ID排序，因此只需要按顺序比对userID，找到超过阈值之后即可返回对应的结果。 具体实现是MAP阶段，各个分节点先处理本地的数据，从中找出Canopy Centers,然后各个节点的Canopy Centers 传入Reduce，reduce从中用同样的算法计算出Canopy Centers,因此这里需要设置reduce num为1 SimpleDistance: 1234567891011121314151617181920212223public static int distance(Canopy o1, Canopy o2, int threshold) &#123; int count = 0; for(int i = 0, j = 0; i &lt; o1.rating.size() &amp;&amp; j &lt; o2.rating.size();) &#123; if(o1.rating.get(i).userid &lt; o2.rating.get(j).userid) &#123; ++i; &#125; else if(o2.rating.get(j).userid.equals(o1.rating.get(i).userid)) &#123; if(++count &gt; threshold) &#123; // LOG.warn( o1.id + " " + o2.id + " " + count); return count; &#125; ++i; ++j; &#125; else &#123; ++j; &#125; &#125; // LOG.warn( o1.id + " " + o2.id + " " + count); return count;&#125; Canopy Select算法： 从point list中随机选取一个点p ，计算该点与当前所有的Canopy Center的距离，如果p与某个Center相同用户数大于阈值8（距离小于T1），则该点属于该Canopy， 如果p与所有的Canopy Center点的相同用户数都小于8，则该p置为一个新的Canopy Center，加入CanopyList. Canopy Selection具体实现： 123456789101112131415for(Text value: values) &#123; count += 1; Canopy curr = new Canopy(value.toString()); boolean newCanopy = true; for(Canopy p: this.canopyList) &#123; if(Canopy.inCanopy(p, curr, threshold)) &#123; newCanopy = false; break; &#125; &#125; if(newCanopy) &#123; this.canopyList.add(curr); context.write(new Text(curr.id), new Text(curr.rating.toString())); &#125;&#125; Mark Data Set by Canopy 这一步需要对每条数据标记其属于的Canopy，在MAP阶段，首先需要load 第二步中得到的Canopy Center的信息，然后对所有的电影数据，计算其与各个Canopy center的距离，若相同用户排列数超过2个，则该条电影数据属于该Canopy Center，最终输出(movieid, rating_list + canopy_list) 即可。 不需要reduce函数。 1234567891011// 本CanopyCanopy canopy = new Canopy(value.toString());for(int i = 0; i &lt; canopies.size(); ++i) &#123; // 遍历中心点 if(Canopy.distance(canopy, canopies.get(i), 2) &gt; 2) &#123; // 在该Canopy中 canopyList.add(canopies.get(i).id); &#125;&#125;context.write(new Text(canopy.id), new Text(canopy.rating.toString() + "\t" + canopyList.toString())); K-means Iteration Map Step map 阶段的输入为每个Movie的movieid, rating_list, canopy_list, 需要在setup阶段先将第二步中选出的Canopy Center点的数据load入内存，在map函数中每接收一个Movie数据，就计算其与所属的Canopy的距离，并选择距离最近的作为其新的中心maxid，输出为（maxid, movieinfo）,此处的距离是余弦距离，其计算方式为\[similarity = \frac{A*B}{\|A\|*\|B\|} = \frac{\sum_{i = 1}^{n}Arate(user(i))*Brate(user(i)))}{\|A\|*\|B\|}\] 出于计算效率考虑，这里使用余弦相似度的平方作为其距离，数值越大，point越接近。 Reduce Step reduce 阶段需要计算新的Canopy Center， 输入为（Canopyid, pointlist）, 首先遍历属于CanopyID这个集合的所有电影的评论数据，统计每个用户的评论数以及评论总分，然后根据用户的评论数进行排序，选择评论数前1000个用户，计算其平均评分，这些ratinglist作为新的Canopy id对应的数据。输出(canopyID, rating_list) 123456789101112131415161718192021222324252627282930313233343536// 将map.entrySet()转换成list, 按照Value 排序List&lt;Map.Entry&lt;Integer, Integer&gt;&gt; list = new ArrayList&lt;Map.Entry&lt;Integer, Integer&gt;&gt;(userMovieCounts.entrySet());// 通过比较器来实现排序Collections.sort(list, new Comparator&lt;Map.Entry&lt;Integer, Integer&gt;&gt;() &#123; @Override public int compare(Map.Entry&lt;Integer, Integer&gt; o1, Map.Entry&lt;Integer, Integer&gt; o2) &#123; // 降序排序 return -o1.getValue().compareTo(o2.getValue()); &#125;&#125;);// StringBuffer ratinglist = new StringBuffer();int counter = 0;List&lt;DataPrep.RatingPair&gt; ratingList = new ArrayList&lt;&gt;();for (Map.Entry&lt;Integer, Integer&gt; mapping : list) &#123; // System.out.println(mapping.getKey() + ":" + mapping.getValue()); if(counter &gt; 1000) break; // LOG.error(mapping.getKey() + " " + mapping.getValue()); int rateCount = mapping.getValue(); Integer averageRate = ratingSum.get(mapping.getKey()) / rateCount; if(averageRate &gt; 0) &#123; ratingList.add(new DataPrep.RatingPair(mapping.getKey().toString(), averageRate.toString())); // ratinglist.append(mapping.getKey() + "," + averageRate); &#125; counter += 1;&#125; Viewer 输出最终的聚类结果，导入Title文件，map阶段同K-means Iteration的MAP函数，但是在输出时将Movieid 转换为对应的Title，输出最终的聚类结果。 结果示意: 哈利波特：Harry Potter out5New 5轮 1000： （4，3，1） out10Times: 10轮 1000： （4，1，1，2） Star Wars out5News: 4+2+1+1+1 out10Times: (5+2+1+1) Write Up 聚类数： 318 分类结果示例 1210011 &#123;5046=Iron Maiden: Visions of the Beast, 6316=New England Metal Hardcore Festival 2003, 7538=Iron Maiden: The Early Days, 7653=CKY: Infiltrate, 10011=Lamb of God: Killadelphia&#125;1028 &#123;1028=The Educational Archives: Vol. 1: Sex &amp; Drugs, 4376=ABBA: The Definitive Collection, 4549=ABBA: Gold: Greatest Hits, 5987=Gates of Heaven, 10896=Errol Morris&apos; First Person: The Complete Series, 10969=Jail Bait, 12449=Bad Girls Go to Hell/ Another Day Another Man, 12691=Vernon, 14696=ABBA: The Winner Takes It All, 14703=ABBA: Super Troupers&#125; 本次实验实现了Kmeans算法和Canopy算法，通过本次实验，我对Hadoop 的MapReduce编程模式理解更加深入了，也对聚类算法有了更多的认识。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Massive-Data-Process-Lab2-PageRank-on-the-Wikipedia-Corpus]]></title>
      <url>%2Farticle%2FMassive-Data-Process-Lab2-PageRank-on-the-Wikipedia-Corpus.html</url>
      <content type="text"><![CDATA[实验目标 实现PageRank，将Wikipedia转换为链接图，然后对其进行PageRank，运行多次直到结果收敛，返回所有文章的PageRank值 语料库 Wikipedia提供的包含Wikipedia站点所有文章的xml文件，其中包括重定向以及消歧义页面 Links to other wikipedia articles are of the form “[[Name of other article]]”. 实验步骤 graphBuiler: 构建链接关系图 pageRankItr: 迭代计算页面的rank pageRankViewer: 将结果文件转换为text format, 并且按照page rank score排序 扩展 在实验中我们使用了TextInputFormat clas， 考虑是否有其他方式处理xml文档 Implement variably weighted links. Explain your heuristic and convince us that its working. Run PageRank over the Nutch crawl or your InvertedIndexer over Wikipedia and then combine both data sets into a rudimentary search engine. 实现 链接关系图的构建 首先构建链接关系图，切割后的数据一行为一个Page。因此直接采用TextInputFormat即可。 map阶段 一次读入一行数据，使用正则表达式从中解析出title以及outlink, 输出(title, outlink) 12private static final Pattern LINKPATTERN = Pattern.compile("\\[\\[(.+?)\\]\\]");private static final Pattern TITLEPATTERN = Pattern.compile("&lt;title&gt;(.+)&lt;/title&gt;"); 为了后续处理方便，在连接图的构建过程中将空格，标点等字符转换为了'_', outlink的格式多样，&quot;[[outlink|others]]&quot;，这类型的外链只保留’|’前的内容 reduce阶段 同一个title链出的页面聚合在一起，outlink拼接即可，同时为了方便后续PageRank计算，输出时同时把PageRank初始值(1.0)输出,输出(title, &quot;1.0\t&quot; + outlink list); 最终得到的链接关系图约5.2G PageRank计算 Map阶段： 一行一行读入，输入格式为(page, (cur_rank, url_list) 对url_list中的每一个link, 输出中间结果(link, cur_rank/url_list.length) 输出(page, &quot;!&quot;)标记当前页面是Wikipedia页面，供reduce阶段使用 输出原始链接关系，供reduce阶段使用(page, (cur_rank, url_list) Reduce阶段： 输入为(page, (cur_rank, url_list), (page, val), (page, &quot;!&quot;)等多类型数据，其中val是指向该page的页面分给该页面的rank值，(page, “!”)标记了该页面存在于链接关系图中，记录rank时需要记录，否则不记录该页面的rank，这样使得最终的rank列表页面数与连接图页面数一致 将所有val相机，阻尼系数d = 0.85, 迭代计算PageRank值\[PR(u) = (1-d) + d*\sum_{v \in B(u)}PR(v)\]其中\(PR(u)\)表示页面u的PageRank值，v指向页面u, \(B(u)\)是连接到页面u的所有页面集合。 12345678910111213141516171819for (Text value: values) &#123; String str = value.toString(); // LOG.warn(key + " " + str + " " + pagerank); if(str.equals("!")) &#123; exist = true; continue; &#125; String []k_v = str.split("\t"); if(k_v.length &gt; 1) &#123; out_link = k_v[1]; &#125; else &#123; pagerank += Double.parseDouble(k_v[0]); &#125;&#125;// 页面不存在则丢弃if(!exist) return;pagerank = pagerank * d + (1 - d); 输出迭代结果（page, (new_rank, url_list)） 一共迭代10轮 RankSort排序 将结果安装PageRank值排序，map阶段输入（page, (rank, url_list)）， 输出格式为(rank, page) 即以rank值作为key, 利用MapReduce的shuffle阶段完成排序操作，默认排序是顺序排序，因此需要自定义排序规则实现倒序排序 1234567891011 public static class FloatWritableComparator extends FloatWritable.Comparator &#123; @Override public int compare(WritableComparable a, WritableComparable b) &#123; return -super.compare(a, b); &#125; @Override public int compare(byte[] b1, int s1, int l1, byte[] b2, int s2, int l2) &#123; return -super.compare(b1, s1, l1, b2, s2, l2); &#125;&#125; 扩展 Xml文档的处理 法1：XMLInputFormat 以XMLInputFormat形式读入输入文件，其方法是继承自TextInputFormat，然后处理好迭代多行的文本内容逻辑，以tag切分数据片，每切分一个完整的tag内容写入到Buffer中 法2：将XML文件转为SequenceFile SequenceFile是Hadoop API 提供的一种二进制文件，它将数据以的形式序列化到文件中。这种二进制文件内部使用Hadoop 的标准的Writable 接口实现序列化和反序列化。 因此需要实现对xml文件的tag解析以及存储，最终将xml文件转换为SequenceFile 加权PageRank 算法描述 在某一主题范围内，如果有许多网页链向某一网页或者有很多高影响的网页指向该页面，则该页面比较重要，在本加权PageRank算法中，页面的重要度取决于指向该页面的连接数以及该页面链出的页面数，即出入度影响页面的权重。其计算公式如下： \[ PR(u)=(1-d)+d\sum_{v \in B(u)}PR(v)W_{(v,u)}^{in}W_{(v,u)}^{out} \] 其中，\(W(v,u)^{out}\)是指链接(v,u)的出链权重， \(W(v,u)^{in}\)是指链接(v,u)的入链权重，其计算公式如下: \[W_{(v,u)}^{in}=\frac{I_u}{\sum_{p \in R(v)}I_p}\] \[W_{(v,u)}^{out}=\frac{O_u}{\sum_{p \in R(v)}O_p}\] \(I_u,I_p\)分别是网页u,网页p的入链数，\(O_u,O_p\)分别是网页u,网页p的出链数。 实现 采用出入度加权算法之后，计算PageRank时需要将页面的出入度考虑在内，因此在构建连接关系图之后，计算PageRank之前，需要先统计每个页面的出入度。 这里分两步完成 链接关系倒排： MAP阶段：输入(url, url_list), 输出(url, &quot;!\t&quot; + url_list.length)，即输出其出度，否则对u in url_list, 输出(u, url), 即输出倒排链接关系对 123456789101112131415161718// 没有出度if(k_v.length &lt; 3 || k_v[2].length() == 0) &#123; // LOG.warn("map1 " + page + " !\t0"); context.write(new Text(page), new Text("!\t0")); return;&#125;String out_link = k_v[2];out_link = out_link.replaceAll("@", "_");String []all_out_link = out_link.split(",");int out_degree = all_out_link.length;for (String link: all_out_link) &#123; context.write(new Text(link), new Text(page));&#125;context.write(new Text(page), new Text("!\t" + out_degree)); Reduce阶段：统计页面的出入度，(page, &quot;!\t&quot; + num)格式的得出page的outdegree, 对（page, parentPage）得出其链接关系，据此统计其入度即可，最后输出（outlink, (indegree, outdegree, parentPage)）或者(parentPage, (indegree, outdegree, &quot;!&quot;, num)) 统计出入度，还原链接关系图： Map阶段: 输入格式为前一步的reduce输出，根据读入的数据，输出(parent, (&quot;!&quot;, indegree@outdegree))或者(parent, outlink@indegree@outdegree) Reduce阶段：将同key下的链接串起来输出，还原链接关系即可 PageRank计算： 考虑出入度之后，PageRank计算也需要更新，在PageRank的map阶段，输入（URL， (rank, url_list)）, 对 u in url_list, 输出(u, rank/url_list.length * indegree(u)/sum(degree) * outdegree(u)/sum(degree)) 12345678910111213for(int i = 0; i &lt; all_out_link.length; ++i) &#123; String []link_arr = all_out_link[i].split("@"); sum_indegree += Integer.parseInt(link_arr[1]); sum_outdegree += Integer.parseInt(link_arr[2]); in_degree[i] = Integer.parseInt(link_arr[1]); out_degree[i] = Integer.parseInt(link_arr[2]);&#125;for(int i = 0; i &lt; all_out_link.length; ++i) &#123; Double my_rank = avg_rank * ((double)in_degree[i]/sum_indegree) *((double)out_degree[i]/sum_outdegree); context.write(new Text(all_out_link[i]), new Text(my_rank.toString()));&#125; 计算结果 非加权PageRank结果 12345678910111213141516171819209529.62 Wikipedia:Persondata7545.4595 United_States5987.736 Category:Living_people3621.945 Wikipedia:Deletion_review2773.274 Wikipedia:Templates_for_discussion/Log/2010_September_102383.2075 England2267.7446 United_Kingdom2217.742 Race_and_ethnicity_in_the_United_States_Census2140.0762 Canada2091.2627 Template:Orphaned_non-free_revisions1902.5581 Germany1872.2252 India1855.1145 France1819.9362 Animal1607.0189 Help:Reverting1603.3832 Australia1553.8312 World_War_II1449.9705 Japan1348.8551 Category:Unprintworthy_redirects1299.4221 London 出入度加权PageRank结果 12345678910111213141516171819201380.1716 Template:Orphaned_non-free_revisions@30019@7947.3123 Wikipedia:File_Upload_Wizard@8722@43733.5275 Wikipedia:Templates_for_discussion/Log/2010_September_10@41951@163536.2137 United_States@573674@1360415.73993 Wikipedia:FurMe@25192@55412.24493 Protein_Data_Bank@4511@74340.89578 Help:Reverting@48182@69322.3545 Wikipedia:Deletion_review@752655@34280.8142 Wikipedia:High-risk_templates@4194@23205.92093 Category:Fb_team_templates@584@14193.67151 England@207733@1647193.24915 India@132738@1290188.27377 Category:Radio_station_logos@5344@4167.8264 Germany@164873@1189154.31766 Wikipedia:Non-free_content/templates@15982@323153.8296 Category:Film_poster_images@13650@15151.34845 France@180649@1849135.47894 Russia@74494@1848134.8195 Category:Albums_by_artist@13822@54134.0424 Wikipedia:Persondata@897003@78 总结 实现的扩展： MapReduce下XML输入文件的处理 基于出入度的加权PageRank算法 学会了MapReduce下PageRank的计算方法，并进一步掌握了加权PageRank算法，对页面的链接关系、主题相关度等对页面价值评价影响有了更深入的认识 未加权的PageRank值前10中有不少是语言名，这是因为这些页面的入度非常高（几乎每个Wikipedia页面都有语言选择的超链接），而考虑出入度加权之后，前10中非语言类页面则更多了，可以直观感受到出入度加权之后的PageRank值更加合理 参考资料 MapReduce-XML处理-定制InputFormat及定制RecordReader http://blog.csdn.net/doegoo/article/details/50401080 Wadkar, S., &amp; Siddalingaiah, M. (2014). Pro Apache Hadoop. Apress. Chapter 7 Desikan, K. (2015). Weighted page rank algorithm based on in-out weight of webpages. Indian Journal of Science &amp; Technology, 8(34).]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Massive-Data-Process-Homework-4]]></title>
      <url>%2Farticle%2FMassive-Data-Process-Homework-4.html</url>
      <content type="text"><![CDATA[在Dryad论文中，有以下的查询语句，在上课的时候已经详细说明了如何将SQL语句转换成DAG1（被称为是Schema DAG，类似C++里面的class）,之后再将DAG1与具体的数据分布结合在一起，形成DAG2（被称为是Instance DAG，类似C++里面的Object）。课堂中的查询语句是： 12345678910111213select distinct p.objIDfrom photoObjAll pjoin neighbors n — call this join “X”on p.objID = n.objIDand n.objID &lt; n.neighborObjIDand p.mode = 1join photoObjAll l — call this join “Y”on l.objid = n.neighborObjIDand l.mode = 1and abs((p.u-p.g)-(l.u-l.g))&lt;0.05and abs((p.g-p.r)-(l.g-l.r))&lt;0.05and abs((p.r-p.i)-(l.r-l.i))&lt;0.05and abs((p.i-p.z)-(l.i-l.z))&lt;0.05 参考Spark的编程接口，如果需要达到同样的目的，Spark程序如何编写？ 1234567891011var p = spark.textFile(photoObjAll)var n = spark.textFile(neighbors)var x_join = p.join(n) // objID, (p, n)x_join = x_join.filter(p.objID &lt; n.neighborObjID).filter(p.mode == 1)sort(x_join, neighborObjID)x_join = x_join.map(objID =&gt; neighborObjID).reduceByKey(neighborObjID)var l = spark.textFile(photoObjAll)y_join = x_join.join(l) // id, (p, l)y_join.filter(l.mode == 1 &amp;&amp; abs((p.u-p.g)-(l.u-l.g))&lt;0.05 &amp;&amp; abs((p.g-p.r)-(l.g-l.r))&lt;0.05 &amp;&amp; abs((p.r-p.i)-(l.r-l.i))&lt;0.05 &amp;&amp; abs((p.i-p.z)-(l.i-l.z))&lt;0.05) 如果有四台服务器执行上述的程序，绘制其Schema DAG以及Instance DAG。假设数据通过哈希的方法分布到4台服务器上。不需要考虑服务器出错的情况。 Schema DAG Instance DAG]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Lab1:A Simple Inverted Index]]></title>
      <url>%2Farticle%2FMassive-Data-Process-Lab1-A-Simple-Inverted-Index.html</url>
      <content type="text"><![CDATA[实验要求 部分单词出现特别频繁以致于其在倒排表中的是一个噪声：会影响文档的其他更有代表性的单词的特征，在本实验的第一部分，需要对莎士比亚全集文档进行单词统计以找出这样的单词 设计一个Map-Reduce 算法在爬取的网络数据上计算其倒排索引表，最终的倒排索引表中不能包含在第一步中识别出的单词 至少进行两项扩展: 数据清洗：不区分大小写，标点符号不敏感等等 在倒排索引文件的基础上实现一个查询程序，接收用户指定的单词或短语，返回包含这些词语的文档的id 实现一个完全倒排索引，记录单词所在文档id及在文档中的位置 实现 Part 1: WordCount 统计高频噪声词 直接使用Hadoop的示例WordCount代码运行即可（去除标点、大小写影响），最终单词共24418个，然后使用python对单词词频分布情况进行统计，统计结果如下: Data Frequency Destribute 词频数最高的2000个单词的分布情况如图1所示，据此判定噪声词为词频大于300的单词，使用python提取出这部分单词。 Part 2:倒排索引 将Part 1中得到的高频词传到HDFS目录下，在MapReduce项目中以CacheFile的形式加载，如下： 12String filename = "/user/alexzhangch/inverted_index/stop.txt";job.addCacheFile(new Path(filename).toUri()); 然后在Map阶段，将该高频词存入HashMap，构建倒排索引时，对每个单词，查询其是否在map中，不是高频词则记录下对应的结果。 Part3: 扩展 扩展一： 数据清洗 A naive parser will group words by attributes which are not relevant to their meaning.Modify your parser to “scrub” words. You can define “scrub” however you wish; some suggestions include case-insensitivity, punctuation-insensitivity, etc. You will get extra karma for creating a language-independent scrubbing algorithm, but this is not required. 实现方式： 通过String类的replace方法将标点符号，特殊符号等替换，并忽略大小写，只保留单个单词，构建索引 123// 把每一句文本分割为单词,根据如下的标点符号进行分割String cleanLine = value.toString().toLowerCase().replaceAll("[_|&amp;$#&lt;&gt;\\^=\\[\\]\\*/\\\\,;,.\\-:()?!\"']", " ");StringTokenizer itr = new StringTokenizer(cleanLine); 扩展二： 完全倒排索引 Instead of creating an inverted file index (which maps words to their document ID), create a full inverted index (which maps words to their document ID + position in the document). How do you specify a word’s position in the document? 实现方式： 使用的是TextInputFormat，每一条记录是输入文件中的某一行， key为该行在文档中的偏移字节数，value为改行的文本，而根据Hadoop的文档，一个MAP 实例处理一个文件，在MAP实例中会将文件一行一行拆开然后顺序调用map函数，因此可以在MAP class采用一个int整数记录行数，然后在处理每一行数据时记录单词在改行的位置，因此map阶段最终的value格式为 1filename-linenumber-offset 具体实现如下: 12345678910111213141516171819202122232425public void map(LongWritable position, Text value, Context context) throws IOException, InterruptedException &#123; line_number += 1; Integer line_off = line_number; String file_name = ((FileSplit) context.getInputSplit()).getPath().getName().toString(); // 把每一句文本分割为单词,根据如下的标点符号进行分割 String cleanLine = value.toString().toLowerCase().replaceAll("[_|&amp;$#&lt;&gt;\\^=\\[\\]\\*/\\\\,;,.\\-:()?!\"']", " "); StringTokenizer itr = new StringTokenizer(cleanLine); Integer count = 0; while (itr.hasMoreTokens()) &#123; String word_ = itr.nextToken(); count += 1; // 非停用词 if(stopWord.get(word_) == null) &#123; word.set(word_); file.set(file_name + "-" + line_off.toString() + "-" + count.toString()); context.write(word, file); &#125; &#125;&#125; 最终的倒排表结构如下： 123456aaron titusandronicus-2984-5, titusandronicus-2379-1, titusandronicus-919-1, titusandronicus-2531-1, titusandronicus-3069-1, titusandronicus-2394-1, titusandronicus-812-5, titusandronicus-1800-1, titusandronicus-2534-7, titusandronicus-3477-7, titusandronicus-924-2, titusandronicus-926-1, titusandronicus-912-1, titusandronicus-3492-1, titusandronicus-3053-1, titusandronicus-936-1, titusandronicus-2523-1, titusandronicus-2407-7, titusandronicus-2409-1, titusandronicus-2410-2, titusandronicus-808-2, titusandronicus-2410-7, titusandronicus-3499-4, titusandronicus-2412-4, titusandronicus-3073-1, titusandronicus-2519-4, titusandronicus-1113-1, titusandronicus-2540-1, titusandronicus-3048-1, titusandronicus-2415-1, titusandronicus-838-1, titusandronicus-48-1, titusandronicus-2422-1, titusandronicus-1440-1, titusandronicus-2548-6, titusandronicus-2517-2, titusandronicus-3116-1, titusandronicus-1784-2, titusandronicus-2426-1, titusandronicus-1089-1, titusandronicus-2358-1, titusandronicus-900-1, titusandronicus-2430-1, titusandronicus-3037-1, titusandronicus-2504-1, titusandronicus-1780-8, titusandronicus-1296-6, titusandronicus-3112-1, titusandronicus-2550-1, titusandronicus-3141-1, titusandronicus-3027-1, titusandronicus-797-1, titusandronicus-795-2, titusandronicus-183-4, titusandronicus-3025-5, titusandronicus-3732-5, titusandronicus-1351-2, titusandronicus-897-2, titusandronicus-3018-1, titusandronicus-854-1, titusandronicus-2438-1, titusandronicus-2387-1, titusandronicus-1348-1, titusandronicus-1853-1, titusandronicus-1772-1, titusandronicus-1770-2, titusandronicus-2490-1, titusandronicus-3081-1, titusandronicus-3744-1, titusandronicus-1334-1, titusandronicus-2833-6, titusandronicus-1824-5, titusandronicus-2312-5, titusandronicus-1389-4, titusandronicus-873-1, titusandronicus-2443-1, titusandronicus-1827-1, titusandronicus-2584-1, titusandronicus-2447-1, titusandronicus-1845-1, titusandronicus-1310-3, titusandronicus-1074-5, titusandronicus-1312-1, titusandronicus-657-6, titusandronicus-3762-5, titusandronicus-494-3, titusandronicus-2320-1, titusandronicus-2455-1, titusandronicus-1068-4, titusandronicus-891-1, titusandronicus-3086-1, titusandronicus-2457-2, titusandronicus-2459-1, titusandronicus-1837-2, titusandronicus-1053-1, titusandronicus-2465-1, titusandronicus-1051-2, titusandronicus-2575-2abaissiez kinghenryv-4547-6abandon 3kinghenryvi-415-5, tamingoftheshrew-453-5, troilusandcressida-2712-3, twelfthnight-482-5, othello-2857-4, timonofathens-3580-8, asyoulikeit-4082-8, asyoulikeit-3493-3, asyoulikeit-3490-2, asyoulikeit-1001-3abash troilusandcressida-791-5abate kinghenryv-1622-1, kinghenryv-1621-1, kinghenryv-1621-4, kinghenryv-828-10, kinghenryv-3413-7, hamlet-4823-9, titusandronicus-137-6, loveslabourslost-3738-1, tamingoftheshrew-278-3, venusandadonis-792-6, kingrichardiii-5768-1, merchantofvenice-3738-3, glossary-151-2, glossary-3-1, midsummersnightsdream-2052-1, cymbeline-654-1, romeoandjuliet-3603-1abated 2kinghenryiv-358-5, kinglear-2302-3, coriolanus-3661-1 实验总结及收获: How long do you think this project would take you to finish? 一天 How much time did you actually spend on this project? 两天 Acknowledge any assistance you received from anyone except assigned course readings and the course staff. Hadoop中的输入数据划分格式等细节问题通过google获得，通过全局变量记录行号方式以获取a word’s position in the document的方式由计41 李永斌告知。 What test corpus did you load into HDFS? Where is it located (i.e. HDFS path)? 课程提供的莎士比亚全集，位置： /user/alexzhangch/inverted_index/input What, if any, “noisy words” did you find as a result of your WordCount? By what criteria did you determine that they were “noisy”? Were you surprised by any of these words? 噪声单词通过Word count 方式进行统计词频，然后使用python绘制对应的分布情况，然后结合对应的单词确定以词频数超过300为界限划分噪声词 Which extensions did you do? Please also detail any interesting results you found. 实现的扩展功能 数据清洗，去除标点符号、特殊符号，不区分大小写等 完全倒排索引，通过行号+在改行的出现位置作为单词的position interesting results MAP阶段每一个输入文件由一个Map实例处理 TextInputFormat输入时，key是改行的byte offset，不是行号，但是由于Map实例中会一行一行读取然后并行执行map函数，因此可以使用一个全局变量去记录当前map函数处理的行号，从而去定位单词在文档中的位置 收获 熟悉了Hadoop的MapReduce编程，对其实现机制有了更深入的理解，对MapReduce过程中的数据流通过程有了更多的认识，特别是输入数据的切分]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[大规模数据处理-Spark]]></title>
      <url>%2Farticle%2F%E5%A4%A7%E8%A7%84%E6%A8%A1%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86-Spark.html</url>
      <content type="text"><![CDATA[回顾 hardware-&gt;OS-&gt;App 硬件集群，网络互连-&gt;DFS,Schechuler,Monitor,FT, consistent -&gt; MPI,mapreduce,spark -&gt; App Master 服务器：Master的crash容灾 printf: 自己动手写CPU,intel 手册 MPI,mapreduce,spark DataBase: WebTable: URL-Page 1-&gt; URL,Page,Title,Language,pagerank,, table: sort, 按照id sort-&gt;join SQL-&gt;compile|EEngine|DBMS-&gt;Storage、LsmTree 分布式数据库 P： id, mode, data N: id, neighborid PN =&gt; P1,N1 、 P2,N2、、、 p.id = n.id 放于一个节点（hash) sort -&gt; merge（p.objid = n.objid) 根据n.objid hash然后发送给对应节点查询数据 CRC Hive =&gt;Sql =&gt;Spark=&gt;Dryad Spark特性 set operation parall action libpacp Dryad Jobs are expressed as a Directed Acyclic Graph (DAG): dataflow Vertices are computations. Edges are communication channels. Each vertex can have several input and output channels. Each vertex runs one or more times. Stop when all vertices have completed their execution at least once. Sprak 论文Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[大规模数据处理-分布式key/value对存储：BigTable、DHT]]></title>
      <url>%2Farticle%2F%E5%A4%A7%E8%A7%84%E6%A8%A1%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86-%E5%88%86%E5%B8%83%E5%BC%8Fkey-value%E5%AF%B9%E5%AD%98%E5%82%A8.html</url>
      <content type="text"><![CDATA[BigTable Bigtable 是一个分布式的结构化数据存储系统，它被设计用来处理海量数据:通常是分布在数千台普通服 务器上的 PB 级的数据。 Bigtable是一个为管理大规模结构化数据而设计的分布式存储系统，可以扩展到PB级数据和上千台服务器。很多google的项目使用Bigtable存储数据，这些应用对Bigtable提出了不同的挑战，比如数据规模的要求、延迟的要求。Bigtable能满足这些多变的要求，为这些产品成功地提供了灵活、高性能的存储解决方案。 Bigtable看起来像一个数据库，采用了很多数据库的实现策略。但是Bigtable并不支持完整的关系型数据模型；而是为客户端提供了一种简单的数据模型，客户端可以动态地控制数据的布局和格式，并且利用底层数据存储的局部性特征。Bigtable将数据统统看成无意义的字节串，客户端需要将结构化和非结构化数据串行化再存入Bigtable。数据的下标是行和列的名字，名字可以是任意的字符串。Bigtable将存储的数据都视为字符串，但是 Bigtable 本身不去解析这些字符串，应用程序通常会在把各种结构化或者半结构化的数据串行化到这些字符串里。通过仔细选择数据的模式，客户可 以控制数据的位置相关性。最后，可以通过 BigTable的模式参数来控制数据是存放在内存中、还是硬盘上。 具有以下特性：适用性广泛、可扩展、高性能和高可用性 BigTable的数据模型 Bigtable 是一个稀疏的、分布式的、持久化存储的多维度排序 Map。Map 的索引是行关键字、列关键字 以及时间戳;Map 中的每个 value 都是一个未经解析的 byte 数组。 1(row:string, column:string,time:int64)-&gt;string BigTable 内部存储数据的文件是 Google SSTable 格式的。SSTable 是一个持久化的、排序的、不可更改的Map 结构，而 Map 是一个 key-value 映射的数据结构，key 和value 的值都是任意的 Byte 串。可以对 SSTable 进行如下的操作:查询与一个 key 值相关的value，或者遍历某个 key 值范围内的所有的 key-value 对。从内 部看，SSTable是一系列的数据块(通常每个块的大小是 64KB，这个大小是可以配置的)。SSTable使用块索引(通常存储在 SSTable 的最后)来定位数据块;在打开 SSTable 的时候，索引被加载到内存。每次查找都可以通过一次磁盘搜索完成:首先使用二分查找法在内存中的索引里找到数据块的位置,然后再从硬盘读取相应的数据块。也可以选择把整个 SSTable 都放在内存中，这样就不必访问硬盘了。 SSTable的全称是Sorted Strings Table，是一种不可修改的有序的键值映射，提供了查询、遍历等功能。每个SSTable由一系列的块（block）组成，Bigtable将块默认设为64KB。在SSTable的尾部存储着块索引，在访问SSTable时，整个索引会被读入内存。BigTable论文没有提到SSTable的具体结构，LevelDb日知录之四： SSTable文件这篇文章对LevelDb的SSTable格式进行了介绍，因为LevelDB的作者JeffreyDean正是BigTable的设计师，所以极具参考价值。每一个片（tablet）在GFS里都是按照SSTable的格式存储的，每个片可能对应多个SSTable。 Bigtable集群 Bigtable集群包括三个主要部分：一个供客户端使用的库，一个主服务器（master server），许多片服务器（tablet server）。 正如数据模型小节所说，Bigtable会将表（table）进行分片，片（tablet）的大小维持在100-200MB范围，一旦超出范围就将分裂成更小的片，或者合并成更大的片。每个片服务器负责一定量的片，处理对其片的读写请求，以及片的分裂或合并。片服务器可以根据负载随时添加和删除。这里片服务器并不真实存储数据，而相当于一个连接Bigtable和GFS的代理，客户端的一些数据操作都通过片服务器代理间接访问GFS。 主服务器负责将片分配给片服务器，监控片服务器的添加和删除，平衡片服务器的负载，处理表和列族的创建等。注意，主服务器不存储任何片，不提供任何数据服务，也不提供片的定位信息。 客户端需要读写数据时，直接与片服务器联系。因为客户端并不需要从主服务器获取片的位置信息，所以大多数客户端从来不需要访问主服务器，主服务器的负载一般很轻。 片服务器访问顺序 首先是第一层，Chubby file。这一层是一个Chubby文件，它保存着root tablet的位置。这个Chubby文件属于Chubby服务的一部分，一旦Chubby不可用，就意味着丢失了root tablet的位置，整个Bigtable也就不可用了。 第二层是root tablet。root tablet其实是元数据表（METADATA table）的第一个分片，它保存着元数据表其它片的位置。root tablet很特别，为了保证树的深度不变，root tablet从不分裂。 第三层是其它的元数据片，它们和root tablet一起组成完整的元数据表。每个元数据片都包含了许多用户片的位置信息。 可以看出整个定位系统其实只是两部分，一个Chubby文件，一个元数据表。注意元数据表虽然特殊，但也仍然服从前文的数据模型，每个分片也都是由专门的片服务器负责，这就是不需要主服务器提供位置信息的原因。客户端会缓存片的位置信息，如果在缓存里找不到一个片的位置信息，就需要查找这个三层结构了，包括访问一次Chubby服务，访问两次片服务器。 元数据表的结构 元数据表（METADATA table）是一张特殊的表，它被用于数据的定位以及一些元数据服务，不可谓不重要。但是Bigtable论文里只给出了少量线索，而对表的具体结构没有说明。这里我试图根据论文的一些线索，猜测一下表的结构。首先列出论文中的线索： The METADATA table stores the location of a tablet under a row key that is an encoding of the tablet’s table identifier and its end row. Each METADATA row stores approximately 1KB of data in memory（因为访问量比较大，元数据表是放在内存里的，这个优化在论文的locality groups中提到）.This feature（将locality group放到内存中的特性） is useful for small pieces of data that are accessed frequently: we use it internally for the location column family in the METADATA table. We also store secondary information in the METADATA table, including a log of all events pertaining to each tablet(such as when a server begins serving it). 第一条线索，元数据表的行键是由片所属表名的id和片最后一行编码而成，所以每个片在元数据表中占据一条记录（一行），而且行键既包含了其所属表的信息也包含了其所拥有的行的范围。譬如采取最简单的编码方式，元数据表的行键等于strcat(表名，片最后一行的行键)。 第二点线索，除了知道元数据表的地址部分是常驻内存以外，还可以发现元数据表有一个列族称为location，我们已经知道元数据表每一行代表一个片，那么为什么需要一个列族来存储地址呢？因为每个片都可能由多个SSTable文件组成，列族可以用来存储任意多个SSTable文件的位置。一个合理的假设就是每个SSTable文件的位置信息占据一列，列名为location:filename。当然不一定非得用列键存储完整文件名，更大的可能性是把SSTable文件名存在值里。获取了文件名就可以向GFS索要数据了。 第三个线索告诉我们元数据表不止存储位置信息，也就是说列族不止location，这些数据暂时不是咱们关心的。 元数据表及恢复相关 Bigtable的存储结构： 所有tablet的位置信息存储在METADATA表中。查找一个tablet的位置需要通过一个3层的类似B+树的结构： 首层是Chubby中的一个文件，它储存METADATA的第一个tablet即root tablet的位置。 root tablet永远不会分裂，以保持3层结构。它保存后面的所有tablet的元信息，如位置、行key范围等（此句不确定，自己的猜测）。 METADATA的其它tablet负责保存用户tablet的元数据（位置、范围、日志等）。tablet的位置信息都在内存中。 client发起的一次成功的查找需要3次网络通信，而若缓存失效，则最多需要6次（前3次发现缓存失效）。 每个tablet会维持若干个memtable（猜测：每个列族一个memtable），对tablet的读写操作会先反映到memtable上。memtable会在某些条件触发时（猜测：体积或更新次数达到阈值）将内容写入GFS。 tablet在GFS中的基本存储单位是SSTable，它提供一种不可改且排序的key/value映射。每个SSTable由多个排序的block组成，SSTable的最后就是各block的索引，在SSTable打开时索引就在内存中，查找时先在内存中定位到block。 tablet内的各SSTable间为乱序，查找时需要依次查找每个SSTable，因此在SSTable数量达到阈值后要分裂。 master对tablet server的追踪： 每个tablet server都会在Chubby的指定文件夹下持有一个唯一的文件并上锁。在失去锁后，若文件存在，则尝试重上锁；若文件不存在或上锁失败若干次，则放弃对tablet的管理并自杀。tablet server退出前也会释放锁。 在生命期中master会监视Chubby文件夹中的文件。若新增文件，则表明有新tablet server。若master获得了已有文件的锁，说明对应的tablet server错误，则master会删除此文件并将其管理的tablet标记为未分配。 master在运行中也会定期询问tablet server的状态，若无回应若报告失去锁，则视其为错误。 每个数据片服务器都会管理一系列的数据片（通常，我们的每台数据片服务器都有大约数十个至上千个数据片）。数据片服务器会处理针对已加载数据片的读写请求，并且还会将已经增长过大的数据片拆分为多个较小的数据片。 参考资料 谷歌技术“三宝”之BigTable]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Massive-Data-Process-Homework-3]]></title>
      <url>%2Farticle%2FMassive-Data-Process-Homework-3.html</url>
      <content type="text"><![CDATA[Key-Value对存储 在BigTable系统中，如果有一个TabletServer出现了错误，不能继续进行工作，这样对应的Tablet将不能够提供服务。在系统中，数据表格中的数据形式就是URLContent的Key-Value对的形式。 磁盘中的元数据表中包含的内容，内存中的元数据表中包含的内容？（只需要描述必要的数据结构帮助恢复过程即可） 简要描述系统恢复的流程。 磁盘中的元数据表中包含的内容，内存中的元数据表中包含的内容？（只需要描述必要的数据结构帮助恢复过程即可） 磁盘中的元数据表包含的内容 Tablet的位置信息及数据范围，数据片的所有事件的日志信息，SSTable文件列表信息，Redo Point 其数据结构： encoding(table_id + end_row_key): tablet_location(SSTable file list), log 内存中的元数据表包含的内容： 内存中的元数据表是已经分配给对应tablet server 管理的Tablet位置信息，因此其value为：ip:port，即保存的Tablet的服务器的进程位置。 简要描述系统恢复的流程。 BigTable 使用 Chubby 跟踪记录 Tablet 服务器的状态。当一个 Tablet 服务器启动时，它在 Chubby 的一个指定目录下建立一个有唯一性名字的文件，并且获取该文件的独占锁。 Master 服务器实时监控着这个目录(服务器目录)，Master 服务器通过轮询 Tablet 服务器文件锁的状态来检测何时 Tablet 服务器不再为 Tablet 提供服务。当Tablet Server宕机之后，Master尝试与之通信多次得不到响应，Master 服务器就会尝试获取该 Tablet 服务器文件的独占锁;如果 Master 服务器成功获取了独占锁，则Master服务器判断该Tablet Server宕机了，于是Master服务器删除该tablet server在chubby中的服务器文件，然后Master将该Tablet Server 管理的所有数据片置为未分配状态。 随后，该宕机的Tablet Server管理的Tablet将被分散到多个Server上，每个服务器只会加载该服务器上很少量的数据片，这些server为了恢复原来tablet server上的修改，需要从commit log文件中读取log信息。在BigTable中，每个数据片服务器记录一个commit-log文件，在恢复时，这些服务器先向Master发出请求表明自己需要从该数据片服务器的提交日志文件中恢复数据片的修改操作时，Master首先将该提交日志文件按照关键字的顺序，对提交日志的条目进行排序。在排序输出中，某个特定数据片的所有修改操作信息都会连续存放在一起。因此，只需要一次磁盘寻道，然后再执行一次连续的读取操作，就可以读取日志文件并将不同的数据片的提交日志分发给对应的新分配数据片服务器，新分配的数据片服务器收到提交日志时redo该日志恢复数据片信息，至此，这些数据片可以开始提供服务了。 在DHT上实现Query 如果直接使用dht来实现key到host的对应，难点在于实现一个range query。一个range query可能需要涉及到所有的节点。有没有办法能够在仍然使用dht的情况下减少所涉及到的节点的数目? 算法基于如下hash实现； 假定hash(key)均匀的分布在一个环上 所有的节点也都分布在同一环上（节点加入时通过随机数的方式确定其位置） 每个节点只负责一部分Key，当节点加入、退出时只影响加入退出的节点和其邻居节点或者其他节点只有少量的Key受影响 为了优化range query, 使用order-preserving hash function(保序散列函数)实现对key的hash, 这样某一个范围的(key, value)对会集中分布在相邻几个节点中，这样range query 时只需要访问这部分相邻节点即可。但是这有可能会导致负载不均衡。 每一个节点加入时需要为其构建Route Table 参考： Klemm, F., Girdzijauskas, S., Boudec, J. Y. L., &amp; Aberer, K. (2007). On routing in distributed hash tables. 113-122. Routing in Distributed Hash Tables | Anne-Marie Kermarrec https://www.youtube.com/watch?v=WqQRQz_XYg4]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Mac下Hadoop安装配置问题 waiting for AM container to be allocated, launched and register with RM.]]></title>
      <url>%2Farticle%2FMac%E4%B8%8BHadoop%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE%E6%95%99%E7%A8%8B.html</url>
      <content type="text"><![CDATA[JAVA 安装配置 java_home 12$ /usr/libexec/java_home/Library/Java/JavaVirtualMachines/jdk1.8.0_101.jdk/Contents/Home 查看java虚拟机启动情况 1$ jps NameNode Web UI http://localhost:50070/ ResourceManager at - http://localhost:8088/ 问题解决 ACCEPTED: waiting for AM container to be allocated, launched and register with RM. 参考自 This will happen when there are not enough resources (memory) to run the AppMaster container needed to control the Tez job. In YARN capacity-scheduler.xml there is a property yarn.scheduler.capacity.maximum-am-resource-percent which controls the percentage of total cluster memory that can be used by AM containers. If you have several jobs running then each AM will consume the memory required for one container. If this exceeds the given % of total cluster memory the next AM to run will wait until there are free resources for it to run. You’ll need to increase yarn.scheduler.capacity.maximum-am-resource-percent to get the AM to run. 另一个可能的原因是: Directory /tmp/hadoop-alexzhangch/nm-local-dir error, used space above threshold of 90.0%, removing from list of valid directories 即存储空间不够，释放足够的空间即可，保持10%的空闲空间 或者重新设置该百分比 1yarn.nodemanager.disk-health-checker.max-disk-utilization-per-disk-percentage 问题 iptables ulimit ssh 加速问题 maprod.map.tasks maprod.reduce.tasks Xmx = 2G : hadoop/env.sh 向HDFS传数据 1bin/hdfs dfs -put etc/hadoop input 取回数据 12$ bin/hdfs dfs -get output output$ cat output/* 或者直接输出 1bin/hdfs dfs -cat output/* WordCount 的运行 创建目录 123hdfs dfs -mkdir /user/alexzhangch/inverted_indexhdfs dfs -mkdir /user/alexzhangch/inverted_index/input 上传文件 1hdfs dfs -put ../shakespeare/* /user/alexzhangch/inverted_index/input/ PATH设置; 12345678910vi ~/.bashrcexport JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_101.jdk/Contents/Homeexport PATH=$JAVA_HOME/bin:$PATHexport HADOOP_HOME=/Users/alexzhangch/Documents/jianguoyun/2017_summer/hadoop-2.8.0export PATH=$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATHexport HADOOP_CLASSPATH=$&#123;JAVA_HOME&#125;/lib/tools.jarsource ~/.bashrc 编译源代码 12$ hadoop com.sun.tools.javac.Main Inverted_Index.java$ jar cf inverted.jar Inverted_Index*.class 运行程序： 1hadoop jar inverted.jar Inverted_Index /user/alexzhangch/inverted_index/input /user/alexzhangch/inverted_index/output 查看运行结果 12345678$ hdfs dfs -ls /user/alexzhangch/inverted_index/outputFound 3 items-rw-r--r-- 1 alexzhangch supergroup 0 2017-07-01 15:42 /user/alexzhangch/inverted_index/output/_SUCCESS-rw-r--r-- 1 alexzhangch supergroup 390915 2017-07-01 15:42 /user/alexzhangch/inverted_index/output/part-r-00000-rw-r--r-- 1 alexzhangch supergroup 392440 2017-07-01 15:42 /user/alexzhangch/inverted_index/output/part-r-00001$ hdfs dfs -cat /user/alexzhangch/inverted_index/output/part-r-00000 把输出结果取回本地 hdfs dfs -get output output]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[大规模数据处理-MapReduce]]></title>
      <url>%2Farticle%2F%E5%A4%A7%E8%A7%84%E6%A8%A1%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86-MapReduce.html</url>
      <content type="text"><![CDATA[MPI MPI编程：n台机器编号，0~n-1 send(from. to, msg, sizeof(msg)) recv() brodcast(from, Group, O o), gather(Group, to) 没有出错处理 MapReduce 123map (in_key, in_value) -&gt; (out_key, intermediate_value) listreduce (out_key, intermediate_value list) -&gt; out_value list MAP -》 Shuffle YARN: ResourceManager: 监控作用，仅一个 AppManager： 调度器 APPMaster work： 作业 Distribute sort MxM 期末考试题目 Key,Value Store put(key, value) get(key) query(key_start, key_stop) 数据结构： Hash Array 平衡树 B树 B+树 磁盘： log get操作： 加载到内存， log+hash,sort,B+ flush 得到index LSM tree 单机 (key,value) store 分布式环境的(key, value) store K -&gt; node -&gt; 本机（key，value） hash(key) % N =&gt; node load banlance? scability 容错 DHT distribute hash K-means算法 选择k个初始点作为k个set的中心，对数据集中的每条数据计算其最近的set, 然后计算每个set的中心，进而计算新的k个set, 重复此算法 但是这太复杂了，因此我们采用canopy算法。首先建立一系列重叠canopy，使得所有数据至少在一个canopy中，然后仿照k-means算法，不需要比较所有数据点，只需要比较其所在canopy的中心点即可。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Massive Data Process Homework 2]]></title>
      <url>%2Farticle%2FMassive-Data-Process-Homework-2.html</url>
      <content type="text"><![CDATA[仔细阅读MapReduce这篇论文，回答下面的问题。 分布式排序 在MapReduce执行的过程中，中间的shuffle过程是一个非常重要的过程。这个过程中其中一个使用的方法就是进行分布式的排序。依据你自己的理解，阐述分布式排序方法。只需要用伪代码写出在各个节点中的程序代码即可。 解: 采用分布式并行排序算法,算法步骤如下： 对于n个分布式计算节点0，1，2，，，，n-1 对每个节点，按采样率p随机抽取\(x_i\)条数据, \(x_i = p * X_i\), 其中\(X_i\)表示编号为i的节点当前存储的数据数 定义节点0为Primary节点，其余节点将各自的采样数据\(x_i\)发送给节点0，节点0根据采样数据的统计信息将数据划分为大致相等的n个数值范围，并将划分信息反馈给各个节点（即根据分布情况各个节点分别处理一定范围的数据） 各个节点收到划分结果后对不属于本节点的数据发送给对应的节点，并进行排序 各节点收到属于该范围的所有节点之后，对数据进行排序操作 最终各个节点存储的是所有数据排序后的分割结果 MapReduce程序完成矩阵相乘的运算 一个非常大的矩阵需要使用MapReduce来完成相乘的运算，写出MapReduce的伪代码。 算法如下： 12345678910111213141516171819202122232425map(key, value, matrix): /* * matrix = "M" / "N" * 矩阵M*N, M矩阵m*n, N矩阵n*p * key: (i, j) 行列标签对 * value = a(i, j) 矩阵对应位置的数值 */ i = key[0]; j = key[1]; if (matrix == "M") for (k = 0 to m - 1) EmitIntermediate((i, k), (value, j, matrix)); else: for (k = 0 to n - 1) EmitIntermediate((k, j), (value, i, matrix));reduce((i, j), Value): / * key = i,j * Value格式为： （value, pos, matrix）,=即（数值，pos, M/N） */ result = 0; for (pos = 0 to n-1) result += Value[pos]["M"].value * Value[pos]["N"].value; Emit(result); 分布式计算MapReduce 请仔细阅读下面这段关于MapReduce的伪代码。这段代码最终将会输出某一个大型文档集合中的出现次数最多的三个单词。 123456789101112131415161718192021222324252627282930void Map(String DocID, Text content) &#123; HashMap&lt;word, count&gt; wordmap=new HashMap(……); for each word in content&#123; if word not in wordmap wordmap.put(word,1); else wordmap.get(word).count++; &#125; Emit("donemap",wordmap);&#125;void Reduce(String key, Iterator&lt;HashMap&gt; maps) &#123; HashMap&lt;word, count&gt; allwords = new HashMap(……); List&lt;word, count&gt; wordlist = new List(……); for map in maps&#123; for each (word, count) in map if word not in allwords allwords.put(word,count) else allwords.get(word)+=count; &#125; for each (word, count) in allwords wordlist.add(word,count); sort(wordlist) on count; Emit(wordlist.get(0)); Emit(wordlist.get(1)); Emit(wordlist.get(2));&#125; 小华同学运行上述的代码，发现部分reducers会出现OutOfMemoryException的错误。请结合代码分析其原因(不要指出语法错误)。 在该MAP函数中，map输出的（key，value）对的key均为“donemap”,故而在最终的reduce环节中，所有的数据中间(key, value)对会全部由某一个reducer进行处理，数据规模较大时必然超出内存 针对上述错误，你有什么修改方案？请简要说明你的修改方案。 词频统计阶段 1234567891011121314151617void Map(String DocID, Text content) &#123; HashMap&lt;word, count&gt; wordmap=new HashMap(……); for each word in content&#123; if word not in wordmap wordmap.put(word,1); else wordmap.get(word).count++; &#125; Emit(wordmap);&#125;void Reduce(String key, Iterator&lt;IntWriteable&gt; counts) &#123; int sum = 0; for count in counts sum += count; Emit((word, sum))&#125; 寻找Top 3 123456789101112131415161718192021222324252627282930void Map(key, value) &#123; /* * key: 将统计结果分成若干组,每组M条数据(自行切割) * value: (word, count) */ wordlist = value to list; sort(wordlist) on count; for i = 0 to 3 Emit("Final", (wordlist[i]));&#125;void Reduce(String key, Iterator&lt;HashMap&gt; maps) &#123; HashMap&lt;word, count&gt; allwords = new HashMap(……); List&lt;word, count&gt; wordlist = new List(……); for map in maps&#123; for each (word, count) in map if word not in allwords allwords.put(word,count) else allwords.get(word)+=count; &#125; for each (word, count) in allwords wordlist.add(word,count); sort(wordlist) on count; Emit(wordlist.get(0)); Emit(wordlist.get(1)); Emit(wordlist.get(2));&#125; 改进方案是采用两轮MapReduce操作的做法，第一轮操作统计单词出现次数，输出(word，count)对，第二轮操作根据第一轮的统计结果找出频数的Top 3 第二轮操作时先将第一轮的统计结果分割成多个部分进行map,每个Map阶段先对本节点的数据按单词数count排序，然后输出前3个（key 一致）， 然后reduce阶段将各节点输出的”前3“对进行排序，输出前3个结果即为所求]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[谷歌文件系统论文学习]]></title>
      <url>%2Farticle%2F%E8%B0%B7%E6%AD%8C%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E8%AE%BA%E6%96%87%E5%AD%A6%E4%B9%A0.html</url>
      <content type="text"><![CDATA[Google File System论文学习 摘要 我们设计并实现了Google GFS文件系统，一个面向大规模数据密集型应用的、可伸缩的分布式文件系统。GFS虽然运行在廉价的普遍硬件设备上，但是它依然了提供灾难冗余的能力，为大量客户机提供了高性能的服务。 虽然GFS的设计目标与许多传统的分布式文件系统有很多相同之处，但是，我们的设计还是以我们对自己的应用的负载情况和技术环境的分析为基础 的，不管现在还是将来，GFS和早期的分布式文件系统的设想都有明显的不同。所以我们重新审视了传统文件系统在设计上的折衷选择，衍生出了完全不同的设计 思路。 GFS完全满足了我们对存储的需求。GFS作为存储平台已经被广泛的部署在Google内部，存储我们的服务产生和处理的数据，同时还用于那些 需要大规模数据集的研究和开发工作。目前为止，最大的一个集群利用数千台机器的数千个硬盘，提供了数百TB的存储空间，同时为数百个客户机服务。 在本论文中，我们展示了能够支持分布式应用的文件系统接口的扩展，讨论我们设计的许多方面，最后列出了小规模性能测试以及真实生产系统中性能相关数据。 分类和主题描述 D [4]: 3—D分布文件系统 常用术语 设计，可靠性，性能，测量 关键词 容错，可伸缩性，数据存储，集群存储 简介 为了满足Google迅速增长的数据处理需求，我们设计并实现了Google文件系统(Google File System – GFS)。GFS与传统的分布式文件系统有着很多相同的设计目标，比如，性能、可伸缩性、可靠性以及可用性。但是，我们的设计还基于我们对我们自己的应用 的负载情况和技术环境的观察的影响，不管现在还是将来，GFS和早期文件系统的假设都有明显的不同。所以我们重新审视了传统文件系统在设计上的折衷选择， 衍生出了完全不同的设计思路。 首先，组件失效被认为是常态事件，而不是意外事件。GFS包括几百甚至几千台普通的廉价设备组装的存储机器，同时被相当数量的客户机访问。 GFS组件的数量和质量导致在事实上，任何给定时间内都有可能发生某些组件无法工作，某些组件无法从它们目前的失效状态中恢复。我们遇到过各种各样的问 题，比如应用程序bug、操作系统的bug、人为失误，甚至还有硬盘、内存、连接器、网络以及电源失效等造成的问题。所以，持续的监控、错误侦测、灾难冗 余以及自动恢复的机制必须集成在GFS中。 其次，以通常的标准衡量，我们的文件非常巨大。数GB的文件非常普遍。每个文件通常都包含许多应用程序对象，比如web文档。当我们经常需要处 理快速增长的、并且由数亿个对象构成的、数以TB的数据集时，采用管理数亿个KB大小的小文件的方式是非常不明智的，尽管有些文件系统支持这样的管理方 式。因此，设计的假设条件和参数，比如I/O操作和Block的尺寸都需要重新考虑。 第三，绝大部分文件的修改是采用在文件尾部追加数据，而不是覆盖原有数据的方式。对文件的随机写入操作在实际中几乎不存在。一旦写完之后，对文 件的操作就只有读，而且通常是按顺序读。大量的数据符合这些特性，比如：数据分析程序扫描的超大的数据集；正在运行的应用程序生成的连续的数据流；存档的 数据；由一台机器生成、另外一台机器处理的中间数据，这些中间数据的处理可能是同时进行的、也可能是后续才处理的。对于这种针对海量文件的访问模式，客户 端对数据块缓存是没有意义的，数据的追加操作是性能优化和原子性保证的主要考量因素。 第四，应用程序和文件系统API的协同设计提高了整个系统的灵活性。比如，我们放松了对GFS一致性模型的要求，这样就减轻了文件系统对应用程 序的苛刻要求，大大简化了GFS的设计。我们引入了原子性的记录追加操作，从而保证多个客户端能够同时进行追加操作，不需要额外的同步操作来保证数据的一 致性。本文后面还有对这些问题的细节的详细讨论。 Google已经针对不同的应用部署了多套GFS集群。最大的一个集群拥有超过1000个存储节点，超过300TB的硬盘空间，被不同机器上的数百个客户端连续不断的频繁访问。 2.设计概述 2.1设计预期 在设计满足我们需求的文件系统时候，我们的设计目标既有机会、又有挑战。之前我们已经提到了一些需要关注的关键点，这里我们将设计的预期目标的细节展开讨论。 系统由许多廉价的普通组件组成，组件失效是一种常态。系统必须持续监控自身的状态，它必须将组件失效作为一种常态，能够迅速地侦测、冗余并恢复失效的组件。 系统存储一定数量的大文件。我们预期会有几百万文件，文件的大小通常在100MB或者以上。数个GB大小的文件也是普遍存在，并且要能够被有效的管理。系统也必须支持小文件，但是不需要针对小文件做专门的优化。 系统的工作负载主要由两种读操作组成：大规模的流式读取和小规模的随机读取。大规模的流式读取通常一次读取数百KB的数据，更常见的是一次读取 1MB甚至更多的数据。来自同一个客户机的连续操作通常是读取同一个文件中连续的一个区域。小规模的随机读取通常是在文件某个随机的位置读取几个KB数 据。如果应用程序对性能非常关注，通常的做法是把小规模的随机读取操作合并并排序，之后按顺序批量读取，这样就避免了在文件中前后来回的移动读取位置。 系统的工作负载还包括许多大规模的、顺序的、数据追加方式的写操作。一般情况下，每次写入的数据的大小和大规模读类似。数据一旦被写入后，文件就很少会被修改了。系统支持小规模的随机位置写入操作，但是可能效率不彰。 系统必须高效的、行为定义明确的 （alex注：well-defined）实现多 客户端并行追加数据到同一个文件里的语意。我们的文件通常被用于”生产者-消费者“队列，或者其它多路文件合并操作。通常会有数百个生产者，每个生产者进 程运行在一台机器上，同时对一个文件进行追加操作。使用最小的同步开销来实现的原子的多路追加数据操作是必不可少的。文件可以在稍后读取，或者是消费者在 追加的操作的同时读取文件。 高性能的稳定网络带宽远比低延迟重要。我们的目标程序绝大部分要求能够高速率的、大批量的处理数据，极少有程序对单一的读写操作有严格的响应时间要求。 2.2 接口 GFS提供了一套类似传统文件系统的API接口函数，虽然并不是严格按照POSIX等标准API的形式实现的。文件以分层目录的形式组织，用路径名来标识。我们支持常用的操作，如创建新文件、删除文件、打开文件、关闭文件、读和写文件。 另外，GFS提供了快照和记录追加操作。快照以很低的成本创建一个文件或者目录树的拷贝。记录追加操作允许多个客户端同时对一个文件进行数据追 加操作，同时保证每个客户端的追加操作都是原子性的。这对于实现多路结果合并，以及”生产者-消费者”队列非常有用，多个客户端可以在不需要额外的同步锁 定的情况下，同时对一个文件追加数据。我们发现这些类型的文件对于构建大型分布应用是非常重要的。快照和记录追加操作将在3.4和3.3节分别讨论。 2.3 架构 一个GFS集群包含一个单独的Master节点 （alex注：这里的一个单 独的Master节点的含义是GFS系统中只存在一个逻辑上的Master组件。后面我们还会提到Master节点复制，因此，为了理解方便，我们把 Master节点视为一个逻辑上的概念，一个逻辑的Master节点包括两台物理主机，即两台Master服务器）、多台 Chunk服务器，并且同时被多个客户端访问，如图1所示。所有的这些机器通常都是普通的Linux机器，运行着用户级别(user-level)的服务 进程。我们可以很容易的把Chunk服务器和客户端都放在同一台机器上，前提是机器资源允许，并且我们能够接受不可靠的应用程序代码带来的稳定性降低的风 险。 谷歌三大核心技术（二）Google MapReduce中文版 GFS存储的文件都被分割成固定大小的Chunk。在Chunk创建的时候，Master服务器会给每个Chunk分配一个不变的、全球唯一的 64位的Chunk标识。Chunk服务器把Chunk以linux文件的形式保存在本地硬盘上，并且根据指定的Chunk标识和字节范围来读写块数据。 出于可靠性的考虑，每个块都会复制到多个块服务器上。缺省情况下，我们使用3个存储复制节点，不过用户可以为不同的文件命名空间设定不同的复制级别。 Master节点管理所有的文件系统元数据。这些元数据包括名字空间、访问控制信息、文件和Chunk的映射信息、以及当前Chunk的位置信息。Master节点还管理着系统范围内的活动，比如，Chunk租用管理 (alex注：BDB也有关于lease的描述，不知道是否相同)、孤儿Chunk (alex注：orphaned chunks)的回收、以及Chunk在Chunk服务器之间的迁移。Master节点使用心跳信息周期地和每个Chunk服务器通讯，发送指令到各个Chunk服务器并接收Chunk服务器的状态信息。 GFS客户端代码以库的形式被链接到客户程序里。客户端代码实现了GFS文件系统的API接口函数、应用程序与Master节点和Chunk服 务器通讯、以及对数据进行读写操作。客户端和Master节点的通信只获取元数据，所有的数据操作都是由客户端直接和Chunk服务器进行交互的。我们不 提供POSIX标准的API的功能，因此，GFS API调用不需要深入到Linux vnode级别。 无论是客户端还是Chunk服务器都不需要缓存文件数据。客户端缓存数据几乎没有什么用处，因为大部分程序要么以流的方式读取一个巨大文件，要 么工作集太大根本无法被缓存。无需考虑缓存相关的问题也简化了客户端和整个系统的设计和实现。（不过，客户端会缓存元数据。）Chunk服务器不需要缓存 文件数据的原因是，Chunk以本地文件的方式保存，Linux操作系统的文件系统缓存会把经常访问的数据缓存在内存中。 2.4 单一Master节点 单一的Master节点的策略大大简化了我们的设计。单一的Master节点可以通过全局的信息精确定位Chunk的位置以及进行复制决策。另 外，我们必须减少对Master节点的读写，避免Master节点成为系统的瓶颈。客户端并不通过Master节点读写文件数据。反之，客户端向 Master节点询问它应该联系的Chunk服务器。客户端将这些元数据信息缓存一段时间，后续的操作将直接和Chunk服务器进行数据读写操作。 我们利用图1解释一下一次简单读取的流程。首先，客户端把文件名和程序指定的字节偏移，根据固定的Chunk大小，转换成文件的Chunk索 引。然后，它把文件名和Chunk索引发送给Master节点。Master节点将相应的Chunk标识和副本的位置信息发还给客户端。客户端用文件名和 Chunk索引作为key缓存这些信息。 之后客户端发送请求到其中的一个副本处，一般会选择最近的。请求信息包含了Chunk的标识和字节范围。在对这个Chunk的后续读取操作中， 客户端不必再和Master节点通讯了，除非缓存的元数据信息过期或者文件被重新打开。实际上，客户端通常会在一次请求中查询多个Chunk信 息，Master节点的回应也可能包含了紧跟着这些被请求的Chunk后面的Chunk的信息。在实际应用中，这些额外的信息在没有任何代价的情况下，避 免了客户端和Master节点未来可能会发生的几次通讯。 2.5 Chunk尺寸 Chunk的大小是关键的设计参数之一。我们选择了64MB，这个尺寸远远大于一般文件系统的Block size。每个Chunk的副本都以普通Linux文件的形式保存在Chunk服务器上，只有在需要的时候才扩大。惰性空间分配策略避免了因内部碎片造成 的空间浪费，内部碎片或许是对选择这么大的Chunk尺寸最具争议一点。 选择较大的Chunk尺寸有几个重要的优点。首先，它减少了客户端和Master节点通讯的需求，因为只需要一次和Mater节点的通信就可以 获取Chunk的位置信息，之后就可以对同一个Chunk进行多次的读写操作。这种方式对降低我们的工作负载来说效果显著，因为我们的应用程序通常是连续 读写大文件。即使是小规模的随机读取，采用较大的Chunk尺寸也带来明显的好处，客户端可以轻松的缓存一个数TB的工作数据集所有的Chunk位置信 息。其次，采用较大的Chunk尺寸，客户端能够对一个块进行多次操作，这样就可以通过与Chunk服务器保持较长时间的TCP连接来减少网络负载。第 三，选用较大的Chunk尺寸减少了Master节点需要保存的元数据的数量。这就允许我们把元数据全部放在内存中，在2.6.1节我们会讨论元数据全部 放在内存中带来的额外的好处。 另一方面，即使配合惰性空间分配，采用较大的Chunk尺寸也有其缺陷。小文件包含较少的Chunk，甚至只有一个Chunk。当有许多的客户 端对同一个小文件进行多次的访问时，存储这些Chunk的Chunk服务器就会变成热点。在实际应用中，由于我们的程序通常是连续的读取包含多个 Chunk的大文件，热点还不是主要的问题。 然而，当我们第一次把GFS用于批处理队列系统的时候，热点的问题还是产生了：一个可执行文件在GFS上保存为single-chunk文件， 之后这个可执行文件在数百台机器上同时启动。存放这个可执行文件的几个Chunk服务器被数百个客户端的并发请求访问导致系统局部过载。我们通过使用更大 的复制参数来保存可执行文件，以及错开批处理队列系统程序的启动时间的方法解决了这个问题。一个可能的长效解决方案是，在这种的情况下，允许客户端从其它 客户端读取数据。 2.6 元数据 Master服务器 （alex注：注意逻辑的Master节点和物理的Master服务器的区别。后续我们谈的是每个Master服务器的行为，如存储、内存等等，因此我们将全部使用物理名称）存 储3种主要类型的元数据，包括：文件和Chunk的命名空间、文件和Chunk的对应关系、每个Chunk副本的存放地点。所有的元数据都保存在 Master服务器的内存中。前两种类型的元数据（命名空间、文件和Chunk的对应关系）同时也会以记录变更日志的方式记录在操作系统的系统日志文件 中，日志文件存储在本地磁盘上，同时日志会被复制到其它的远程Master服务器上。采用保存变更日志的方式，我们能够简单可靠的更新Master服务器 的状态，并且不用担心Master服务器崩溃导致数据不一致的风险。Master服务器不会持久保存Chunk位置信息。Master服务器在启动时，或 者有新的Chunk服务器加入时，向各个Chunk服务器轮询它们所存储的Chunk的信息。 2.6.1 内存中的数据结构 因为元数据保存在内存中，所以Master服务器的操作速度非常快。并且，Master服务器可以在后台简单而高效的周期性扫描自己保存的全部 状态信息。这种周期性的状态扫描也用于实现Chunk垃圾收集、在Chunk服务器失效的时重新复制数据、通过Chunk的迁移实现跨Chunk服务器的 负载均衡以及磁盘使用状况统计等功能。4.3和4.4章节将深入讨论这些行为。 将元数据全部保存在内存中的方法有潜在问题：Chunk的数量以及整个系统的承载能力都受限于Master服务器所拥有的内存大小。但是在实际 应用中，这并不是一个严重的问题。Master服务器只需要不到64个字节的元数据就能够管理一个64MB的Chunk。由于大多数文件都包含多个 Chunk，因此绝大多数Chunk都是满的，除了文件的最后一个Chunk是部分填充的。同样的，每个文件的在命名空间中的数据大小通常在64字节以 下，因为保存的文件名是用前缀压缩算法压缩过的。 即便是需要支持更大的文件系统，为Master服务器增加额外内存的费用是很少的，而通过增加有限的费用，我们就能够把元数据全部保存在内存里，增强了系统的简洁性、可靠性、高性能和灵活性。 2.6.2 Chunk位置信息 Master服务器并不保存持久化保存哪个Chunk服务器存有指定Chunk的副本的信息。Master服务器只是在启动的时候轮询Chunk服 务器以获取这些信息。Master服务器能够保证它持有的信息始终是最新的，因为它控制了所有的Chunk位置的分配，而且通过周期性的心跳信息监控 Chunk服务器的状态。 最初设计时，我们试图把Chunk的位置信息持久的保存在Master服务器上，但是后来我们发现在启动的时候轮询Chunk服务器，之后定期轮询 更新的方式更简单。这种设计简化了在有Chunk服务器加入集群、离开集群、更名、失效、以及重启的时候，Master服务器和Chunk服务器数据同步 的问题。在一个拥有数百台服务器的集群中，这类事件会频繁的发生。 可以从另外一个角度去理解这个设计决策：只有Chunk服务器才能最终确定一个Chunk是否在它的硬盘上。我们从没有考虑过在Master服务器 上维护一个这些信息的全局视图，因为Chunk服务器的错误可能会导致Chunk自动消失(比如，硬盘损坏了或者无法访问了)，亦或者操作人员可能会重命 名一个Chunk服务器。 2.6.3 操作日志 操作日志包含了关键的元数据变更历史记录。这对GFS非常重要。这不仅仅是因为操作日志是元数据唯一的持久化存储记录，它也作为判断同步操作顺序的逻辑时间基线（alex注：也就是通过逻辑日志的序号作为操作发生的逻辑时间，类似于事务系统中的LSN）。文件和Chunk，连同它们的版本(参考4.5节)，都由它们创建的逻辑时间唯一的、永久的标识。 操作日志非常重要，我们必须确保日志文件的完整，确保只有在元数据的变化被持久化后，日志才对客户端是可见的。否则，即使Chunk本身没有出现任 何问题，我们仍有可能丢失整个文件系统，或者丢失客户端最近的操作。所以，我们会把日志复制到多台远程机器，并且只有把相应的日志记录写入到本地以及远程 机器的硬盘后，才会响应客户端的操作请求。Master服务器会收集多个日志记录后批量处理，以减少写入磁盘和复制对系统整体性能的影响。 Master服务器在灾难恢复时，通过重演操作日志把文件系统恢复到最近的状态。为了缩短Master启动的时间，我们必须使日志足够小 （alex注：即重演系统操作的日志量尽量的少）。Master服务器在日志增长到一定量时对系统状态做一次Checkpoint (alex注：Checkpoint是一种行为，一种对数据库状态作一次快照的行为)，将所有的状态数据写入一个Checkpoint文件 （alex注：并删除之前的日志文件）。 在灾难恢复的时候，Master服务器就通过从磁盘上读取这个Checkpoint文件，以及重演Checkpoint之后的有限个日志文件就能够恢复系统。Checkpoint文件以压缩B-树形势的数据结构存储，可以直接映射到内存，在用于命名空间查询时无需额外的解析。这大大提高了恢复速度，增强了可用性。 由于创建一个Checkpoint文件需要一定的时间，所以Master服务器的内部状态被组织为一种格式，这种格式要确保在Checkpoint 过程中不会阻塞正在进行的修改操作。Master服务器使用独立的线程切换到新的日志文件和创建新的Checkpoint文件。新的Checkpoint 文件包括切换前所有的修改。对于一个包含数百万个文件的集群，创建一个Checkpoint文件需要1分钟左右的时间。创建完成后，Checkpoint 文件会被写入在本地和远程的硬盘里。 Master服务器恢复只需要最新的Checkpoint文件和后续的日志文件。旧的Checkpoint文件和日志文件可以被删除，但是为了应对灾难性的故障（alex注：catastrophes，数据备份相关文档中经常会遇到这个词，表示一种超出预期范围的灾难性事件），我们通常会多保存一些历史文件。Checkpoint失败不会对正确性产生任何影响，因为恢复功能的代码可以检测并跳过没有完成的Checkpoint文件。 2.7 一致性模型 GFS支持一个宽松的一致性模型，这个模型能够很好的支撑我们的高度分布的应用，同时还保持了相对简单且容易实现的优点。本节我们讨论GFS的一致 性的保障机制，以及对应用程序的意义。我们也着重描述了GFS如何管理这些一致性保障机制，但是实现的细节将在本论文的其它部分讨论。 2.7.1 GFS一致性保障机制 文件命名空间的修改（例如，文件创建）是原子性的。它们仅由Master节点的控制：命名空间锁提供了原子性和正确性（4.1章）的保障；Master节点的操作日志定义了这些操作在全局的顺序（2.6.3章）。 谷歌三大核心技术（二）Google MapReduce中文版 数据修改后文件region（alex注：region这个词用中文非常难以表达，我认为应该是修改操作所涉及的文件中的某个范围）的 状态取决于操作的类型、成功与否、以及是否同步修改。表1总结了各种操作的结果。如果所有客户端，无论从哪个副本读取，读到的数据都一样，那么我们认为文 件region是“一致的”；如果对文件的数据修改之后，region是一致的，并且客户端能够看到写入操作全部的内容，那么这个region是“已定义 的”。当一个数据修改操作成功执行，并且没有受到同时执行的其它写入操作的干扰，那么影响的region就是已定义的（隐含了一致性）：所有的客户端都可 以看到写入的内容。并行修改操作成功完成之后，region处于一致的、未定义的状态：所有的客户端看到同样的数据，但是无法读到任何一次写入操作写入的 数据。通常情况下，文件region内包含了来自多个修改操作的、混杂的数据片段。失败的修改操作导致一个region处于不一致状态（同时也是未定义 的）：不同的客户在不同的时间会看到不同的数据。后面我们将描述应用如何区分已定义和未定义的region。应用程序没有必要再去细分未定义region 的不同类型。 数据修改操作分为写入或者记录追加两种。写入操作把数据写在应用程序指定的文件偏移位置上。即使有多个修改操作并行执行时，记录追加操作至少可以把数据原子性的追加到文件中一次，但是偏移位置是由GFS选择的（3.3章） （alex注：这句话有点费解，其含义是所有的追加写入都会成功，但是有可能被执行了多次，而且每次追加的文件偏移量由GFS自己计算）。 （相比而言，通常说的追加操作写的偏移位置是文件的尾部。）GFS返回给客户端一个偏移量，表示了包含了写入记录的、已定义的region的起点。另 外，GFS可能会在文件中间插入填充数据或者重复记录。这些数据占据的文件region被认定是不一致的，这些数据通常比用户数据小的多。 经过了一系列的成功的修改操作之后，GFS确保被修改的文件region是已定义的，并且包含最后一次修改操作写入的数据。GFS通过以下措施确保 上述行为：（a） 对Chunk的所有副本的修改操作顺序一致（3.1章），（b）使用Chunk的版本号来检测副本是否因为它所在的Chunk服务器宕机（4.5章）而错 过了修改操作而导致其失效。失效的副本不会再进行任何修改操作，Master服务器也不再返回这个Chunk副本的位置信息给客户端。它们会被垃圾收集系 统尽快回收。 由于Chunk位置信息会被客户端缓存，所以在信息刷新前，客户端有可能从一个失效的副本读取了数据。在缓存的超时时间和文件下一次被打开的时 间之间存在一个时间窗，文件再次被打开后会清除缓存中与该文件有关的所有Chunk位置信息。而且，由于我们的文件大多数都是只进行追加操作的，所以，一 个失效的副本通常返回一个提前结束的Chunk而不是过期的数据。当一个Reader （alex注：本文中将用到两个专有名词，Reader和Writer，分别表示执行GFS读取和写入操作的程序）重新尝试并联络Master服务器时，它就会立刻得到最新的Chunk位置信息。 即使在修改操作成功执行很长时间之后，组件的失效也可能损坏或者删除数据。GFS通过Master服务器和所有Chunk服务器的定期“握手” 来找到失效的Chunk服务器，并且使用Checksum来校验数据是否损坏（5.2章）。一旦发现问题，数据要尽快利用有效的副本进行恢复（4.3 章）。只有当一个Chunk的所有副本在GFS检测到错误并采取应对措施之前全部丢失，这个Chunk才会不可逆转的丢失。在一般情况下GFS的反应时间 （alex注：指Master节点检测到错误并采取应对措施）是几分钟。即使在这种情况下，Chunk也只是不可用了，而不是损坏了：应用程序会收到明确的错误信息而不是损坏的数据。 2.7.2 程序的实现 使用GFS的应用程序可以利用一些简单技术实现这个宽松的一致性模型，这些技术也用来实现一些其它的目标功能，包括：尽量采用追加写入而不是覆盖，Checkpoint，自验证的写入操作，自标识的记录。 在实际应用中，我们所有的应用程序对文件的写入操作都是尽量采用数据追加方式，而不是覆盖方式。一种典型的应用，应用程序从头到尾写入数据，生 成了一个文件。写入所有数据之后，应用程序自动将文件改名为一个永久保存的文件名，或者周期性的作Checkpoint，记录成功写入了多少数据。 Checkpoint文件可以包含程序级别的校验和。Readers仅校验并处理上个Checkpoint之后产生的文件region，这些文件 region的状态一定是已定义的。这个方法满足了我们一致性和并发处理的要求。追加写入比随机位置写入更加有效率，对应用程序的失败处理更具有弹性。 Checkpoint可以让Writer以渐进的方式重新开始，并且可以防止Reader处理已经被成功写入，但是从应用程序的角度来看还并未完成的数 据。 我们再来分析另一种典型的应用。许多应用程序并行的追加数据到同一个文件，比如进行结果的合并或者是一个生产者-消费者队列。记录追加方式的 “至少一次追加”的特性保证了Writer的输出。Readers使用下面的方法来处理偶然性的填充数据和重复内容。Writers在每条写入的记录中都 包含了额外的信息，例如Checksum，用来验证它的有效性。Reader可以利用Checksum识别和抛弃额外的填充数据和记录片段。如果应用不能 容忍偶尔的重复内容(比如，如果这些重复数据触发了非幂等操作)，可以用记录的唯一标识符来过滤它们，这些唯一标识符通常用于命名程序中处理的实体对象， 例如web文档。这些记录I/O功能 （alex注：These functionalities for record I/O）（除了剔除重复数据）都包含在我们的程序共享的库中，并且适用于Google内部的其它的文件接口实现。所以，相同序列的记录，加上一些偶尔出现的重复数据，都被分发到Reader了。 系统交互 我们在设计这个系统时，一个重要的原则是最小化所有操作和Master节点的交互。带着这样的设计理念，我们现在描述一下客户机、Master服务器和Chunk服务器如何进行交互，以实现数据修改操作、原子的记录追加操作以及快照功能。 3.1 租约（lease）和变更顺序 （alex注：lease是数据库中的一个术语） 变更是一个会改变Chunk内容或者元数据的操作，比如写入操作或者记录追加操作。变更操作会在Chunk的所有副本上执行。我们使用租约 （lease）机制来保持多个副本间变更顺序的一致性。Master节点为Chunk的一个副本建立一个租约，我们把这个副本叫做主Chunk。主 Chunk对Chunk的所有更改操作进行序列化。所有的副本都遵从这个序列进行修改操作。因此，修改操作全局的顺序首先由Master节点选择的租约的 顺序决定，然后由租约中主Chunk分配的序列号决定。 设计租约机制的目的是为了最小化Master节点的管理负担。租约的初始超时设置为60秒。不过，只要Chunk被修改了，主Chunk就可以 申请更长的租期，通常会得到Master节点的确认并收到租约延长的时间。这些租约延长请求和批准的信息通常都是附加在Master节点和Chunk服务 器之间的心跳消息中来传递。有时Master节点会试图提前取消租约（例如，Master节点想取消在一个已经被改名的文件上的修改操作）。即使 Master节点和主Chunk失去联系，它仍然可以安全地在旧的租约到期后和另外一个Chunk副本签订新的租约。 在图2中，我们依据步骤编号，展现写入操作的控制流程。 谷歌三大核心技术（二）Google MapReduce中文版 客户机向Master节点询问哪一个Chunk服务器持有当前的租约，以及其它副本的位置。如果没有一个Chunk持有租约，Master节点就选择其中一个副本建立一个租约（这个步骤在图上没有显示）。 Master节点将主Chunk的标识符以及其它副本（又称为secondary副本、二级副本）的位置返回给客户机。客户机缓存这些数据以便 后续的操作。只有在主Chunk不可用，或者主Chunk回复信息表明它已不再持有租约的时候，客户机才需要重新跟Master节点联系。 客户机把数据推送到所有的副本上。客户机可以以任意的顺序推送数据。Chunk服务器接收到数据并保存在它的内部LRU缓存中，一直到数据被使 用或者过期交换出去。由于数据流的网络传输负载非常高，通过分离数据流和控制流，我们可以基于网络拓扑情况对数据流进行规划，提高系统性能，而不用去理会 哪个Chunk服务器保存了主Chunk。3.2章节会进一步讨论这点。 当所有的副本都确认接收到了数据，客户机发送写请求到主Chunk服务器。这个请求标识了早前推送到所有副本的数据。主Chunk为接收到的所 有操作分配连续的序列号，这些操作可能来自不同的客户机，序列号保证了操作顺序执行。它以序列号的顺序把操作应用到它自己的本地状态中 （alex注：也就是在本地执行这些操作，这句话按字面翻译有点费解，也许应该翻译为“它顺序执行这些操作，并更新自己的状态”）。 主Chunk把写请求传递到所有的二级副本。每个二级副本依照主Chunk分配的序列号以相同的顺序执行这些操作。 所有的二级副本回复主Chunk，它们已经完成了操作。 主Chunk服务器 （alex注：即主Chunk所在的Chunk服务器）回 复客户机。任何副本产生的任何错误都会返回给客户机。在出现错误的情况下，写入操作可能在主Chunk和一些二级副本执行成功。（如果操作在主Chunk 上失败了，操作就不会被分配序列号，也不会被传递。）客户端的请求被确认为失败，被修改的region处于不一致的状态。我们的客户机代码通过重复执行失 败的操作来处理这样的错误。在从头开始重复执行之前，客户机会先从步骤（3）到步骤（7）做几次尝试。 如果应用程序一次写入的数据量很大，或者数据跨越了多个Chunk，GFS客户机代码会把它们分成多个写操作。这些操作都遵循前面描述的控制流 程，但是可能会被其它客户机上同时进行的操作打断或者覆盖。因此，共享的文件region的尾部可能包含来自不同客户机的数据片段，尽管如此，由于这些分 解后的写入操作在所有的副本上都以相同的顺序执行完成，Chunk的所有副本都是一致的。这使文件region处于2.7节描述的一致的、但是未定义的状 态。 3.2 数据流 为了提高网络效率，我们采取了把数据流和控制流分开的措施。在控制流从客户机到主Chunk、然后再到所有二级副本的同时，数据以管道的方式， 顺序的沿着一个精心选择的Chunk服务器链推送。我们的目标是充分利用每台机器的带宽，避免网络瓶颈和高延时的连接，最小化推送所有数据的延时。 为了充分利用每台机器的带宽，数据沿着一个Chunk服务器链顺序的推送，而不是以其它拓扑形式分散推送（例如，树型拓扑结构）。线性推送模式下，每台机器所有的出口带宽都用于以最快的速度传输数据，而不是在多个接受者之间分配带宽。 为了尽可能的避免出现网络瓶颈和高延迟的链接（eg，inter-switch最有可能出现类似问题），每台机器都尽量的在网络拓扑中选择一台 还没有接收到数据的、离自己最近的机器作为目标推送数据。假设客户机把数据从Chunk服务器S1推送到S4。它把数据推送到最近的Chunk服务器 S1。S1把数据推送到S2，因为S2和S4中最接近的机器是S2。同样的，S2把数据传递给S3和S4之间更近的机器，依次类推推送下去。我们的网络拓 扑非常简单，通过IP地址就可以计算出节点的“距离”。 最后，我们利用基于TCP连接的、管道式数据推送方式来最小化延迟。Chunk服务器接收到数据后，马上开始向前推送。管道方式的数据推送对我 们帮助很大，因为我们采用全双工的交换网络。接收到数据后立刻向前推送不会降低接收的速度。在没有网络拥塞的情况下，传送B字节的数据到R个副本的理想时 间是 B/T+RL ，T是网络的吞吐量，L是在两台机器数据传输的延迟。通常情况下，我们的网络连接速度是100Mbps（T），L将远小于1ms。因此，1MB的数据在理 想情况下80ms左右就能分发出去。 3.3 原子的记录追加 GFS提供了一种原子的数据追加操作–记录追加。传统方式的写入操作，客户程序会指定数据写入的偏移量。对同一个region的并行写入操作不 是串行的：region尾部可能会包含多个不同客户机写入的数据片段。使用记录追加，客户机只需要指定要写入的数据。GFS保证至少有一次原子的写入操作 成功执行（即写入一个顺序的byte流），写入的数据追加到GFS指定的偏移位置上，之后GFS返回这个偏移量给客户机。这类似于在Unix操作系统编程 环境中，对以O_APPEND模式打开的文件，多个并发写操作在没有竞态条件时的行为。 记录追加在我们的分布应用中非常频繁的使用，在这些分布式应用中，通常有很多的客户机并行地对同一个文件追加写入数据。如果我们采用传统方式的 文件写入操作，客户机需要额外的复杂、昂贵的同步机制，例如使用一个分布式的锁管理器。在我们的工作中，这样的文件通常用于多个生产者/单一消费者的队列 系统，或者是合并了来自多个客户机的数据的结果文件。 记录追加是一种修改操作，它也遵循3.1节描述的控制流程，除了在主Chunk有些额外的控制逻辑。客户机把数据推送给文件最后一个Chunk 的所有副本，之后发送请求给主Chunk。主Chunk会检查这次记录追加操作是否会使Chunk超过最大尺寸（64MB）。如果超过了最大尺寸，主 Chunk首先将当前Chunk填充到最大尺寸，之后通知所有二级副本做同样的操作，然后回复客户机要求其对下一个Chunk重新进行记录追加操作。（记 录追加的数据大小严格控制在Chunk最大尺寸的1/4，这样即使在最坏情况下，数据碎片的数量仍然在可控的范围。）通常情况下追加的记录不超过 Chunk的最大尺寸，主Chunk把数据追加到自己的副本内，然后通知二级副本把数据写在跟主Chunk一样的位置上，最后回复客户机操作成功。 如果记录追加操作在任何一个副本上失败了，客户端就需要重新进行操作。重新进行记录追加的结果是，同一个Chunk的不同副本可能包含不同的数 据–重复包含一个记录全部或者部分的数据。GFS并不保证Chunk的所有副本在字节级别是完全一致的。它只保证数据作为一个整体原子的被至少写入一次。 这个特性可以通过简单观察推导出来：如果操作成功执行，数据一定已经写入到Chunk的所有副本的相同偏移位置上。这之后，所有的副本至少都到了记录尾部 的长度，任何后续的记录都会追加到更大的偏移地址，或者是不同的Chunk上，即使其它的Chunk副本被Master节点选为了主Chunk。就我们的 一致性保障模型而言，记录追加操作成功写入数据的region是已定义的（因此也是一致的），反之则是不一致的（因此也就是未定义的）。正如我们在 2.7.2节讨论的，我们的程序可以处理不一致的区域。 3.4 快照 (alex注：这一节非常难以理解，总的来说依次讲述了什么是快照、快照使用的COW技术、快照如何不干扰当前操作) 快照操作几乎可以瞬间完成对一个文件或者目录树（“源”）做一个拷贝，并且几乎不会对正在进行的其它操作造成任何干扰。我们的用户可以使用快照 迅速的创建一个巨大的数据集的分支拷贝（而且经常是递归的拷贝拷贝），或者是在做实验性的数据操作之前，使用快照操作备份当前状态，这样之后就可以轻松的 提交或者回滚到备份时的状态。 就像AFS （alex注：AFS，即Andrew File System，一种分布式文件系统），我 们用标准的copy-on-write技术实现快照。当Master节点收到一个快照请求，它首先取消作快照的文件的所有Chunk的租约。这个措施保证 了后续对这些Chunk的写操作都必须与Master交互交互以找到租约持有者。这就给Master节点一个率先创建Chunk的新拷贝的机会。 租约取消或者过期之后，Master节点把这个操作以日志的方式记录到硬盘上。然后，Master节点通过复制源文件或者目录的元数据的方式，把这条日志记录的变化反映到保存在内存的状态中。新创建的快照文件和源文件指向完全相同的Chunk地址。 在快照操作之后，当客户机第一次想写入数据到Chunk C，它首先会发送一个请求到Master节点查询当前的租约持有者。Master节点注意到Chunke C的引用计数超过了1 (alex注：不太明白为什么会大于1.难道是Snapshot没有释放引用计数？)。 Master节点不会马上回复客户机的请求，而是选择一个新的Chunk句柄C。之后，Master节点要求每个拥有Chunk C当前副本的Chunk服务器创建一个叫做C的新Chunk。通过在源Chunk所在Chunk服务器上创建新的Chunk，我们确保数据在本地而不是 通过网络复制（我们的硬盘比我们的100Mb以太网大约快3倍）。从这点来讲，请求的处理方式和任何其它Chunk没什么不同：Master节点确保新 Chunk C`的一个副本拥有租约，之后回复客户机，客户机得到回复后就可以正常的写这个Chunk，而不必理会它是从一个已存在的Chunk克隆出来的。 Master节点的操作 Master节点执行所有的名称空间操作。此外，它还管理着整个系统里所有Chunk的副本：它决定Chunk的存储位置，创建新Chunk和 它的副本，协调各种各样的系统活动以保证Chunk被完全复制，在所有的Chunk服务器之间的进行负载均衡，回收不再使用的存储空间。本节我们讨论上述 的主题。 4.1 名称空间管理和锁 Master节点的很多操作会花费很长的时间：比如，快照操作必须取消Chunk服务器上快照所涉及的所有的Chunk的租约。我们不希望在这 些操作的运行时，延缓了其它的Master节点的操作。因此，我们允许多个操作同时进行，使用名称空间的region上的锁来保证执行的正确顺序。 不同于许多传统文件系统，GFS没有针对每个目录实现能够列出目录下所有文件的数据结构。GFS也不支持文件或者目录的链接（即Unix术语中 的硬链接或者符号链接）。在逻辑上，GFS的名称空间就是一个全路径和元数据映射关系的查找表。利用前缀压缩，这个表可以高效的存储在内存中。在存储名称 空间的树型结构上，每个节点（绝对路径的文件名或绝对路径的目录名）都有一个关联的读写锁。 每个Master节点的操作在开始之前都要获得一系列的锁。通常情况下，如果一个操作涉及/d1/d2/…/dn/leaf，那么操作首先要获 得目录/d1，/d1/d2，…，/d1/d2/…/dn的读锁，以及/d1/d2/…/dn/leaf的读写锁。注意，根据操作的不同，leaf可以是 一个文件，也可以是一个目录。 现在，我们演示一下在/home/user被快照到/save/user的时候，锁机制如何防止创建文件/home/user/foo。快照操 作获取/home和/save的读取锁，以及/home/user和/save/user的写入锁。文件创建操作获得/home和/home/user的 读取锁，以及/home/user/foo的写入锁。这两个操作要顺序执行，因为它们试图获取的/home/user的锁是相互冲突。文件创建操作不需要 获取父目录的写入锁，因为这里没有”目录”，或者类似inode等用来禁止修改的数据结构。文件名的读取锁足以防止父目录被删除。 采用这种锁方案的优点是支持对同一目录的并行操作。比如，可以再同一个目录下同时创建多个文件：每一个操作都获取一个目录名的上的读取锁和文件 名上的写入锁。目录名的读取锁足以的防止目录被删除、改名以及被快照。文件名的写入锁序列化文件创建操作，确保不会多次创建同名的文件。 因为名称空间可能有很多节点，读写锁采用惰性分配策略，在不再使用的时候立刻被删除。同样，锁的获取也要依据一个全局一致的顺序来避免死锁：首先按名称空间的层次排序，在同一个层次内按字典顺序排序。 4.2 副本的位置 GFS集群是高度分布的多层布局结构，而不是平面结构。典型的拓扑结构是有数百个Chunk服务器安装在许多机架上。Chunk服务器被来自同 一或者不同机架上的数百个客户机轮流访问。不同机架上的两台机器间的通讯可能跨越一个或多个网络交换机。另外，机架的出入带宽可能比机架内所有机器加和在 一起的带宽要小。多层分布架构对数据的灵活性、可靠性以及可用性方面提出特有的挑战。 Chunk副本位置选择的策略服务两大目标：最大化数据可靠性和可用性，最大化网络带宽利用率。为了实现这两个目的，仅仅是在多台机器上分别存 储这些副本是不够的，这只能预防硬盘损坏或者机器失效带来的影响，以及最大化每台机器的网络带宽利用率。我们必须在多个机架间分布储存Chunk的副本。 这保证Chunk的一些副本在整个机架被破坏或掉线（比如，共享资源，如电源或者网络交换机造成的问题）的情况下依然存在且保持可用状态。这还意味着在网 络流量方面，尤其是针对Chunk的读操作，能够有效利用多个机架的整合带宽。另一方面，写操作必须和多个机架上的设备进行网络通信，但是这个代价是我们 愿意付出的。 4.3 创建，重新复制，重新负载均衡 Chunk的副本有三个用途：Chunk创建，重新复制和重新负载均衡。 当Master节点创建一个Chunk时，它会选择在哪里放置初始的空的副本。Master节点会考虑几个因素。（1）我们希望在低于平均硬盘 使用率的Chunk服务器上存储新的副本。这样的做法最终能够平衡Chunk服务器之间的硬盘使用率。（2）我们希望限制在每个Chunk服务器上”最 近”的Chunk创建操作的次数。虽然创建操作本身是廉价的，但是创建操作也意味着随之会有大量的写入数据的操作，因为Chunk在Writer真正写入 数据的时候才被创建，而在我们的”追加一次，读取多次”的工作模式下，Chunk一旦写入成功之后就会变为只读的了。（3）如上所述，我们希望把 Chunk的副本分布在多个机架之间。 当Chunk的有效副本数量少于用户指定的复制因数的时候，Master节点会重新复制它。这可能是由几个原因引起的：一个Chunk服务器不 可用了，Chunk服务器报告它所存储的一个副本损坏了，Chunk服务器的一个磁盘因为错误不可用了，或者Chunk副本的复制因数提高了。每个需要被 重新复制的Chunk都会根据几个因素进行排序。一个因素是Chunk现有副本数量和复制因数相差多少。例如，丢失两个副本的Chunk比丢失一个副本的 Chunk有更高的优先级。另外，我们优先重新复制活跃（live）文件的Chunk而不是最近刚被删除的文件的Chunk（查看4.4节）。最后，为了 最小化失效的Chunk对正在运行的应用程序的影响，我们提高会阻塞客户机程序处理流程的Chunk的优先级。 Master节点选择优先级最高的Chunk，然后命令某个Chunk服务器直接从可用的副本”克隆”一个副本出来。选择新副本的位置的策略和 创建时类似：平衡硬盘使用率、限制同一台Chunk服务器上的正在进行的克隆操作的数量、在机架间分布副本。为了防止克隆产生的网络流量大大超过客户机的 流量，Master节点对整个集群和每个Chunk服务器上的同时进行的克隆操作的数量都进行了限制。另外，Chunk服务器通过调节它对源Chunk服 务器读请求的频率来限制它用于克隆操作的带宽。 最后，Master服务器周期性地对副本进行重新负载均衡：它检查当前的副本分布情况，然后移动副本以便更好的利用硬盘空间、更有效的进行负载 均衡。而且在这个过程中，Master服务器逐渐的填满一个新的Chunk服务器，而不是在短时间内用新的Chunk填满它，以至于过载。新副本的存储位 置选择策略和上面讨论的相同。另外，Master节点必须选择哪个副本要被移走。通常情况，Master节点移走那些剩余空间低于平均值的Chunk服务 器上的副本，从而平衡系统整体的硬盘使用率。 4.4 垃圾回收 GFS在文件删除后不会立刻回收可用的物理空间。GFS空间回收采用惰性的策略，只在文件和Chunk级的常规垃圾收集时进行。我们发现这个方法使系统更简单、更可靠。 4.4.1 机制 当一个文件被应用程序删除时，Master节点象对待其它修改操作一样，立刻把删除操作以日志的方式记录下来。但是，Master节点并不马上 回收资源，而是把文件名改为一个包含删除时间戳的、隐藏的名字。当Master节点对文件系统命名空间做常规扫描的时候，它会删除所有三天前的隐藏文件 （这个时间间隔是可以设置的）。直到文件被真正删除，它们仍旧可以用新的特殊的名字读取，也可以通过把隐藏文件改名为正常显示的文件名的方式“反删除”。 当隐藏文件被从名称空间中删除，Master服务器内存中保存的这个文件的相关元数据才会被删除。这也有效的切断了文件和它包含的所有Chunk的连接 （alex注：原文是This effectively severs its links to all its chunks）。 在对Chunk名字空间做类似的常规扫描时，Master节点找到孤儿Chunk（不被任何文件包含的Chunk）并删除它们的元数据。 Chunk服务器在和Master节点交互的心跳信息中，报告它拥有的Chunk子集的信息，Master节点回复Chunk服务器哪些Chunk在 Master节点保存的元数据中已经不存在了。Chunk服务器可以任意删除这些Chunk的副本。 4.4.2 讨论 虽然分布式垃圾回收在编程语言领域是一个需要复杂的方案才能解决的难题，但是在GFS系统中是非常简单的。我们可以轻易的得到Chunk的所有 引用：它们都只存储在Master服务器上的文件到块的映射表中。我们也可以很轻易的得到所有Chunk的副本：它们都以Linux文件的形式存储在 Chunk服务器的指定目录下。所有Master节点不能识别的副本都是”垃圾”。 垃圾回收在空间回收方面相比直接删除有几个优势。首先，对于组件失效是常态的大规模分布式系统，垃圾回收方式简单可靠。Chunk可能在某些 Chunk服务器创建成功，某些Chunk服务器上创建失败，失败的副本处于无法被Master节点识别的状态。副本删除消息可能丢失，Master节点 必须重新发送失败的删除消息，包括自身的和Chunk服务器的 （alex注：自身的指删除metadata的消息）。 垃圾回收提供了一致的、可靠的清除无用副本的方法。第二，垃圾回收把存储空间的回收操作合并到Master节点规律性的后台活动中，比如，例行扫描和与 Chunk服务器握手等。因此，操作被批量的执行，开销会被分散。另外，垃圾回收在Master节点相对空闲的时候完成。这样Master节点就可以给那 些需要快速反应的客户机请求提供更快捷的响应。第三，延缓存储空间回收为意外的、不可逆转的删除操作提供了安全保障。 根据我们的使用经验，延迟回收空间的主要问题是，延迟回收会阻碍用户调优存储空间的使用，特别是当存储空间比较紧缺的时候。当应用程序重复创建 和删除临时文件时，释放的存储空间不能马上重用。我们通过显式的再次删除一个已经被删除的文件的方式加速空间回收的速度。我们允许用户为命名空间的不同部 分设定不同的复制和回收策略。例如，用户可以指定某些目录树下面的文件不做复制，删除的文件被即时的、不可恢复的从文件系统移除。 4.5 过期失效的副本检测 当Chunk服务器失效时，Chunk的副本有可能因错失了一些修改操作而过期失效。Master节点保存了每个Chunk的版本号，用来区分当前的副本和过期副本。 无论何时，只要Master节点和Chunk签订一个新的租约，它就增加Chunk的版本号，然后通知最新的副本。Master节点和这些副本 都把新的版本号记录在它们持久化存储的状态信息中。这个动作发生在任何客户机得到通知以前，因此也是对这个Chunk开始写之前。如果某个副本所在的 Chunk服务器正好处于失效状态，那么副本的版本号就不会被增加。Master节点在这个Chunk服务器重新启动，并且向Master节点报告它拥有 的Chunk的集合以及相应的版本号的时候，就会检测出它包含过期的Chunk。如果Master节点看到一个比它记录的版本号更高的版本 号，Master节点会认为它和Chunk服务器签订租约的操作失败了，因此会选择更高的版本号作为当前的版本号。 Master节点在例行的垃圾回收过程中移除所有的过期失效副本。在此之前，Master节点在回复客户机的Chunk信息请求的时候，简单的 认为那些过期的块根本就不存在。另外一重保障措施是，Master节点在通知客户机哪个Chunk服务器持有租约、或者指示Chunk服务器从哪个 Chunk服务器进行克隆时，消息中都附带了Chunk的版本号。客户机或者Chunk服务器在执行操作时都会验证版本号以确保总是访问当前版本的数据。 5. 容错和诊断 我们在设计GFS时遇到的最大挑战之一是如何处理频繁发生的组件失效。组件的数量和质量让这些问题出现的频率远远超过一般系统意外发生的频率： 我们不能完全依赖机器的稳定性，也不能完全相信硬盘的可靠性。组件的失效可能造成系统不可用，更糟糕的是，还可能产生不完整的数据。我们讨论我们如何面对 这些挑战，以及当组件失效不可避免的发生时，用GFS自带工具诊断系统故障。 5.1 高可用性 在GFS集群的数百个服务器之中，在任何给定的时间必定会有些服务器是不可用的。我们使用两条简单但是有效的策略保证整个系统的高可用性：快速恢复和复制。 5.1.1 快速恢复 不管Master服务器和Chunk服务器是如何关闭的，它们都被设计为可以在数秒钟内恢复它们的状态并重新启动。事实上，我们并不区分正常关闭和异常关闭；通常，我们通过直接kill掉进程来关闭服务器。客户机和其它的服务器会感觉到系统有点颠簸 (alex注：a minor hiccup)，正在发出的请求会超时，需要重新连接到重启后的服务器，然后重试这个请求。6.6.2章节记录了实测的启动时间。 5.1.2 Chunk复制 正如之前讨论的，每个Chunk都被复制到不同机架上的不同的Chunk服务器上。用户可以为文件命名空间的不同部分设定不同的复制级别。缺省是 3。当有Chunk服务器离线了，或者通过Chksum校验（参考5.2节）发现了已经损坏的数据，Master节点通过克隆已有的副本保证每个 Chunk都被完整复制（alex注：即每个Chunk都有复制因子制定的个数个副本，缺省是3）。虽然Chunk复制策略对我们非常有效，但是我们也在寻找其它形式的跨服务器的冗余解决方案，比如使用奇偶校验、或者Erasure codes（alex注：Erasure codes用来解决链接层中不相关的错误，以及网络拥塞和buffer限制造成的丢包错误）来解决我们日益增长的只读存储需求。我们的系统主要的工作负载是追加方式的写入和读取操作，很少有随机的写入操作，因此，我们认为在我们这个高度解耦合的系统架构下实现这些复杂的冗余方案很有挑战性，但并非不可实现。 5.1.3 Master服务器的复制 为了保证Master服务器的可靠性，Master服务器的状态也要复制。Master服务器所有的操作日志和checkpoint文件都被复 制到多台机器上。对Master服务器状态的修改操作能够提交成功的前提是，操作日志写入到Master服务器的备节点和本机的磁盘。简单说来，一个 Master服务进程负责所有的修改操作，包括后台的服务，比如垃圾回收等改变系统内部状态活动。当它失效的时，几乎可以立刻重新启动。如果Master 进程所在的机器或者磁盘失效了，处于GFS系统外部的监控进程会在其它的存有完整操作日志的机器上启动一个新的Master进程。客户端使用规范的名字访 问Master（比如gfs-test）节点，这个名字类似DNS别名，因此也就可以在Master进程转到别的机器上执行时，通过更改别名的实际指向访 问新的Master节点。 此外，GFS中还有些“影子”Master服务器，这些“影子”服务器在“主”Master服务器宕机的时候提供文件系统的只读访问。它们是影子， 而不是镜像，所以它们的数据可能比“主”Master服务器更新要慢，通常是不到1秒。对于那些不经常改变的文件、或者那些允许获取的数据有少量过期的应 用程序，“影子”Master服务器能够提高读取的效率。事实上，因为文件内容是从Chunk服务器上读取的，因此，应用程序不会发现过期的文件内容。在 这个短暂的时间窗内，过期的可能是文件的元数据，比如目录的内容或者访问控制信息。 “影子”Master服务器为了保持自身状态是最新的，它会读取一份当前正在进行的操作的日志副本，并且依照和主Master服务器完全相同的顺序 来更改内部的数据结构。和主Master服务器一样，“影子”Master服务器在启动的时候也会从Chunk服务器轮询数据（之后定期拉数据），数据中 包括了Chunk副本的位置信息；“影子”Master服务器也会定期和Chunk服务器“握手”来确定它们的状态。在主Master服务器因创建和删除 副本导致副本位置信息更新时，“影子”Master服务器才和主Master服务器通信来更新自身状态。 5.2 数据完整性 每个Chunk服务器都使用Checksum来检查保存的数据是否损坏。考虑到一个GFS集群通常都有好几百台机器、几千块硬盘，磁盘损坏导致数据 在读写过程中损坏或者丢失是非常常见的（第7节讲了一个原因）。我们可以通过别的Chunk副本来解决数据损坏问题，但是跨越Chunk服务器比较副本来 检查数据是否损坏很不实际。另外，GFS允许有歧义的副本存在：GFS修改操作的语义，特别是早先讨论过的原子纪录追加的操作，并不保证副本完全相同(alex注：副本不是byte-wise完全一致的)。因此，每个Chunk服务器必须独立维护Checksum来校验自己的副本的完整性。 我们把每个Chunk都分成64KB大小的块。每个块都对应一个32位的Checksum。和其它元数据一样，Checksum与其它的用户数据是分开的，并且保存在内存和硬盘上，同时也记录操作日志。 对于读操作来说，在把数据返回给客户端或者其它的Chunk服务器之前，Chunk服务器会校验读取操作涉及的范围内的块的Checksum。因此 Chunk服务器不会把错误数据传递到其它的机器上。如果发生某个块的Checksum不正确，Chunk服务器返回给请求者一个错误信息，并且通知 Master服务器这个错误。作为回应，请求者应当从其它副本读取数据，Master服务器也会从其它副本克隆数据进行恢复。当一个新的副本就绪 后，Master服务器通知副本错误的Chunk服务器删掉错误的副本。 Checksum对读操作的性能影响很小，可以基于几个原因来分析一下。因为大部分的读操作都至少要读取几个块，而我们只需要读取一小部分额外的相 关数据进行校验。GFS客户端代码通过每次把读取操作都对齐在Checksum block的边界上，进一步减少了这些额外的读取操作的负面影响。另外，在Chunk服务器上，Chunksum的查找和比较不需要I/O操 作，Checksum的计算可以和I/O操作同时进行。 Checksum的计算针对在Chunk尾部的追加写入操作作了高度优化（与之对应的是覆盖现有数据的写入操作），因为这类操作在我们的工作中占了 很大比例。我们只增量更新最后一个不完整的块的Checksum，并且用所有的追加来的新Checksum块来计算新的Checksum。即使是最后一个 不完整的Checksum块已经损坏了，而且我们不能够马上检查出来，由于新的Checksum和已有数据不吻合，在下次对这个块进行读取操作的时候，会 检查出数据已经损坏了。 相比之下，如果写操作覆盖已经存在的一个范围内的Chunk，我们必须读取和校验被覆盖的第一个和最后一个块，然后再执行写操作；操作完成之后再重 新计算和写入新的Checksum。如果我们不校验第一个和最后一个被写的块，那么新的Checksum可能会隐藏没有被覆盖区域内的数据错误。 在Chunk服务器空闲的时候，它会扫描和校验每个不活动的Chunk的内容。这使得我们能够发现很少被读取的Chunk是否完整。一旦发现有 Chunk的数据损坏，Master可以创建一个新的、正确的副本，然后把损坏的副本删除掉。这个机制也避免了非活动的、已损坏的Chunk欺骗 Master节点，使Master节点认为它们已经有了足够多的副本了。 5.3 诊断工具 详尽的、深入细节的诊断日志，在问题隔离、调试、以及性能分析等方面给我们带来无法估量的帮助，同时也只需要很小的开销。没有日志的帮助，我们很难 理解短暂的、不重复的机器之间的消息交互。GFS的服务器会产生大量的日志，记录了大量关键的事件（比如，Chunk服务器启动和关闭）以及所有的RPC 的请求和回复。这些诊断日志可以随意删除，对系统的正确运行不造成任何影响。然而，我们在存储空间允许的情况下会尽量的保存这些日志。 RPC日志包含了网络上发生的所有请求和响应的详细记录，但是不包括读写的文件数据。通过匹配请求与回应，以及收集不同机器上的RPC日志记录，我们可以重演所有的消息交互来诊断问题。日志还用来跟踪负载测试和性能分析。 日志对性能的影响很小（远小于它带来的好处），因为这些日志的写入方式是顺序的、异步的。最近发生的事件日志保存在内存中，可用于持续不断的在线监控。 度量 本节中，我们将使用一些小规模基准测试来展现GFS系统架构和实现上的一些固有瓶颈，还有些来自Google内部使用的真实的GFS集群的基准数据。 6.1 小规模基准测试 我们在一个包含1台Master服务器，2台Master服务器复制节点，16台Chunk服务器和16个客户机组成的GFS集群上测量性能。注意，采用这样的集群配置方案只是为了易于测试。典型的GFS集群有数百个Chunk服务器和数百个客户机。 所有机器的配置都一样：两个PIII 1.4GHz处理器，2GB内存，两个80G/5400rpm的硬盘，以及100Mbps全双工以太网连接到一个HP2524交换机。GFS集群中所有的 19台服务器都连接在一个交换机，所有16台客户机连接到另一个交换机上。两个交换机之间使用1Gbps的线路连接。 6.1.1 读取 N个客户机从GFS文件系统同步读取数据。每个客户机从320GB的文件集合中随机读取4MB region的内容。读取操作重复执行256次，因此，每个客户机最终都读取1GB的数据。所有的Chunk服务器加起来总共只有32GB的内存，因此， 我们预期只有最多10%的读取请求命中Linux的文件系统缓冲。我们的测试结果应该和一个在没有文件系统缓存的情况下读取测试的结果接近。 谷歌三大核心技术（二）Google MapReduce中文版 图三：合计吞吐量：上边的曲线显示了我们网络拓扑下的合计理论吞吐量上限。下边的曲线显示了观测到的吞吐量。这个曲线有着95%的可靠性，因为有时候测量会不够精确。 图3（a）显示了N个客户机整体的读取速度以及这个速度的理论极限。当连接两个交换机的1Gbps的链路饱和时，整体读取速度达到理论的极限值 是125MB/S，或者说每个客户机配置的100Mbps网卡达到饱和时，每个客户机读取速度的理论极限值是12.5MB/s。实测结果是，当一个客户机 读取的时候，读取的速度是10MB/s，也就是说达到客户机理论读取速度极限值的80%。对于16个客户机，整体的读取速度达到了94MB/s，大约是理 论整体读取速度极限值的75%，也就是说每个客户机的读取速度是6MB/s。读取效率从80%降低到了75%，主要的原因是当读取的客户机增加时，多个客 户机同时读取一个Chunk服务器的几率也增加了，导致整体的读取效率下降。 6.1.2 写入 N个客户机同时向N个不同的文件中写入数据。每个客户机以每次1MB的速度连续写入1GB的数据。图3（b）显示了整体的写入速度和它们理论上 的极限值。理论上的极限值是67MB/s，因为我们需要把每一byte写入到16个Chunk服务器中的3个上，而每个Chunk服务器的输入连接速度是 12.5MB/s。 一个客户机的写入速度是6.3MB，大概是理论极限值的一半。导致这个结果的主要原因是我们的网络协议栈。它与我们推送数据到Chunk服务器时采用的管道模式不相适应。从一个副本到另一个副本的数据传输延迟降低了整个的写入速度。 16个客户机整体的写入速度达到了35MB/s（即每个客户机2.2MB/s），大约只是理论极限值的一半。和多个客户机读取的情形很类型，随 着客户机数量的增加，多个客户机同时写入同一个Chunk服务器的几率也增加了。而且，16个客户机并行写入可能引起的冲突比16个客户机并行读取要大得 多，因为每个写入都会涉及三个不同的副本。 写入的速度比我们想象的要慢。在实际应用中，这没有成为我们的主要问题，因为即使在单个客户机上能够感受到延时，它也不会在有大量客户机的时候对整体的写入带宽造成显著的影响。 6.1.3 记录追加 图3（c）显示了记录追加操作的性能。N个客户机同时追加数据到一个文件。记录追加操作的性能受限于保存文件最后一个Chunk的Chunk服务器 的带宽，而与客户机的数量无关。记录追加的速度由一个客户机的6.0MB/s开始，下降到16个客户机的4.8MB/s为止，速度的下降主要是由于不同客 户端的网络拥塞以及网络传输速度的不同而导致的。 我们的程序倾向于同时处理多个这样的文件。换句话说，即N个客户机同时追加数据到M个共享文件中，这里N和M都是数十或者数百以上。所以，在我们的 实际应用中，Chunk服务器的网络拥塞并没有成为一个严重问题，如果Chunk服务器的某个文件正在写入，客户机会去写另外一个文件。 6.2 实际应用中的集群 我们现在来仔细评估一下Google内部正在使用的两个集群，它们具有一定的代表性。集群A通常被上百个工程师用于研究和开发。典型的任务是被 人工初始化后连续运行数个小时。它通常读取数MB到数TB的数据，之后进行转化或者分析，最后把结果写回到集群中。集群B主要用于处理当前的生产数据。集 群B的任务持续的时间更长，在很少人工干预的情况下，持续的生成和处理数TB的数据集。在这两个案例中，一个单独的”任务”都是指运行在多个机器上的多个 进程，它们同时读取和写入多个文件。 谷歌三大核心技术（二）Google MapReduce中文版 6.2.1 存储 如上表前五行所描述的，两个集群都由上百台Chunk服务器组成，支持数TB的硬盘空间；两个集群虽然都存储了大量的数据，但是还有剩余的空间。 “已用空间”包含了所有的Chunk副本。实际上所有的文件都复制了三份。因此，集群实际上各存储了18TB和52TB的文件数据。 两个集群存储的文件数量都差不多，但是集群B上有大量的死文件。所谓“死文件”是指文件被删除了或者是被新版本的文件替换了，但是存储空间还没有来得及被回收。由于集群B存储的文件较大，因此它的Chunk数量也比较多。 6.2.2 元数据 Chunk服务器总共保存了十几GB的元数据，大多数是来自用户数据的、64KB大小的块的Checksum。保存在Chunk服务器上其它的元数据是Chunk的版本号信息，我们在4.5节描述过。 在Master服务器上保存的元数据就小的多了，大约只有数十MB，或者说平均每个文件100字节的元数据。这和我们设想的是一样的，Master 服务器的内存大小在实际应用中并不会成为GFS系统容量的瓶颈。大多数文件的元数据都是以前缀压缩模式存放的文件名。Master服务器上存放的其它元数 据包括了文件的所有者和权限、文件到Chunk的映射关系，以及每一个Chunk的当前版本号。此外，针对每一个Chunk，我们都保存了当前的副本位置 以及对它的引用计数，这个引用计数用于实现写时拷贝（alex注：即COW，copy-on-write）。 对于每一个单独的服务器，无论是Chunk服务器还是Master服务器，都只保存了50MB到100MB的元数据。因此，恢复服务器是非常快速 的：在服务器响应客户请求之前，只需要花几秒钟时间从磁盘上读取这些数据就可以了。不过，Master服务器会持续颠簸一段时间–通常是30到60秒–直 到它完成轮询所有的Chunk服务器，并获取到所有Chunk的位置信息。 6.2.3 读写速率 谷歌三大核心技术（二）Google MapReduce中文版 表三显示了不同时段的读写速率。在测试的时候，这两个集群都运行了一周左右的时间。（这两个集群最近都因为升级新版本的GFS重新启动过了）。 集群重新启动后，平均写入速率小于30MB/s。当我们提取性能数据的时候，集群B正进行大量的写入操作，写入速度达到了100MB/s，并且因为每个Chunk都有三个副本的原因，网络负载达到了300MB/s。 读取速率要比写入速率高的多。正如我们设想的那样，总的工作负载中，读取的比例远远高于写入的比例。两个集群都进行着繁重的读取操作。特别是， 集群A在一周时间内都维持了580MB/s的读取速度。集群A的网络配置可以支持750MB/s的速度，显然，它有效的利用了资源。集群B支持的峰值读取 速度是1300MB/s，但是它的应用只用到了380MB/s。 6.2.4 Master服务器的负载 表3的数据显示了发送到Master服务器的操作请求大概是每秒钟200到500个。Master服务器可以轻松的应付这个请求速度，所以Master服务器的处理能力不是系统的瓶颈。 在早期版本的GFS中，Master服务器偶尔会成为瓶颈。它大多数时间里都在顺序扫描某个很大的目录（包含数万个文件）去查找某个特定的文 件。因此我们修改了Master服务器的数据结构，通过对名字空间进行二分查找来提高效率。现在Master服务器可以轻松的每秒钟进行数千次文件访问。 如果有需要的话，我们可以通过在名称空间数据结构之前设置名称查询缓冲的方式进一步提高速度。 6.2.5 恢复时间 当某个Chunk服务器失效了，一些Chunk副本的数量可能会低于复制因子指定的数量，我们必须通过克隆副本使Chunk副本数量达到复制因 子指定的数量。恢复所有Chunk副本所花费的时间取决于资源的数量。在我们的试验中，我们把集群B上的一个Chunk服务器Kill掉。这个Chunk 服务器上大约有15000个Chunk，共计600GB的数据。为了减小克隆操作对正在运行的应用程序的影响，以及为GFS调度决策提供修正空间，我们缺 省的把集群中并发克隆操作的数量设置为91个（Chunk服务器的数量的40%），每个克隆操作最多允许使用的带宽是6.25MB/s（50mbps）。 所有的Chunk在23.2分钟内恢复了，复制的速度高达440MB/s。 在另外一个测试中，我们Kill掉了两个Chunk服务器，每个Chunk服务器大约有16000个Chunk，共计660GB的数据。这两个 故障导致了266个Chunk只有单个副本。这266个Chunk被GFS优先调度进行复制，在2分钟内恢复到至少有两个副本；现在集群被带入到另外一个 状态，在这个状态下，系统可以容忍另外一个Chunk服务器失效而不丢失数据。 6.3 工作负荷分析(Workload Breakdown) 本节中，我们展示了对两个GFS集群工作负载情况的详细分析，这两个集群和6.2节中的类似，但是不完全相同。集群X用于研究和开发，集群Y用于生产数据处理。 6.3.1 方法论和注意事项 本章节列出的这些结果数据只包括客户机发起的原始请求，因此，这些结果能够反映我们的应用程序对GFS文件系统产生的全部工作负载。它们不包含 那些为了实现客户端请求而在服务器间交互的请求，也不包含GFS内部的后台活动相关的请求，比如前向转发的写操作，或者重新负载均衡等操作。 我们从GFS服务器记录的真实的RPC请求日志中推导重建出关于IO操作的统计信息。例如，GFS客户程序可能会把一个读操作分成几个RPC请 求来提高并行度，我们可以通过这些RPC请求推导出原始的读操作。因为我们的访问模式是高度程式化，所以我们认为任何不符合的数据都是误差 (alex注：Since our access patterns are highly stylized, we expect any error to be in the noise)。应用程序如果能够记录更详尽的日志，就有可能提供更准确的诊断数据；但是为了这个目的去重新编译和重新启动数千个正在运行的客户机是不现实的，而且从那么多客户机上收集结果也是个繁重的工作。 应该避免从我们的工作负荷数据中过度的归纳出普遍的结论 (alex注：即不要把本节的数据作为基础的指导性数据)。因为Google完全控制着GFS和使用GFS的应用程序，所以，应用程序都针对GFS做了优化，同时，GFS也是为了这些应用程序而设计的。这样的相互作用也可能存在于一般程序和文件系统中，但是在我们的案例中这样的作用影响可能更显著。 6.3.2 Chunk服务器工作负荷 谷歌三大核心技术（二）Google MapReduce中文版 表4显示了操作按涉及的数据量大小的分布情况。读取操作按操作涉及的数据量大小呈现了双峰分布。小的读取操作（小于64KB）一般是由查找操作的客户端发起的，目的在于从巨大的文件中查找小块的数据。大的读取操作（大于512KB）一般是从头到尾顺序的读取整个文件。 在集群Y上，有相当数量的读操作没有返回任何的数据。在我们的应用中，尤其是在生产系统中，经常使用文件作为生产者-消费者队列。生产者并行的向文 件中追加数据，同时，消费者从文件的尾部读取数据。某些情况下，消费者读取的速度超过了生产者写入的速度，这就会导致没有读到任何数据的情况。集群X通常 用于短暂的数据分析任务，而不是长时间运行的分布式应用，因此，集群X很少出现这种情况。 写操作按数据量大小也同样呈现为双峰分布。大的写操作（超过256KB）通常是由于Writer使用了缓存机制导致的。Writer缓存较小的数据，通过频繁的Checkpoint或者同步操作，或者只是简单的统计小的写入（小于64KB）的数据量(alex注：即汇集多次小的写入操作，当数据量达到一个阈值，一次写入)，之后批量写入。 再来观察一下记录追加操作。我们可以看到集群Y中大的记录追加操作所占比例比集群X多的多，这是因为集群Y用于我们的生产系统，针对GFS做了更全面的调优。 谷歌三大核心技术（二）Google MapReduce中文版 表5显示了按操作涉及的数据量的大小统计出来的总数据传输量。在所有的操作中，大的操作（超过256KB）占据了主要的传输量。小的读取（小于64KB）虽然传输的数据量比较少，但是在读取的数据量中仍占了相当的比例，这是因为在文件中随机Seek的工作负荷而导致的。 6.3.3 记录追加 vs. 写操作 记录追加操作在我们的生产系统中大量使用。对于集群X，记录追加操作和普通写操作的比例按照字节比是108:1，按照操作次数比是8:1。对于 作为我们的生产系统的集群Y来说，这两个比例分别是3.7:1和2.5:1。更进一步，这一组数据说明在我们的两个集群上，记录追加操作所占比例都要比写 操作要大。对于集群X，在整个测量过程中，记录追加操作所占比率都比较低，因此结果会受到一两个使用某些特定大小的buffer的应用程序的影响。 如同我们所预期的，我们的数据修改操作主要是记录追加操作而不是覆盖方式的写操作。我们测量了第一个副本的数据覆盖写的情况。这近似于一个客户机故 意覆盖刚刚写入的数据，而不是增加新的数据。对于集群X，覆盖写操作在写操作所占据字节上的比例小于0.0001%，在所占据操作数量上的比例小于 0.0003%。对于集群Y，这两个比率都是0.05%。虽然这只是某一片断的情况，但是仍然高于我们的预期。这是由于这些覆盖写的操作，大部分是由于客 户端在发生错误或者超时以后重试的情况。这在本质上应该不算作工作负荷的一部分，而是重试机制产生的结果。 6.3.4 Master的工作负荷 谷歌三大核心技术（二）Google MapReduce中文版 表6显示了Master服务器上的请求按类型区分的明细表。大部分的请求都是读取操作查询Chunk位置信息（FindLocation）、以及修改操作查询lease持有者的信息（FindLease-Locker）。 集群X和Y在删除请求的数量上有着明显的不同，因为集群Y存储了生产数据，一般会重新生成数据以及用新版本的数据替换旧有的数据。数量上的差异 也被隐藏在了Open请求中，因为旧版本的文件可能在以重新写入的模式打开时，隐式的被删除了（类似UNIX的open函数中的“w”模式）。 FindMatchingFiles是一个模式匹配请求，支持“ls”以及其它类似的文件系统操作。不同于Master服务器的其它请求，它可 能会检索namespace的大部分内容，因此是非常昂贵的操作。集群Y的这类请求要多一些，因为自动化数据处理的任务进程需要检查文件系统的各个部分， 以便从全局上了解应用程序的状态。与之不同的是，集群X的应用程序更加倾向于由单独的用户控制，通常预先知道自己所需要使用的全部文件的名称。 经验 在建造和部署GFS的过程中，我们经历了各种各样的问题，有些是操作上的，有些是技术上的。 起初，GFS被设想为我们的生产系统的后端文件系统。随着时间推移，在GFS的使用中逐步的增加了对研究和开发任务的支持。我们开始增加一些小 的功能，比如权限和配额，到了现在，GFS已经初步支持了这些功能。虽然我们生产系统是严格受控的，但是用户层却不总是这样的。需要更多的基础架构来防止 用户间的相互干扰。 我们最大的问题是磁盘以及和Linux相关的问题。很多磁盘都声称它们支持某个范围内的Linux IDE硬盘驱动程序，但是实际应用中反映出来的情况却不是这样，它们只支持最新的驱动。因为协议版本很接近，所以大部分磁盘都可以用，但是偶尔也会有由于 协议不匹配，导致驱动和内核对于驱动器的状态判断失误。这会导致数据因为内核中的问题意外的被破坏了。这个问题促使我们使用Checksum来校验数据， 同时我们也修改内核来处理这些因为协议不匹配带来的问题。 较早的时候，我们在使用Linux 2.2内核时遇到了些问题，主要是fsync()的效率问题。它的效率与文件的大小而不是文件修改部分的大小有关。这在我们的操作日志文件过大时给出了难 题，尤其是在我们尚未实现Checkpoint的时候。我们费了很大的力气用同步写来解决这个问题，但是最后还是移植到了Linux2.4内核上。 另一个和Linux相关的问题是单个读写锁的问题，也就是说，在某一个地址空间的任意一个线程都必须在从磁盘page in（读锁）的时候先hold住，或者在mmap()调用（写锁）的时候改写地址空间。我们发现即使我们的系统负载很轻的情况下也会有偶尔的超时，我们花 费了很多的精力去查找资源的瓶颈或者硬件的问题。最后我们终于发现这个单个锁在磁盘线程交换以前映射的数据到磁盘的时候，锁住了当前的网络线程，阻止它把 新数据映射到内存。由于我们的性能主要受限于网络接口，而不是内存copy的带宽，因此，我们用pread()替代mmap()，用了一个额外的copy 动作来解决这个问题。 尽管偶尔还是有其它的问题，Linux的开放源代码还是使我们能够快速探究和理解系统的行为。在适当的时候，我们会改进内核并且和公开源码组织共享这些改动。 相关工作 和其它的大型分布式文件系统，比如AFS[5]类似，GFS提供了一个与位置无关的名字空间，这使得数据可以为了负载均衡或者灾难冗余等目的在 不同位置透明的迁移。不同于AFS的是，GFS把文件分布存储到不同的服务器上，这种方式更类似Xfs[1]和Swift[3]，这是为了提高整体性能以 及灾难冗余的能力。 由于磁盘相对来说比较便宜，并且复制的方式比RAID[9]方法简单的多，GFS目前只使用复制的方式来进行冗余，因此要比xFS或者Swift占用更多的裸存储空间 (alex注：Raw storage，裸盘的空间)。 与AFS、xFS、Frangipani[12]以及Intermezzo[6]等文件系统不同的是，GFS并没有在文件系统层面提供任何 Cache机制。我们主要的工作在单个应用程序执行的时候几乎不会重复读取数据，因为它们的工作方式要么是流式的读取一个大型的数据集，要么是在大型的数 据集中随机Seek到某个位置，之后每次读取少量的数据。 某些分布式文件系统，比如Frangipani、xFS、Minnesota’s GFS[11]、GPFS[10]，去掉了中心服务器，只依赖于分布式算法来保证一致性和可管理性。我们选择了中心服务器的方法，目的是为了简化设计，增 加可靠性，能够灵活扩展。特别值得一提的是，由于处于中心位置的Master服务器保存有几乎所有的Chunk相关信息，并且控制着Chunk的所有变 更，因此，它极大地简化了原本非常复杂的Chunk分配和复制策略的实现方法。我们通过减少Master服务器保存的状态信息的数量，以及将Master 服务器的状态复制到其它节点来保证系统的灾难冗余能力。扩展能力和高可用性（对于读取）目前是通过我们的影子Master服务器机制来保证的。对 Master服务器状态更改是通过预写日志的方式实现持久化。为此，我们可以调整为使用类似Harp[7]中的primary-copy方案，从而提供比 我们现在的方案更严格的一致性保证。 我们解决了一个难题，这个难题类似Lustre[8]在如何在有大量客户端时保障系统整体性能遇到的问题。不过，我们通过只关注我们的应用程序 的需求，而不是提供一个兼容POSIX的文件系统，从而达到了简化问题的目的。此外，GFS设计预期是使用大量的不可靠节点组建集群，因此，灾难冗余方案 是我们设计的核心。 GFS很类似NASD架构[4]。NASD架构是基于网络磁盘的，而GFS使用的是普通计算机作为Chunk服务器，就像NASD原形中方案一 样。所不同的是，我们的Chunk服务器采用惰性分配固定大小的Chunk的方式，而不是分配变长的对象存储空间。此外，GFS实现了诸如重新负载均衡、 复制、恢复机制等等在生产环境中需要的特性。 不同于与Minnesota’s GFS和NASD，我们并不改变存储设备的Model (alex注：对这两个文件系统不了解，因为不太明白改变存储设备的Model用来做什么，这不明白这个model是模型、还是型号)。我们只关注用普通的设备来解决非常复杂的分布式系统日常的数据处理。 我们通过原子的记录追加操作实现了生产者-消费者队列，这个问题类似River[2]中的分布式队列。River使用的是跨主机的、基于内存的 分布式队列，为了实现这个队列，必须仔细控制数据流；而GFS采用可以被生产者并发追加记录的持久化的文件的方式实现。River模式支持m-到-n的分 布式队列，但是缺少由持久化存储提供的容错机制，GFS只支持m-到-1的队列。多个消费者可以同时读取一个文件，但是它们输入流的区间必须是对齐的。 结束语 Google文件系统展示了一个使用普通硬件支持大规模数据处理的系统的特质。虽然一些设计要点都是针对我们的特殊的需要定制的，但是还是有很多特性适用于类似规模的和成本的数据处理任务。 首先，我们根据我们当前的和可预期的将来的应用规模和技术环境来评估传统的 文件系统的特性。我们的评估结果将我们引导到一个使用完全不同于传统的设计思路上。根据我们的设计思路，我们认为组件失效是常态而不是异常，针对采用追加 方式（有可能是并发追加）写入、然后再读取（通常序列化读取）的大文件进行优化，以及扩展标准文件系统接口、放松接口限制来改进整个系统。 我们系统通过持续监控，复制关键数据，快速和自动恢复提供灾难冗余。 Chunk复制使得我们可以对Chunk服务器的失效进行容错。高频率的组件失效要求系统具备在线修复机制，能够周期性的、透明的修复损坏的数据，也能够 第一时间重新建立丢失的副本。此外，我们使用Checksum在磁盘或者IDE子系统级别检测数据损坏，在这样磁盘数量惊人的大系统中，损坏率是相当高 的。 我们的设计保证了在有大量的并发读写操作时能够提供很高的合计吞吐量。我们通过分离控制流和数据流来实现这个目标，控制流在Master服务器处理，而数据流在Chunk服务器和客户端处理。当一般的操作涉及到Master服务器时，由于GFS选择的Chunk尺寸较大(alex注：从而减小了元数据的大小)，以及通过Chunk Lease将控制权限移交给主副本，这些措施将Master服务器的负担降到最低。这使得一个简单、中心的Master不会成为成为瓶颈。我们相信我们对网络协议栈的优化可以提升当前对于每客户端的写入吞吐量限制。 GFS成功的实现了我们对存储的需求，在Google内部，无论是作为研究和开发的存储平台，还是作为生产系统的数据处理平台，都得到了广泛的应用。它是我们持续创新和处理整个WEB范围内的难题的一个重要工具。 致谢 We wish to thankt he following people for their contributions to the system or the paper. Brain Bershad (our shepherd) and the anonymous reviewers gave us valuable comments and suggestions. Anurag Acharya, Jeff Dean, and David des-Jardins contributed to the early design. Fay Chang worked on comparison of replicas across chunkservers. Guy Edjlali worked on storage quota. Markus Gutschke worked on a testing frameworkan d security enhancements. David Kramer worked on performance enhancements. Fay Chang, Urs Hoelzle, Max Ibel, Sharon Perl, Rob Pike, and Debby Wallach commented on earlier drafts of the paper. Many of our colleagues at Google bravely trusted their data to a new file system and gave us useful feedback. Yoshka helped with early testing. 参考 [1] Thomas Anderson, Michael Dahlin, Jeanna Neefe, David Patterson, Drew Roselli, and Randolph Wang. Serverless networkfil e systems. In Proceedings of the 15th ACM Symposium on Operating System Principles, pages 109–126, Copper Mountain Resort, Colorado, December 1995. [2] Remzi H. Arpaci-Dusseau, Eric Anderson, Noah Treuhaft, David E. Culler, Joseph M. Hellerstein, David Patterson, and Kathy Yelick. Cluster I/O with River: Making the fast case common. In Proceedings of the Sixth Workshop on Input/Output in Parallel and Distributed Systems (IOPADS ’99), pages 10–22, Atlanta, Georgia, May 1999. [3] Luis-Felipe Cabrera and Darrell D. E. Long. Swift: Using distributed disks triping to provide high I/O data rates. Computer Systems, 4(4):405–436, 1991. [4] Garth A. Gibson, David F. Nagle, Khalil Amiri, Jeff Butler, Fay W. Chang, Howard Gobioff, Charles Hardin, ErikR iedel, David Rochberg, and Jim Zelenka. A cost-effective, high-bandwidth storage architecture. In Proceedings of the 8th Architectural Support for Programming Languages and Operating Systems, pages 92–103, San Jose, California, October 1998. [5] John Howard, Michael Kazar, Sherri Menees, David Nichols, Mahadev Satyanarayanan, Robert Sidebotham, and Michael West. Scale and performance in a distributed file system. ACM Transactions on Computer Systems, 6(1):51–81, February 1988. [6] InterMezzo. http://www.inter-mezzo.org , 2003. [7] Barbara Liskov, Sanjay Ghemawat, Robert Gruber, Paul Johnson, Liuba Shrira, and Michael Williams. Replication in the Harp file system. In 13th Symposium on Operating System Principles, pages 226–238, Pacific Grove, CA, October 1991. [8] Lustre. http://www.lustreorg , 2003. [9] David A. Patterson, Garth A. Gibson, and Randy H. Katz. A case for redundant arrays of inexpensive disks (RAID). In Proceedings of the 1988 ACM SIGMOD International Conference on Management of Data, pages 109–116, Chicago, Illinois, September 1988. [10] FrankS chmuck and Roger Haskin. GPFS: A shared-diskfi le system for large computing clusters. In Proceedings of the First USENIX Conference on File and Storage Technologies, pages 231–244, Monterey, California, January 2002. [11] Steven R. Soltis, Thomas M. Ruwart, and Matthew T.O’Keefe. The Gobal File System. In Proceedings of the Fifth NASA Goddard Space Flight Center Conference on Mass Storage Systems and Technologies, College Park, Maryland, September 1996. [12] Chandramohan A. Thekkath, Timothy Mann, and Edward K. Lee. Frangipani: A scalable distributed file system. In Proceedings of the 16th ACM Symposium on Operating System Principles, pages 224–237, Saint-Malo, France, October 1997]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[大规模集群数据处理-分布式文件系统]]></title>
      <url>%2Farticle%2F%E5%A4%A7%E8%A7%84%E6%A8%A1%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86-%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F.html</url>
      <content type="text"><![CDATA[课程简介 课程主要内容： Hadoop MapReduce Spark 2-3人一组搭建实验环境 成绩构成： 作业占20分，实验1：10分，实验2：20分，实验3：20分，最后项目：30分。 第二周到最后在教学实验室上课。 Lab1：10，倒排表 Lab2：20，PageRank Lab3：20，聚类 Project：30， 自选题目，集中展示打分 课程关键词：时间、GFS、MapReduce + PageRank、KVS, BigTable &lt;- HBase、Spark, Clusteing、Hadoop、HDFS、MapReduce、Spark、HBase 分布式文件系统 文件系统基础知识回顾 文件系统的基本功能 文件系统是用户以及应用程序与底层的磁盘系统的接口。文件系统给上层的应用提供了以目录树为组织方式的名字空间，而底层则是以磁盘块数组为接口的磁盘 FS: 文件名 -》 地址， 即实现文件名到地址的转换 目录树:支持增加，删除，遍历，读写，等功能 o/r/w/c 磁盘数据块 512B 磁盘顺序读写的速度100MB/s, 乱序读写：1MB/s 寻址与定位 应用程序访问文件-》文件系统将文件名翻译为磁盘的具体位置-》磁盘转到对应磁道，定位磁头，完成读写 文件系统的组织 目录树的组织，目录操作，文件读写操作，文件系统元数据，文件系统数据库，文件系统数据块以及文件系统缓存，文件系统的安全策略。 虚拟文件系统 文件系统元数据 文件系统的元数据是对于数据的描述，而不是数据的本身 文件系统的元数据能够反应文件的特征，并对数据的读写具有重要意义 文件系统中针对文件的典型元数据： 文件在目录树中的目录名以及文件名 文件的大小以及在磁盘上的分布情况 文件的访问时间，修改时间 文件的用户数据等 文件系统总体的元数据 文件系统的编码信息 文件系统的格式化的信息 文件系统的可用空间等相关信息 文件系统数据块，磁盘一个簇512B,文件系统一般按照一块4096B（4KB）写入和读出数据 分布式文件系统 Google文件系统 需求 分布式写 一次写多次读 对吞吐率要求高，对延迟不敏感 基本设计 数据块：64MB 可靠性：副本，3个节点以上 系统设计简化：Matser单节点存储文件系统的元数据 没有数据缓存 追加Append操作 论文 A file region is consistent if all clients will always see the same data, regardless of which replicas they read from. 如果所有客户端，无论从哪个副本读取，读到的数据都一样，那么我们认为文 件region是“一致的”； A region is defined after a file data mutation if it is consistent and clients will see what the mutation writes in its entirety. 如果对文件的数据修改之后，region是一致的，并且客户端能够看到写入操作全部的内容，那么这个region是“已定义 的”。 当一个数据修改操作成功执行，并且没有受到同时执行的其它写入操作的干扰，那么影响的region就是已定义的（隐含了一致性）：所有的客户端读取的恰好是写入的内容。 并行修改操作成功完成之后，region处于一致的、未定义的状态：所有的客户端读取到的是同样的数据，但是无法读到任何一次写入操作写入的数据。 通常情况下，文件region内包含了来自多个修改操作的、混杂的数据片段。失败的修改操作导致一个region处于不一致状态（同时也是未定义 的）：不同的客户在不同的时间会看到不同的数据。后面我们将描述应用如何区分已定义和未定义的region。应用程序没有必要再去细分未定义region 的不同类型。 Write操作和记录追加操作 定位机器，定位磁盘 string -》 | node -》 本地文件系统Data Reliability：Master 损坏 可靠性 —》 备份（Shadow） chunkServer的备份？ Assumption，独立同分布 Consistency： 一致性 Performance； Load balance， scalability（增加机器优化性能） Performance ChunkServer 容量负载均衡 Random 分配给剩余空间最大的ChunkServer Hot Data Master 性能瓶颈 内存， 掉电 10TB -》 10MB Cache MetaDaba - 一致性 10PB -&gt; 64MB块 块的元数据64B， \(2^{32}-1 = 4G\) 40亿 10GB 带宽，延迟 DDR4 20GB/s,]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Massive Data Process Homework 1]]></title>
      <url>%2Farticle%2FMassive-Data-Process-Homework-1.html</url>
      <content type="text"><![CDATA[分布式文件系统（The Google File System）仔细阅读Google File System这篇论文，回答下面的问题。 简述在分布式文件系统中维护数据块一致的步骤。 简述在分布式文件系统中的删除文件的过程。 在分布式文件系统中，Append操作为什么被认为是defined interspersed with inconsistent，并发写为什么会出现undefined的情况。 简述在分布式文件系统中维护数据块一致的步骤。 分布式文件系统中通过以下措施维护数据块一致性： 在所有副本上按相同的顺序执行一个块上的修改操作 使用版本号来检测并复制过期文件，这种过期可能是由于块服务器宕机而造成了部分修改丢失引起的。过期的副本不会再涉及修改操作，主节点也不会将该副本返回给客户端。它们会尽快的进行垃圾回收操作。 而这种措施的实现是由于分布式文件系统完成修改操作时遵循了lease mechanism，通过该机制来确保多个副本间变更顺序的一致性。Master节点为Chunk的一个副本建立一个租约，我们把这个副本叫做Primary Chunk。Primary Chunk对Chunk的所有更改操作进行序列化。所有的副本都遵从这个序列进行修改操作。因此，修改操作全局的顺序首先由Master节点选择的租约的顺序决定，然后由租约中主Chunk分配的序列号决定。通过这样的方式，在完成修改操作时所有的Chunk节点及其副本是经过了同一个修改序列进行的，因此当修改操作完成时数据块是一致的。 初始数据一致 修改操作都是一致的 操作顺序是一致的 简述在分布式文件系统中的删除文件的过程 分布式文件系统中通过垃圾回收机制实现“慢删除文件操作”, 当一个文件被应用程序删除时，Master 将该删除操作以log方式记录下来，并且把文件名修改为包含删除时间戳的、隐藏的文件名，Master节点进行定期的常规文件系统命名空间扫描时，会删除已经被隐藏的超过3天的文件(时间间隔可自定义)， 在隐藏文件被删除前仍然可以通过特殊的名字进行读取，也可以通过重命名为普通文件名而实现撤销删除操作的功能。当隐藏文件从命名空间中删除之后，它在内存中的元数据也会被清除。 在对Chunk Server的命名空间做类似的扫描时，Master标识出孤儿Chunk块（比如：任何文件都无法访问到的块），并将那些块的元数据清除。在Chunk服务器与Master节点进行定期的心跳信息交换时，报告其拥有的Chunk子集的信息，Master回复Chunk服务器哪些块在Master节点保存的元数据中已经不存在了，Chunk服务器将释放并删除这些块的副本。 也就是说在分布式文件系统中，删除文件时，先在Master节点中删除对应的元数据信息，然后Chunk 服务器再通过Master和Chunk服务器之间的心跳信息收发待删除数据块信息，实现垃圾回收，最终删除文件。 在分布式文件系统中，Append操作为什么被认为是defined interspersed with inconsistent，并发写为什么会出现undefined的情况。 分布式文件系统的写操作流程解析 图1是写操作的控制流和数据流： 写操作控制流和数据流 分布式系统中的数据修改详细步骤如下： 客户端询问Master哪个块服务器持有这个块的当前租约，以及这个块的其它副本位置。如果没有一个租约，则Master选择一个副本并授予一个租约。 Master回复客户端primary的标识，以及其它（secondary）副本的位置。客户端缓存这些数据用于以后的修改操作。只有当primary不可达或者接收到primary不再持有租约时才需要再一次请求主节点。 客户端将数据推送到所有的副本。一个客户端能够以任意顺序进行推送。每个块服务器将数据保存在内部的LRU缓存中，直到数据被使用或者过期被替换掉。通过对数据流和控制流的分流，我们能够通过基于网络拓扑来调度数据流，不管哪个块服务器为primary，以此提高性能。 一旦所有的副本都确认接收到了数据，客户端将向primary发送一个写请求。这个请求确定了之前的数据被推送到了所有副本。Primary为接收到的所有修改操作分配连续的序列号，这些操作可能来自多个客户端，序列号提供了严格的序列化，应用按序列号顺序执行修改操作，进而改变自己的状态。 Primary将写请求发送到所有的secondary副本上。每个secondary副本按照primary分配的相同的序列号顺序执行这些修改操作。 Secondary副本回复primary，表示它们已经完成了所有的操作。 Primary回复客户端。任意副本上的任意错误都将报告给客户端。在一些错误情况下，写操作可能在primary和一些secondary副本上执行成功。（如果失败发生在primary，它将不会分片一个序列号，并且不会被传递。）客户端的请求被视为已经失败，这个修改的区域停留在不一致的状态上。我们的客户端代码通过重试失败的修改操作来处理这种错误。在从头开始重复执行之前，它将在3-7步骤上做几次尝试。 分布式文件系统的并发写 当一个应用的写操作数据很大以至于跨越了多个数据块时，分布式文件系统会将该写操作分割为多个写操作，这些操作都遵循1.3.1节所描述的控制流，但是可能会被其它客户端上的请求打断或覆盖，即并发写时多个客户端先后发出的写操作的文件共享部分可能最终会包含多个客户端的交叉碎片，即undefined，但是由于lease机制的保障所有副本都是执行的同一个写操作序列，孤儿所有的副本都是完全相同的，即consistent but undefined 分布式文文件系统的Append操作 分布式文件系统中还存在另一种写操作append record，append只在文件的尾部以record为单位（为了避免内部碎片，record一般不会很大）写入数据。append是原子性 的，GFS保证将数据顺序地写到文件尾部至少一次。append record的流程和图2类似，只是在primary有点区别，GFS必须保证一个record存储在一个chunk内，所以当primary判断当前 chunk无足够空间时，就通知所有副本将chunk填充，然后汇报失败，client会申请创建新chunk并重新执行一次append record操作。如果该chunk大小合适，primary会将数据写到数据块的尾部，然后通知其它副本将数据写到一样的偏移。任何副本append失 败，各个副本会处于不一致状态（完成或未完成），这时primary必然是成功写入的（不然就没有4以后的操作了）。客户端重试append record操作时，因为primary的chunk长度已经变化了，primary就必须在新的偏移写入数据，而其它副本也是照做。这就导致上一个失败 的偏移位置，各个副本处于不一致状态，应用必须自己区分record正确与否，我称这为无效数据区。 记录追加操作是修改操作中的一种，在一个记录追加操作中，客户端只需要指定数据，GFS将数据至少一次的原子性的追加到文件。该操作遵循1.3.1节中所示控制流程，只在primary上有一些额外的逻辑。客户端把数据推送到文件最后一个块的所有的副本上，然后将向primary发送它的请求。Primary会检查这次追加操作是否使块的大小超过了最大尺寸（64MB）。如果超过，它将把这个块填充满，通知所有的secondary副本进行相同的操作，并回复客户端表明这个操作将在下一个块上重新执行。（记录追加操作的数据大小严格控制在最大尺寸的1/4以内，以确保最坏情况下碎片的数量在一个可接受范围。）通常情况下，如果记录不超过最大尺寸，primary将数据追加到它的副本上，然后通知secondary把数据写到与primary相同的位置上，最后回复客户端操作成功。 如果在任意一个副本上的记录追加失败，客户端将重试这个操作。因此，在同一个块的副本上可能包含不同的数据，包括同一个记录的全部或部分的重复数据。GFS不保证写入的数据在字节上完全相同，它只保证作为一个原子单元至少被写入一次。这个特性能够通过简单的观察得到：如果操作执行成功，数据肯定被写入到了某些块副本的相同位置。此外，在这之后，所有副本至少都达到了记录尾部的长度，因此，即使一个不同的副本成为了primary，以后的任何记录也都将被放置在更大的偏移位置或者是一个不同的块上。在我们的一致性保障方面，记录追加操作成功的写入数据的区域是被定义的（因此是一致的），反之，介于中间状态的区域是不一致的（因此是undefined的）。 由上述分析可知，在Append操作中，分布式文件系统确保至少有一个记录追加操作被至少一次地原子性追加到文件，从而确保其defined，但是，如果在Primary Chunk中追加成功，而副本追加失败时，客户端会重试追加操作，而之前的部分追加操作会被应用程序通过某种机制使之失效（即不影响其明确性），不同的副本可能失败情况不一致，因此最终各个副本中数据可能局部不一致，故而Append操作是defined interspersed with inconsistent的 write(offset, **) Append(data): 不关心数据位置]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[系统结构第二次实验:设计实现 Tomasulo 调度算法]]></title>
      <url>%2Farticle%2FTomasulo%E8%B0%83%E5%BA%A6%E7%AE%97%E6%B3%95.html</url>
      <content type="text"><![CDATA[实验原理 Tomasulo 算法以硬件方式实现了寄存器重命名，允许指令乱序执行，这是提高流水线的吞吐率和效率的一种有效方式。该算法首先出现在 IBM360/91 处理机的浮点处理部件中，后广泛应用于现代处理器设计中。 假设浮点处理部件结构如下图所示。浮点处理部件从取指单元接收指令，存入浮点操作队列。浮点操作队列每拍最多发射1条指令给浮点加法器或浮点乘除法器。 浮点处理部件包含一个浮点加法器和一个浮点乘除法器。浮点加法器为两段流水线，输入端有三个保留站A1、A2、A3，浮点乘除法器为六段流水线，输入端有两个保留站M1，M2。当任意一个保留站中的两个源操作数到齐后，如果对应的操作部件空闲，可以把两个操作数立即送到浮点操作部件中执行。Load Buffer和Store Buffer各缓存三条访存操作。 实现要求 设计实现 Tomasulo 算法模拟器，要求： Tomasulo 算法模拟器能够执行浮点加、减、乘、除运算及 LOAD 和 STORE 操作。为了保持一致性，我们小组采用了【教科书上的指令格式】 指令格式 指令说明 指令执行周期 保留站/缓冲队列项数 ADDD F1,F2,F3 F1,F2,F3为浮点寄存器，寄存器至少支持F0~F10 2个周期 3 SUBD F1,F2,F3 同上 2个周期 3 MULD F1,F2,F3 同上 10个周期 2 DIVD F1,F2,F3 同上 40个周期 2 LD F1, NUM, R1 F1为寄存器，NUM 为偏移量，R1为基地址 2个周期 3 ST F1, NUM, R1 同上 2个周期 3 支持单步执行及连续执行(n 条指令)，实时显示算法的运行状况，包括各条指令的运行状态、各寄存器以及内存的值、保留站(Reservation Stations)状态、Load Buffer 和 Store Buffer 缓存的值等; 程序执行完毕后，能够显示指令执行周期数等信息; 为了简化设计，建议模拟器 供编辑内存值功能，以便实现数据输入;浮点除法可不做除 0 判断; 能够以文本方式输入指令序列。 显示界面自由设计，下图仅供参考: 完成内容 能够以文件方式导入指令 能够以文本框编辑方式编辑指令 能够设置FU,RU寄存器的初始值 能够编辑内存值 能够设置实时显示内存内容的起始内存地址（5个） 支持单步和多步连续执行 程序执行完毕后，能够显示指令执行周期数等信息 实现设计思路 后台 算法框架和基本流程 模拟环境: 我们用一次循环来代表一次始终周期,每次循环始终加一. 对于每个时钟周期(一个节拍),一条指令最多只能被执行发射,执行,或者写入中的一种操作(若所需运算数据未就绪,所需运算器件未就绪均会阻塞).即对于一条需要2个周期来执行运算的加法指令,共需要至少1+2+1=4个周期,其运算结果才会被写入目的寄存器. 我们将Load/Store缓冲区和保留站统一处理,即缓冲区和保留站均有Qi,Qj,Vi,Vj,A等数据信息.对于Load保留站,其Qi始终为0,即Vi始终就绪,Vj为计算所得的地址;对于Store缓冲区,其Vi为所要存储的数据,Vj为通过整数寄存器计算所得的地址. 算法流程: 系统在完成初始化后，会将指令按给定的序列加入指令队列,即在UI界面点击开始后,程序会根据给定的指令序列自动初始化指定队列。接下来,对于每一个时钟周期会进行如下操作 (1)发射指令 对于位于指令队列队首的指令，我们先寻找其对应的空闲保留站，若存在，则发射该指令到对应的保留站，更新该保留站的数据信息(包括busy,Vi,Vj,Qi,Qj,A),并更新要写入寄存器的状态.若不存在空闲站,则队首指令将无法流出，该指令会停留在队首等待下个周期。当指令队列清空时,说明所有指令已经流出,不再发射指令. (2)执行流水线 我们共提供了三条流水线分别处理访存(根据课件认为是一个2段流水线,每段1周期)、加减法、乘除法指令，每条流水线有自己的流水段和相应的各段周期。每条位于保留站的指令必须在其所需数据均准备好才能进入对应的流水线进行执行.(对于Load指令来说,所需数据为整型寄存器所表示的基址,对于Store指令来说,所需数据出了基址还有要存储的数据) 指令进入流水线后,我们通过如下两个过程来模拟流水线运行流程: 新指令进入流水线 当保留站中缓存的指令要进入对应的流水线时必须满足两个条件：一,流水线当前进行的运算能够执行该指令（即乘法指令要进入乘除法器的流水线时，必须保证没有除法指令在执行，我们认为加减法和访存不需要改变运算器的功能逻辑,因此加法和减法进入加法器流水线以及访存指令进入访存流水线时没有该要求）;二,对应流水线首段空闲,即流水线首段没有指令在占用。 更新流水线上各个段的指令 对于每个段的指令，我们会递减其需要的执行周期。当剩余周期为0时，说明该段运算完成，需要进入下一流水段继续运算，此时若下一流水段空闲，则直接进入下一流水段,否则阻塞等待直到其空闲为止，在此期间会持续占用原有流水段。若当前完成段为流水线最后一段，则进行结果运算，并记录结果到对应的保留站，等待写入(对于Store指令我们在此时更新相应的内存)同时释放对流水段的占有。 (3)写入数据： 对于每条完成的指令(除了Store指令)，我们会将其结果写入对应的寄存器并更新相应的订阅的保留站的等待数据。我们假设总线在一个周期内可以完成多次数据传输，即对于同一个周期内要进行多条指令的写回时，我们按照顺序将其结果更新到对应的位置，Tomasulo算法保证了该过程不会触发数据相关的错误。 算法状态显示: 在指令的每个时期(发射,执行完毕,写回完毕)完成后，我们会记录对应指令的完成时间，并更新到前台。 在完成每个周期的操作后,我们将保留站和寄存器以及指令的执行信息更新到前台，以供观察。 主要数据结构: 我们通过4个主要的数据结构来执行该算法. 寄存器class Register:维护了寄存器的数据和状态信息Qi 指令class Inst: 维护了指令的操作码,操作数,目的地址,指令的运行状态,指令发射,运算完毕,写回的时间 保留站class ReservationStation: 保留站是关键的数据结构,它维护了保留站的基本数据信息(op,busy,vj,vk,qj,qk,A,result).此外还维护了便于观察保留站状态的各种数据: execover//保留站所缓存的指令是否已经运算完成 status//保留栈所缓存指令的状态,包括(发射,等待数据,等待运算器相应的流水段空闲,正在运算,运算完毕,保留站空闲) times: 所缓存的指令还需要在运算器运算的总周期数(不包括等待流水段的时间) 流水线模拟器class Pipeline: 流水线模拟器维护了各个流水段需要的运行周期,各个流水段正在运行的指令和其所需的剩余的运算时间.我们设计实现的流水线有以下性质: 对于n段流水线,最多同时支持n条指令在其中运行 当第i条指令完成第k段流水段运算后,若第i-1条指令还占据第k+1段流水段,则第i条指令会阻塞并持续占用第k段流水段. 当第i条指令未完成第k段流水段运算时,其不会受其他段指令的影响,即第i+1条指令或者i-1条指令阻塞时,第i条指令能继续进行第k段的运算. 流水线支持功能切换,即可以切换流水段数量和每段的周期数,以此支持乘除法周期数不一致的要求.但功能切换必须在流水线中所有指令全部流出时才能进行.即若一条除法指令想进入流水线,必须在流水线处理完所有的乘法运算后切换成除法模式后才能进入. 对于系统默认的参数,我们将访存处理和加法减法处理设计为了2段流水线,每段1周期. 对于乘法为6段流水线.分别为1,1,2,4,1,1周期. 对于除法为6,6,6,6,6,10周期(参数可以在代码中重新自由设置,如设置为Firgure.A31所示的流水线) 前端 程序运行时主窗口如下图所示 程序主界面 如【图 程序主界面】所示，前端的实现中界面采用GridBagLayout()布局模式，顶部是10个操作按钮，点击即可完成对应的设置和执行功能。 其下方依次是指令队列状态展示区，当前周期展示区，内存展示区，保留站状态信息，浮点寄存器状态信息，整型寄存器展示区。 下面一次展示其各个部分的运行方式 从文件导入指令 点击【导入指令文件】按钮，即可进入指令文件选择界面，点击open按钮，默认打开当前工作目录，选择示例代码文件example.txt,点击导入即可导入指令，指令队列展示区会做出对应的更新。 从文件导入指令 文本框方式编辑指令 点击【编辑指令】按钮，即可进入指令编辑界面。指令编辑完成后点击确认即可导入编辑的指令，指令队列展示区会做出对应的更新。 文本框方式编辑指令 设置内存 点击【内存赋值】按钮，即可进入内存编辑界面，在第一个框中输入内存地址(0~4096), 第二个输入框中输入内存值，点击赋值按钮，若输入合法则会更新对应地址处内存。点击关闭按钮退出内存编辑界面。 设置内存 设置内存显示 点击【内存显示】按钮，即可设置内存展示的起始地址(0~4091),输入合法时点击确认即可更新内存展示区。 内存显示设置 多步执行设置 点击【步数设置】按钮，即可进行多步运行步数设置。 步长设置 寄存器设置 点击【设置寄存器】按钮，即可进入寄存器设置界面，页面中可进行FU0~FU10, RU0~RU10，共22个寄存器的初始值的设置，设置完成后点击保存若输入合法即可完成更改，输入不合法时保持当前值。 寄存器设置 程序运行 初始或点击【重置】按钮时是初始状态，此时可进行指令的导入，编辑，内存赋值，寄存器赋值，步长设置，内存显示设置等功能，点击执行后，即进入执行状态，此时部分设置按钮不可点击，即不可再进行更改指令，设置内存，设置寄存器等操作。此时若点击【步进】按钮，程序则会运行一步，点击执行n步（n可点击【步数设置】按钮进行设置）即可执行多步（n步）, 每一次步进都会更新对应部件状态展示区，程序运行结束后则不可继续点击【步进】，【执行n步】按钮。 运行效果图 程序运行说明： 在项目根目录下运行sh run.sh即可，或者在在命令行中输入如下编译运行指令 123mkdir binjavac -d bin/ src/com/company/UI.java src/com/company/Main.javajava -cp bin/ com.company.UI 项目文件说明 12345678910111213.├── README.md 运行说明├── Report.md 实验报告(MD版)├── Tomasulo调度算法实验报告.pdf 实验报告PDF版├── example_book.txt 教科书中的示例├── example_阻塞等待指令序列.txt 会在流水线各个段发生阻塞等待指令序列 ├── example_RAW相关的ST指令序列.txt 有 RAW 相关的 ST 指令序列├── run.sh 项目编译运行脚本└── src └── com └── company ├── Main.java 后台代码 └── UI.java 前端代码 运行实例 对于课件中所给的样例指令序列: 123456LD F6, 34, R2LD F2, 45, R3MULD F0, F2, F4SUBD F8, F6, F2DIVD F10, F0, F6ADDD F6, F8, F2 我们得到的指令队列状态与课件中相同: 指令 发射指令时间 执行完成时间 写入结果时间 LD F6,34,R2 1 3 4 LD F2,45,R3 2 4 5 MULD F0,F2,F4 3 15 16 SUBD F8,F6,F2 4 7 8 DIVD F10,F0,F6 5 56 57 ADDD F6,F8,F2 6 10 11 运行结束需要57个周期与课件一致 对于有RAW相关的ST指令序列: 123ST F1,0,R2MULD F0, F2, F4ST F0,4,R0 也能得到正确结果: 指令 发射指令时间 执行完成时间 写入结果时间 ST F1,0,R2 1 3 4 MULD F0,F2,F4 2 12 13 ST F0,4,R0 3 15 16 对于会在流水线各个段发生阻塞等待指令序列: 123456DIVD F6, F0, F4MULD F2, F1, F5MULD F0, F6, F4SUBD F8, F6, F2DIVD F10, F0, F6ADDD F6, F8, F2 我们也能得到正确的结果: 指令 发射指令时间 执行完成时间 写入结果时间 DIVD F6,F0,F4 1 41 42 MULD F2,F1,F5 2 51 52 MULD F0,F6,F4 43 55 56 SUBD F8,F6,F2 44 54 55 DIVD F10,F0,F6 53 96 97 ADDD F6,F8,F2 54 57 58 在该例子中前三条指令间存在结构冲突,由于除法在流水线中执行时,新来的乘法指令必须等待除法指令完成才能进入流水线,因此乘法指令完成时的时钟周期为51(42进入首段,51完成最后一段). 另外由于保留站占满,第三条乘法指令等到43周期才被发射. 对于第三条乘法指令,由于乘法流水线各个段的运行周期为(1,1,2,4,1,1),第三条指令会在第三段完成后阻塞等待2个时钟周期,以等待第二条乘法指令完成第4段,因此第三条乘法指令的执行完成时间为55(44进入,55完成,共用12个周期,比第二条慢了2(慢2周期进入)+2(期间等待2周期)=4个周期而不是2个周期,在第48,49会看到对应的保留站状态为WAIT_ALU意为等待处理器)]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[校园搜索引擎]]></title>
      <url>%2Farticle%2F%E6%A0%A1%E5%9B%AD%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E.html</url>
      <content type="text"><![CDATA[内容 综合运用搜索引擎体系结构和核心算法方面的知识，基于开源资源搭建搜索引擎 开源搜索引擎工具资源： Heritrix 1.14.4 Lucene 4.0 分词工具： IK Analyzer 2012 Html解析：Jsoup 1.7.2 PDF 解析：pdfbox 1.8.1 Doc 解析：poi 3.16 前端服务：apache+tomcat (http://tomcat.apache.org/) 实验要求 抓取清华校内绝大部分网页资源以及大部分在线万维网文本资源（含M.S.office文档、pdf文档等，约20-30万个文件） 实现基于概率模型的内容排序算法； 实现基于HTML结构的分域权重计算，并应用到搜索结果排序中； 实现基于PageRank的链接结构分析功能，并应用到搜索结果排序中； 采用便于用户信息交互的Web界面。 实现流程 爬虫爬取清华校内网页数据 Heritrix抓取对象 Heritrix环境搭建教程 清华新闻网网页（不包括图书馆）资源 种子http://news.tsinghua.edu.cn/ 使用正则表达式对URL进行过滤 过滤无关页面，过滤无关格式文件，，保留html页面和pdf,word文档，去除奇怪链接： 1.*(?i)\.(mso|tar|txt|asx|asf|bz2|mpe?g|MPE?G| tiff?|gif|GIF|png|PNG|ico|ICO|css|sit|eps|wmf|zip|pptx?|xlsx?|gz|rpm|tgz|mov|MOV|exe|jpe?g|JPE?G|bmp|BMP|rar|RAR|jar|JAR|ZIP|zip|gz|GZ|wma|WMA|rm|RM|rmvb|RMVB|avi|AVI|swf|SWF|mp3|MP3|wmv|WMV|ps|PS|d|dd|yyyy|nivo-nextNav|xiao_ming)$ 禁止抓取图书馆资源： [\S]*lib.tsinghua.edu.cn[\S]*；[\S]*166.111.120.[\S]* 只抓取清华新闻网的数据 [\S]*news.tsinghua.edu.cn[\S]*； Module设置，参考PPT中的设置进行 Heritrix 抓取过程中遇到的问题 页面剔除问题 在一开始的抓取过程中发现了部分奇怪的页面，如图[Heritrix 异常页面]所示，发现了很多以yyyy.MM.dd、yyyy.M.d、MM.dd.yyyy、a.nivo-nextNav 为结尾的奇怪链接，后来经过分析清华新闻网的源码，发现这些链接都是js代码里的内容，虽然已经在配置中设定为不从css,js等域中获取超链接，但是仍然会得到这样的url, 因此后来在正则表达式中加上了d|dd|yyyy|nivo-nextNav|xiao_ming字段进行过滤，最终得到的页面有52769个，共2.3G Heritrix 异常页面 Heritrix 加速问题 在抓取页面过程中，第一次尝试时发现抓取速度特别慢，爬取整整一个晚上只能获取300MB的数据，通过查询相关资料尝试了很多加速方法，最终在博客Heritrix提高抓取效率的若干尝试 {http://blog.csdn.net/yangding_/article/details/41122977} 找到Heritrix抓取速度特别慢的原因：heritrix在抓取时一般只运行了一个线程。这是因为在默认的情况下，Heritrix使用HostnameQueueAssignmentPolicy来产生key值，而这个策略是用hostname作为key值的，因此一个域名下的所有链接都会被放到同一个线程中去。如果对Heritrix分配URI时的策略进行改进，利用ELFhash算法把url尽量平均分部到各个队列中去，就能够用较多的线程同时抓取一个域名下的网页，速度将得到大大的提高。 12345678910111213141516171819202122@Override//重写 getClassKey()方法public String getClassKey(CrawlController controller, CandidateURI cauri) &#123; String uri = cauri.getURI().toString(); long hash = ELFHash(uri);//利用 ELFHash 算法为 uri 分配 Key 值 String a = Long.toString(hash % 50);//取模 50，对应 50 个线程 return a;&#125;public long ELFHash(String str)&#123; long hash = 0; long x = 0; for(int i = 0; i &lt; str.length(); i++) &#123; hash = (hash &lt;&lt; 4) + str.charAt(i);//将字符中的每个元素依次按前四位与上 if((x = hash &amp; 0xF0000000L) != 0)//个元素的低四位想与 &#123; hash ^= (x &gt;&gt; 24);//长整的高四位大于零，折回再与长整后四位异或 hash &amp;= ~x; &#125; &#125; return (hash &amp; 0x7FFFFFFF); &#125; 按照该教程完成代码的修改之后，重新运行，抓取速度得到了大幅提升，4个半小时的时间完成了清华新闻网的页面抓取工作。 未修改hash函数时爬取速度 修改代码后的爬取速度明显加快 数据清洗及PageRank计算 在实验过程中我们需要使用页面的PageRank值来对网页进行打分，因此需要对抓取的数据计算其PageRank值，由于Heritrix在抓取时就已经有了crawl.log文件用于记录抓取到的网页链接及访问信息，因此在计算PageRank时直接使用了该文件的记录信息。 预处理及PageRank计算 12345├── clean_page.py : 页面清洗，├── parse_graph_title_anchor.py ： ├── share.py : 一些共有的文件名记录，html页面处理函数└── tsinghua_rank.py ： 计算PageRank值，结果保存到pagerank.txt中└── crawl.log : Heritrix 抓取的结果记录 需要注意的是，由于 Heritrix 在抓取带 GET 请求的网页时，存储文件的文件名和网址URL并不能一一对应(其去掉了问号，挪动了文件类型的位置)，且单从文件名并不能找到对应的 URL，所以第一步分析Heritrix爬取日志是必要且是必须的。通过分析日志，得到了URL到文件名的双向映射，同时删除了 404网页，将网页个数减少到 52205个页面。 在提取链接、标题、anchor时一开始使用的是BeautifulSoup进行提取，后来发现这种方式太慢，于是手动使用正则表达式进行提取。 123href_pattern = re.compile(r'&lt;a href=[\"\']([^\"\']*?\.(html|pdf|doc|docx))[\"\'].+?&gt;(.+?)&lt;/a&gt;', re.S)html_pattern = re.compile(r'&lt;a href=[\"\']([^\"\']*?\.html)[\"\']', re.S)title_pattern = re.compile(r'&lt;title&gt;(.+?)&lt;/title&gt;', re.I | re.M | re.S) 计算完成之后的PageRank值如下所示： 排名前10的页面 12345678910 /publish/thunews/index.html 0.0563122679453 首页 清华大学新闻网/publish/thunews/9652/index.html 0.0448806058384 更多 &amp;#8250; 清华大学新闻网 - 图说清华/publish/thunewsen/index.html 0.0384040176156 ENGLISH Tsinghua University News/publish/thunews/9650/index.html 0.0257207878312 媒体清华 清华大学新闻网 - 媒体清华/publish/thunews/10303/index.html 0.0257202750476 综合新闻 清华大学新闻网 - 综合新闻/publish/thunews/9656/index.html 0.0250196913473 清华人物 清华大学新闻网 - 清华人物/publish/thunews/9649/index.html 0.02500709101 要闻聚焦 清华大学新闻网 - 要闻聚焦/publish/thunews/9657/index.html 0.0249803912605 新闻合集 清华大学新闻网 - 新闻合集/publish/thunews/10304/index.html 0.0249798815124 新闻排行 清华大学新闻网 - 新闻排行/publish/thunews/9655/index.html 0.0243158953181 专题新闻 清华大学新闻网 - 专题新闻 新闻页前10 123456789101112 /publish/thunews/9648/2017/20170520203232435687344/20170520203232435687344_.html 0.00197507591613 邱勇出席第二届中以创新论坛：畅谈国际创新创业教育合作 邱勇出席第二届中以创新论坛：畅谈国际创新创业教育合作/publish/thunews/9648/2017/20170518115011788320647/20170518115011788320647_.html 0.00197507591613 清华医学院程功研究组揭示寨卡病毒感染暴发机制 清华医学院程功研究组揭示寨卡病毒感染暴发机制/publish/thunews/9648/2017/20170519190126950804131/20170519190126950804131_.html 0.00197507591613 邱勇会见以色列总统鲁文·里夫林·接受以色列特拉维夫大学荣誉博士学位 邱勇会见以色列总统鲁文·里夫林·接受以色列特拉维夫大学荣誉博士学位/publish/thunews/9648/2017/20170522184445768862282/20170522184445768862282_.html 0.00197507591613 清华大学新闻与传播学院举办纪念成立15周年系列活动 清华大学新闻与传播学院举办纪念成立15周年系列活动/publish/thunews/9648/2017/20170515184412579525281/20170515184412579525281_.html 0.00197507591613 清华大学全球可持续发展研究院正式揭牌成立 清华大学全球可持续发展研究院正式揭牌成立/publish/thunews/9648/2017/20170516121327519550489/20170516121327519550489_.html 0.00197507591613 清华微电子所钱鹤、吴华强课题组在基于新型忆阻器阵列的类脑计算取得重大突破 清华微电子所钱鹤、吴华强课题组在基于新型忆阻器阵列的类脑计算取得重大突破/publish/thunews/9652/2017/20170307133841881904789/20170307133841881904789_.html 0.00193350555685 【组图】最美三月女生节 浪漫创意盈满“幅” 【组图】最美三月女生节 浪漫创意盈满“幅”/publish/thunews/9652/2017/20170314142112142471080/20170314142112142471080_.html 0.00193350555685 【组图】春到绿茵场 马杯足球赛正酣 【组图】春到绿茵场 马杯足球赛正酣/publish/thunews/9945/2017/20170524164751321376872/20170524164751321376872_.html 0.00143223231223 5月18日，以“一带一路低碳前行”为主题的第十二届世界低碳城市联盟大会暨低碳城市发展论坛在三亚召开。 ... 2017-05-24 清华共同主办第十二届世界低碳城市联盟大会/publish/thunews/9945/2017/20170524113049223303463/20170524113049223303463_.html 0.00143197048315 2017年“共和国的脊梁——科学大师名校宣传工程”汇演在重庆大学启动。清华大学原创话剧《马兰花开》被 ... 2017-05-24 清华原创话剧《马兰花开》在科学大师名校宣传工程汇演上首演/publish/thunews/9945/2017/20170522171134178522272/20170522171134178522272_.html 0.00143197048315 5月19日晚，清华大学巅峰对话第二十期物理分论坛在清华大学举行。本次活动邀请了2015年诺贝尔物理学 ... 2017-05-23 诺贝尔物理学奖得主梶田隆章做客“巅峰对话”/publish/thunews/9945/2017/20170524093408535456498/20170524093408535456498_.html 0.00143197048315 5月18日—21日，清华大学第一附属医院党委书记类延旭和副院长朱栓立带领一附院医疗分队走进昆明市东川 ... 2017-05-24 清华大学第一附属医院走进昆明健康义诊 由PageRank计算结果可以发现，正常的新闻页面的PageRank值大多在\(10^{-6} ~10^{-4}\)之间，如果直接将该PageRank值与BM25算法的得分相乘会导致PageRank值高的页面，即使关键词出现次数少，也会在最终排名中特别靠前，为了减少其影响，在将PageRank值应用到Score计算时对PageRank值进行压缩\[newPageRank = 16 + ln(PageRank)\] 构建索引及倒排索引 文档解析 Html解析 网页文件元素十分丰富。实验中使用Jsoup工具包解析网页，抽取title标签的文本内容作为文档的的标题域；抽取p、span、td、div、li、a标签的文本内容作为文档的内容域；a标签的内容表示页面链出的内容，也作为一个域单独索引；h1-h6标签的文本内容表示页面内的小标题，拿出来作为一个域；此外，进入页面的链接有着和页面标题相似的作用，单独成为一个域。 PDF解析 PDF的元素不易区分，实验中使用pdfbox解析文件获得内容域，直接以文件名作为标题域。 Doc解析 Doc文件与PDF文件类似，实验中使用POI包解析文件获得内容域，直接以文件名作为标题域。需要注意的是，POI工具解析.doc文件.docx文件的方法并不一样，在实验中，我们为此耽误了不少时间。 检索 修改图片搜索框架 在实验开始，我们修改了图片搜索的框架，进行如下操作。 对查询进行分词后获得token列表。 对每一个token的倒排索引，只需满足一个域的文档即认为是属于该token的文档 满足所有token的文档才能作为整个查询的文档进行评分 对每个token的每个域计算BM25并求和，最后加上页面的PageRank值。加上PageRank值而非相乘，可以避免索引页面总排在最前面而在评分相差不大时获得优势 使用MultiFieldQueryParser 通过修改框架的方式获得了很大的自由空间，但实现上效率很低，搜索结果用时很长。由此，我们使用MultiFieldQueryParser替代自己实现的SimpleQuery、SimpleSimilarity、SimpleScorer等类。为了使用BM25评分，Lucene也改为4.0版本，相应地，IK Analyzer也修改了版本。至此，我们使用Lucene提供的BM25Simlarity计算BM25评分。 分域权重 我们抽取1000个文档进行测试，给各个域赋予不同的权重，作为boosts参数传给MultiFieldQueryParser。在使用整体数据进行测试的过程中，我们也进行了相应调整，最后确定了100、25、35、1、0.001的一组权重。 文档摘要 呈现文档时，我们对文档内容抽取摘要进行展示。建立所有token在文档内容中的位置构成的集合，从前开始，并呈现token前后的30个字符；若两个token临近则连续输出。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990public static String genAbstract(List&lt;String&gt; tokens, String content) &#123; int maxLength = 300; int range = 30; String result = ""; content = content.trim(); List&lt;Integer&gt; startPositions = new ArrayList&lt;Integer&gt;(); List&lt;Integer&gt; endPositions = new ArrayList&lt;Integer&gt;(); for (String t : tokens) &#123; String token = new String(t); int colonIndex = token.indexOf(':'); if (colonIndex &gt;= 0) &#123; token = token.split(":")[1]; &#125; int pos = 0; Pattern pattern = Pattern.compile(token, Pattern.CASE_INSENSITIVE); Matcher matcher = pattern.matcher(content); int num = 0; while (matcher.find(pos) &amp;&amp; ++num &lt; maxLength / range) &#123; pos = matcher.start(); startPositions.add(pos); endPositions.add(pos + token.length()); ++pos; &#125; &#125; Collections.sort(startPositions); Collections.sort(endPositions); int i = 0; int size = startPositions.size(); while (i &lt; size) &#123; int pos = startPositions.get(i); int end = endPositions.get(i); int ptr; for (ptr = pos; ptr &gt;= pos - range; --ptr) &#123; if (ptr &lt; 0 || stopChar.contains(content.charAt(ptr))) &#123; ++ptr; break; &#125; &#125; result += content.subSequence(ptr, pos); result += "&lt;em&gt;"; result += content.subSequence(pos, end); result += "&lt;/em&gt;"; ++i; while (i &lt; size) &#123; pos = startPositions.get(i); if (end &gt; pos) &#123; result += "&lt;em&gt;"; result += content.subSequence(end, endPositions.get(i)); result += "&lt;/em&gt;"; pos = end; end = endPositions.get(i); ++i; &#125; else &#123; if (pos == end) &#123; result += content.subSequence(end, pos); end = endPositions.get(i); result += "&lt;em&gt;"; result += content.subSequence(pos, end); result += "&lt;/em&gt;"; ++i; &#125; else if (pos - end &lt; range) &#123; result += content.subSequence(end, pos); end = endPositions.get(i); result += "&lt;em&gt;"; result += content.subSequence(pos, end); result += "&lt;/em&gt;"; ++i; if (result.length() &gt; maxLength - range) &#123; break; &#125; &#125; else &#123; break; &#125; &#125; &#125; for (ptr = end; ptr &lt; end + range; ++ptr) &#123; if (ptr &gt;= content.length() || stopChar.contains(content.charAt(ptr))) &#123; break; &#125; &#125; result += content.subSequence(end, ptr) + "... "; if (result.length() &gt; maxLength) &#123; break; &#125; &#125; return result;&#125; 但这种方法并不总能正确呈现查询词的位置。之后，我们使用Lucene自带的highlight进行处理，一些原来认为毫无关系的文档也能看到相关性。 实验结果 刘奕群 陈旭 超算 长文本搜索 心得体会 实验开始，我们完成了对图像搜索框架的修改，并实现了摘要提取，费尽周章进行调试，但是效果始终不理想。一个是响应速度慢，需要十几秒，一个是显示的摘要大多时候表现不出和查询的相关性。 这个时候，我们采用了Lucene自带的MultiFieldQueryParser进行查询，显著缩短了查询时间，增强了查询体验；同时，Lucene框架内的highlights提供了更加友好的摘要，使得一些标题好像根本不相关的页面也表现出了相关性。与此同时，原有的代码被删减近半。 痛惜之余，我们也感受到了开源社区的优越性，反复造轮子的过程是对时间的浪费，多使用已有的工具包可以获得更好的效果。回过头来看，似乎所有的工作在最后一天下午又重新做起了。 在重新构建框架的过程中也进一步发现了很多有用的开源工具，也发现了很多可进一步扩展的功能，不过由于前期反复造轮子耗时过多导致最后时间不够没能进一步实现。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Ipv4 over Ipv6隧道协议实验客户端报告]]></title>
      <url>%2Farticle%2F4over6%E5%AE%9E%E9%AA%8C%E6%8A%A5%E5%91%8A.html</url>
      <content type="text"><![CDATA[实验目的 掌握Android下应用程序开发环境的搭建和使用 掌握IPv4 over IPv6隧道的工作原理 客户端实验要求 在安卓设备上实现一个4over6隧道系统的客户端程序 实现安卓界面程序，显示隧道报文收发状态(java语言)； 启用安卓VPN服务(java语言)； 实现底层通信程序，对4over6隧道系统控制消息和数据消息的处理(C语言)。 实验内容 前台是java语言的显示界面 进行网络检测并获取上联物理接口IPV6地址； 启动后台线程； 开启定时器刷新界面； 界面显示网络状态； 开启安卓VPN服务。 后台是C语言客户端与4over6隧道服务器之间的数据交互 连接服务器； 获取下联虚接口IPV4地址并通过管道传到前台； 获取前台传送到后台的虚接口描述符； 读写虚接口； 对数据进行解封装； 通过IPV6套接字与4over6隧道服务器进行数据交互； 实现保活机制，定时给服务器发送keeplive消息。 实验原理 面向Android终端的隧道原理 4over6隧道原理 在4over6隧道中，客户端首先向过渡网关请求分配IPv4内网地址；过渡网关分配IPv4内网地址，并提供对应的IPv6网络地址；接着，安卓客户端发送4over6报文，过渡网关接受报文并进行分析，得到源地址和目的地址，将IPv4地址转化为公网地址，发送到公网之中；当过渡网关收到公网的IPv4报文之后，根据记录好的映射关系，重新封装成4over6报文，发给对应的内网用户，完成数据的转发和接受。 本实验中分别完成了过渡网关和客户端的功能，本报告主要阐述客户端的相关实现。 客户端用户处于IPV6网络环境，通过过渡网关完成IPV4网络的访问，过渡网关横跨IPV4和IPV6,提供地址转换和数据包的分发。 VPN Service 原理 在客户端实现过程中，使用了VPNService API, 打开VPN服务后，Android系统通过iptables使用NAT将所有数据包转发到TUN虚拟网络设备，通过mInterface.getFd();可以获取到虚接口描述符，从而通过读虚接口获取系统的IP数据包，再将IPV4数据包封装经IPV6 socket转发给服务器端即可；服务器端根据网络请求信息完成访问，并将结果通过IPV6 socket发回，后台解析取出IPV4数据包，写入虚接口以实现数据接收。 具体实现 客户端实现主要分前台和后台，前台是Java实现的Android客户端显示界面，后台主要是C++ 实现的客户端与服务器端的数据交互。前后台通过读写管道以及JNI函数调用实现交互。总体流程如下： 总体流程图 前端实现内容 前端流程及完成的工作 前端详细流程 前台流程如上图所示, 主要完成以下操作 开启定时器之前，创建一个读取IP信息管道的全局标志位flag，默认置0； 开始读取管道，首先读取IP信息管道，判断是否有后台传送来的IP等信息； 假如没有，下次循环继续读取； 有IP信息，就启用安卓VPN服务(此部分在后面有详细解释)； 把获取到的安卓虚接口描述符写入管道传到后台； 把flag置1，下次循环不再读取该IP信息管道； 读取流量信息管道； 从管道读取后台传来的实时流量信息； 把流量信息进行格式转换； 显示到界面； 界面显示的信息有运行时长、上传和下载速度、上传总流量和包数、下载总流量和包数、下联虚接口V4地址、上联物理接口IPV6地址。 具体实现 界面 主界面采用了ScrollView嵌套LinearLayout的布局方式，界面中中有2个输入框，以及一个Button,分别用于输入服务器端的IPV6地址，端口号, 以及点击按钮链接VPN。且有一个文本框用于显示当前IPV6地址（若无，提示无IPV6网络访问权限）链接VPN之后，IPV6地址和端口输入框消失，显示一个TextView（用于显示运行时长、上传和下载速度、上传总流量和包数、下载总流量和包数、下联虚接口V4地址、上联物理接口IPV6地址等信息）和断开连接按钮。 未连接界面 连接VPN后界面 UI主线程 客户端开启后，即检查当前网络环境是否支持IPV6访问，若不可访问IPV6网络则用户点击【链接VPN】按钮无效。检查代码如下： 123456789101112131415161718192021static String getIPv6Address(Context context) &#123; if(! isWIFIConnected(context)) &#123; return null; &#125; try &#123; final Enumeration&lt;NetworkInterface&gt; e = NetworkInterface.getNetworkInterfaces(); while (e.hasMoreElements()) &#123; final NetworkInterface networkInterface = e.nextElement(); for (Enumeration&lt;InetAddress&gt; enumAddress = networkInterface.getInetAddresses(); enumAddress.hasMoreElements(); ) &#123; InetAddress inetAddress = enumAddress.nextElement(); if (!inetAddress.isLoopbackAddress() &amp;&amp; !inetAddress.isLinkLocalAddress()) &#123; return inetAddress.getHostAddress(); &#125; &#125; &#125; &#125; catch (SocketException e) &#123; Log.e("NET", "无法获取IPV6地址"); &#125; return null;&#125; 为【链接VPN】按钮注册监听服务，若可访问IPV6网络（即有IPV6地址），则用户点击按钮后，启动VPN服务，并将用户填入的服务器IPV6地址和端口号通过Intent传入 启动VPN服务后，主界面定时刷新，通过JNI java调用C函数方式读取流量收发信息并显示在主界面。 VPNService 继承一个VpnService的类，启动后，先读取MainActivite中传入的Intent，获取服务器端的IPV6地址和端口号, 通过JNI java调用C函数的方式将该数据传给C后台，C后台自动与服务器端建立IPV6 Socket, 然后VPNService 循环查询C后台是否获取到服务器发回的101数据信息，得到C后台传入的IP地址，DNS，路由，以及IPV6 Socket标识信息，据此初始化VPN服务并启动，获取到TUN虚接口的文件描述符后，通过JNI函数调用传给C后台，至此前端的任务完成。 12345678910111213141516171819202122232425262728293031323334353637383940414243// 2. 开始读取管道，首先读取IP信息管道，判断是否有后台传送来的IP等信息// 3. 假如没有，下次循环继续读取；while(!isGet_ip()) &#123;&#125;// 4. 有IP信息，就启用安卓VPN服务String ip_response = ip_info();Log.e(TAG, "GET IP " + ip_response);String[] parameterArray = ip_response.split(" ");if (parameterArray.length &lt;= 5) &#123; throw new IllegalStateException("Wrong IP response");&#125;// 从服务器端读到的IP数据ipv4Addr = parameterArray[0];router = parameterArray[1];dns1 = parameterArray[2];dns2 = parameterArray[3];dns3 = parameterArray[4];// sockrt 描述符String sockfd = parameterArray[5];Builder builder = new Builder();builder.setMtu(1500);builder.addAddress(ipv4Addr, 32);builder.addRoute(router, 0); // router is "0.0.0.0" by defaultbuilder.addDnsServer(dns1);builder.addDnsServer(dns2);builder.addDnsServer(dns3);builder.setSession("Top Vpn");try &#123; mInterface = builder.establish();&#125;catch (Exception e) &#123; e.printStackTrace(); Log.e(TAG,"Fatal error: " + e.toString()); return;&#125;// 5. 把获取到的安卓虚接口描述符写入管道传到后台int fd = csend_fd(fd, PIPE_DIR);if (!protect(Integer.parseInt(sockfd))) &#123; throw new IllegalStateException("Cannot protect the mTunnel");&#125;start = true;Log.e(TAG, "configure: end"); 后台实现内容 后台实现流程及内容 创建IPV6套接字； 连接4over6隧道服务器； 开启定时器线程（间隔1秒）： 读写虚接口的流量信息写入管道； 获取上次收到心跳包距离当前时间的秒数S； 假如S大于60，说明连接超时，就关闭套接字； S小于60就每隔20秒给服务器发送一次心跳包。 发送消息类型为100的IP请求消息； while循环中接收服务器发送来的消息，并对消息类型进行判断； 101类型(IP响应)： 取出数据段，解析出IP地址，路由，DNS； 把解析到的IP地址，路由，DNS写入管道； 从管道读取前台传送来的虚接口文件描述符； 创建读取虚接口线程： 持续读取虚接口； 记录读取的长度和次数； 封装102(上网请求)类型的报头； 通过IPV6套接字发送给4over6隧道服务器。 103类型(上网应答)： 取出数据部分； 写入虚接口； 存下写入长度和写入次数。 104类型(心跳包)： 记录当前时间到一个全局变量。 后台实现流程 具体实现 与服务器建立Socket连接 后台使用C++ Socket编程实现，用户点击【连接VPN】按钮之后即启动MyVPNService服务，同时调用C后台的start_VPN函数，该函数是后台主函数，MyVPNService服务通过public native int send_addr_port(String addr, int port);函数向C后台传递服务器端IPV6地址和端口，后台检测到取得IPV6地址信息后即完成与服务器的Socket连接及绑定。 1234567891011if ((sockfd = socket(AF_INET6, SOCK_STREAM, 0)) &lt; 0) &#123; LOGE("can't create socket");&#125;server.sin6_family = AF_INET6;server.sin6_port = htons(SERVER_PORT);Inet_pton(AF_INET6, SERVER_IPV6, &amp;server.sin6_addr);int temp;if ((temp = connect(sockfd, (struct sockaddr *) &amp;server, sizeof(server)) )== -1)&#123; __android_log_print(ANDROID_LOG_ERROR, TAG, "can't access server %s", strerror(errno)); return -1;&#125; 后台主线程中新建了3个线程manage_data, readTun以及send_heart, 分别用于处理数据请求，读取虚接口并发送102上网请求, 以及发送定时心跳包 manage_data线程 在实验中我们重新定义了Message结构体，每次从socket读取数据时均先读取Msg_Hdr, 然后根据length字段长度读取相应的data字段 123456789struct Msg_Hdr &#123; uint32_t length; // payload 长度,不包括type, 注意协议切割 char type; //&#125;;struct Msg&#123; struct Msg_Hdr hdr; char data[MAX_MESSAGE_LENGTH];&#125;; 只要与服务器socket连接保持，则manage_data线程循环运行，在manage_data线程中主要完成以下操作: 判断心跳包是否超时，超时则修改对应状态，并关闭socket,通知java前端VPN已断开连接 从socket中读取信息结构体的Msg_Hdr部分数据，根据读取到的type和length字段决定是否继续读取data 根据type字段做对应的处理 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051n = read(fd, &amp;msg, needbs);if(n &lt; 0) &#123; LOGE("read sockfd %d error: %s \n",fd,strerror(errno)); kill_myself(); return -1;&#125;else if(n == 0) &#123; LOGE("recv 0 byte from server, close sockfd %d \n",fd); kill_myself(); return -1;&#125;else if(n == needbs)&#123; process_payload: char* ipv4_payload = msg.data; if(msg.hdr.type != 100 &amp;&amp; msg.hdr.type != 104) &#123; n = read(fd, ipv4_payload, msg.hdr.length); if(n != msg.hdr.length) &#123; LOGE("read payload error, need %d byte, read x byte\n",msg.hdr.length); if(n &lt;= 0) &#123; LOGE("读取data出错，关闭"); kill_myself(); return -1; &#125; &#125; while(n &lt; msg.hdr.length) n += read(fd, ipv4_payload + n, msg.hdr.length-n); &#125; switch(msg.hdr.type)&#123; case 101: LOGE("get 101"); recv_ipv4_addr(&amp;msg); break; case 103: LOGE("get 103"); recv_ipv4_packet(&amp;msg); break; case 104: // 心跳包,记录接收时间 s = time(NULL); LOGE("%s %ld", "收到心跳包104", s); break; default: return -1; &#125;&#125;else &#123;// 读到长度小于头长度说明可能出错(也有可能粘包,继续读取) while (n &lt; needbs) n += read(fd, ((char*)&amp;msg) + n , needbs-n); goto process_payload;&#125; 读取到103上网回应包recv_ipv4_packet:当收到103数据包，就代表收到了服务器转发的数据，这时候直接将其data字段写入tun虚接口之中即可，并更新统计信息。 1234567891011121314int write_tun(char* payload, uint32_t len) &#123; // 写虚接口，收到信息，写入虚接口 byte_in += len + 8; packet_in += 1; total_byte += len + 8; total_packet += 1; Write_nByte(tun_des, payload, len); return 0;&#125;void recv_ipv4_packet(Msg* msg) &#123; write_tun(msg-&gt;data, msg-&gt;hdr.length); debugPacket_recv(msg, msg-&gt;hdr.length);&#125; 收到104心跳包时，更新收到心跳包的实际即可 实验结果及分析 实验测试阶段，我们使用魅族和华为安卓手机进行了测试，网络环境为宿舍的Tsinghua无线网（可访问IPV6）以及实验室的DIVI网络， 开始VPN后，可以正常连接，测试了多种网络传输对象和环境： 浏览器打开网页，网页加载速度流畅 斗鱼直播，视频播放流畅 微信，qq文字消息，表情包消息等发送接收正常 经过测试可以验证实现基本正确，且我们也测试过其稳定性，在IPV6网络稳定条件下，VPN服务不会异常中断，且手动断开VPN后可重新点击连接VPN按钮正常连接。 遇到的问题 连接VPN之后无法与服务器之间进行通信 C后台请求到IPV4地址信息之后，前端建立好VPN服务之后读取虚接口向服务器发送数据流量信息时服务器收不到相应的数据包，经过检查发现在VPN的建立过程中未对C后台Socket数据进行保护，导致后台与服务端之间的ipv6 socket链接也转发到了VPN的虚接口，这样就导致102的上网请求数据包无法发送给服务器，修改方案是利用VPNService类里的protect方法保护自己的socket。 123if (!protect(Integer.parseInt(sockfd))) &#123; throw new IllegalStateException("Cannot protect the mTunnel");&#125; 如上所示，其中socketfd即为C后台与服务端链接时的Socket，通过JNI或者管道方式从C后台读取即可。 数据包读取不完整 在一开始的实现中有时候会发现会读取到非预设类型的报文，也就是说这些报文是读取不完整的，在传输过程中被截断或者读取时未读取完整，导致数据混乱，查阅相关资料后与服务器端一同重新定义Message结构体（见【manage_data线程】一节），并且在读取数据包时确保每次读取sizeof(Msg_Hdr)个字节或者msg_hdr.length个字节data字段，从而避免粘包现象。 12345678910111213141516171819202122232425262728293031size_t needbs = sizeof(struct Msg_Hdr);n = read(fd, &amp;msg, needbs);if(n &lt; 0) &#123; LOGE("read sockfd %d error: %s \n",fd,strerror(errno)); kill_myself(); return -1;&#125;else if(n == 0) &#123; LOGE("recv 0 byte from server, close sockfd %d \n",fd); kill_myself(); return -1;&#125;else if(n == needbs)&#123; process_payload: if(msg.hdr.type != 100 &amp;&amp; msg.hdr.type != 104) &#123; n = read(fd, ipv4_payload, msg.hdr.length); if(n != msg.hdr.length) &#123; /*做出对应处理*/ &#125; while(n &lt; msg.hdr.length) n += read(fd, ipv4_payload + n, msg.hdr.length-n); &#125; /* * 处理读取到的数据 */&#125;else &#123;// 读到长度小于头长度说明可能出错(也有可能粘包,继续读取) while (n &lt; needbs) n += read(fd, ((char*)&amp;msg) + n , needbs-n); goto process_payload;&#125; 异常退出问题 在测试时发现有时候应用程序会突然崩掉，直接被系统kill掉，输出调试信息后发现总是会报如下错误 105-10 19:28:45.915 24188-24188/com.example.ipv4_over_ipv6 A/libc: Fatal signal 5 (SIGTRAP), code 1 in tid 24188 (.ipv4_over_ipv6) Google之后发现是因为通过JNI方式调用的C后台的部分函数忘记了return, 而编译app时并不会检查出这种错误，而运行时检测出该问题之后则会直接kill整个程序，解决方法是添加对应的返回值语句即可。 实验心得体会 在这次实验中，我们掌握了Android下应用程序开发环境的搭建和使用，理解了IPV4 over IPv6的原理，掌握了Android下VPN的原理及实现，掌握了JNI调用原理，并实现了一个测试通过，效果良好，性能稳定的客户端。除此之外，我们对于原理课上所讲的IPV4 over IPv6又有了更深的理解和体会，对IPV6数据包和IPV4数据包的格式和特点有了更深入的认识，加深了对socket的理解，特别是对数据包的读取粘包问题的处理，让我们对网络包的传输有了更深入的认识。 感谢老师和助教的悉心指导。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[密码学:设计一个安全的（没有中间人攻击）的Diffie-Hellman密钥交换协议]]></title>
      <url>%2Farticle%2F%E5%AF%86%E7%A0%81%E5%AD%A6%E7%AC%AC%E4%B8%89%E6%AC%A1%E5%A4%A7%E4%BD%9C%E4%B8%9A.html</url>
      <content type="text"><![CDATA[设计一个安全的（没有中间人攻击）的Diffie-Hellman密钥交换协议 DH算法描述 DH密钥交换算法是为了使两个用户能够安全地交换密钥的密钥交换协议。 假定Alice和Bob期望在一个不安全的网络中协商一个共同的密钥，那么进行如下步骤： 双方约定大素数\(q\)和它的一个本原根\(g\), 其中\(g&lt;q\)。 Alice随机产生一个数(私钥) \(a\),\(a &lt; q\),并计算公钥\(Y_A = g^a \bmod q\), 发送给Bob。 Bob随机产生一个数(私钥) \(b\),\(b&lt;q\), 并计算公钥\(Y_B= g^b \bmod q\)，发送给Alice。 此时， Alice手握Bob发过来的公钥\(Y_B\)，结合自己产生私钥a计算会话密钥：\[K = Y_B^a \bmod q = (g^b \bmod q)^a \bmod q = g^{ab} \bmod q\] Bob也拿到了Alice发来的公钥\(Y_A\)，同时结合自己的私钥b，计算会话密钥：\[K = Y_A^b \bmod q = (g^a \bmod q)^b \bmod q = g^{ab} \bmod q\] 这样Alice和Bob都得到了相同的会话密钥K，即通过DH算法完成了密钥交换。 DH协议的安全性依赖于离散对数的安全性，其假定它假定当q足够大时，通过公共值\(g^a \bmod q\)和\(g^b \bmod q\)无法计算出共享的secret key，即\(K = g^{ab} \bmod q\)。 DH算法的中间人攻击漏洞描述 D-H的密钥交换容易被中间人攻击。其攻击思路如下: Alice 发送公钥\(Y_A\)给Bob, 中间人EVE截取该值，并选择了一个自己的私钥\(c\)，计算出自己的公钥\(Y_C = g ^ c \bmod q\)然后发送自己的公钥给Bob Bob向Alice传递自己的公钥\(Y_B\)时，也被中间人EVE截获该值,EVE代替Bob发送它自己的公共值\(Y_C\)给Alice 此时，Alice收到中间人EVE的公钥，Alice和EVE计算出会话密钥\(K_1 = g^{ac} \bmod q\), Bob也收到中间人发送的公钥，Bob和EVE计算出会话密钥\(K_2 = g^{bc} \bmod q\) Alice和Bob都以为是和对方协商好了会话密钥，于是双方互相发送数据，Alice用\(K_1\)加密数据之后发送给Bob，EVE截获该数据，用\(K_1\)解密，即可查看Alice发送给Bob的数据，Eve还可对其进行修改，然后用\(K_2\)加密发送给Bob，这时Bob收到的消息已经被中间人Eve窥探甚至篡改，但Bob对此毫不知情。 这就是中间人攻击的原理。 DH算法的防止中间人攻击的方法—–签名认证 DH算法易被攻击的原因 D-H之所以容易被中间人攻击，是因为key在交换时并不对其参与者进行认证。可能的解决办法是使用数字签名，以及使用其它的协议变种。 DH + 签名认证方案的具体描述 A与B都提供自己的公钥/密钥对和公钥的证书，A根据一些消息来计算一个签名，其中包括公共值\(g^a mod q\)，B也做类似的运算。即使C仍然能够截取A与B之间的信息，但他并不能在没有A和B的密钥的情况下伪造签名，因此，这个增强的协议抵挡了中间人攻击。 具体实现为以下步骤： 会话开始前准备工作： 首先通信双方在交换协议前每一方都有一个用于签名的非对称密钥对，这个用来做签名认证，密钥对的公钥需要在会话开始前共享，即会话前，Alice的签名非对称密钥对\((P_A, S_A)\), 其中公钥\(P_A\)全局公开，Bob的签名非对称密钥对\((P_B, S_B)\),其中公钥\(P_B\)全局公开 双方约定大素数\(q\)和它的一个本原根\(\alpha\), 其中\(\alpha&lt;q\)，这些步骤在会话开始前共享能够降低会话完成的复杂性。 会话过程（注意：如果其中某一个步骤未完成，则整个协议交换过程停止）： Alice随机产生一个数(私钥) \(x\),\(x &lt; q\),并计算公钥\(Y_A = \alpha^x \bmod q\), 发送给Bob。 Bob随机产生一个数(私钥) \(y\),\(y&lt;q\), 并计算公钥\(Y_B= \alpha^y \bmod q\) Bob 会话密钥：\[K = Y_A^y \bmod q = (\alpha^x \bmod q)^y \bmod q = \alpha^{xy} \bmod q\] Bob计算\((\alpha^y, \alpha^x)\)的hash, 然后使用非对称私钥\(S_B\)对该hash进行签名，然后使用会话密钥K对该签名进行加密，Bob将加密后的签名\(E_K(S_B(\alpha_y, \alpha^x))\)及公钥\(\alpha^y\)发送给Alice 此时， Alice手握Bob发过来的公钥\(Y_B\)，结合自己的私钥x计算会话密钥：\[K = Y_B^x \bmod q = (\alpha^y \bmod q)^x \bmod q = \alpha^{xy} \bmod q\] Alice使用会话密钥K解密签名消息，再用Bob的签名公钥\(P_B\)验证Bob的签名。 Alice计算\((\alpha^x, \alpha^y)\)的hash, 然后使用非对称私钥\(S_A\)对该hash进行签名，然后使用会话密钥K对该签名进行加密，Alice将加密后的签名\(E_K(S_A(\alpha_x, \alpha^y))\)发送给Bob。 Bob利用会话密钥和Alice的公钥\(P_A\)解密并验证Alice的签名 以上所有步骤顺利完成之后Alice与Bob相互认证，并且拥有了共享密钥，K,密钥K可由于加密进一步的通信，该算法的过程如下图所示 签名认证的DH算法 DH + 签名认证防中间人攻击的原理描述 没有中间人攻击时的密钥交换过程 123Alice -&gt; ax -&gt; BobAlice &lt;- ay, EK(SB(ay, ax)) &lt;- BobAlice -&gt; EK(SA(ax, ay)) -&gt; Bob 假设有一个中间人Eve从中截获消息并篡改消息 12Alice -&gt; ax -&gt; Eve -&gt; ab -&gt; BobAlice &lt;- aa, EK1(SB(ay, ab)) &lt;- Eve &lt;- ay, EK2(SB(ay, ab)) &lt;- Bob 由上述描述可以发现，若中间人Eve想完成攻击，会在算法第4步，Bob将公钥\(\alpha^y\)和加密后的签名发给Alice时，中间人Eve截获，虽然Eve可以解密并得到hash串，但是由于Eve没有Bob的用于签名的私钥\(A_B\),因此，Eve无法伪造出一条Bob的签名，那么第5步Alice收到消息后会发现签名验证失败，继而发现协议交换过程不安全，然后停止密钥交换过程，那么Eve的攻击就此失败，由此可见，签名认证的确可以有效防止中间人攻击。 参考文献 李必涛,徐赐文,王晓菲,贾杰,郭远,郑辉. 一种能够抵抗主动攻击的改进Diffie-Hellman密钥协商方案[J]. 中央民族大学学报(自然科学版),2008,(04):54-57. 徐恒,陈恭亮,杨福祥. 密钥交换中中间人攻击的防范[J]. 信息安全与通信保密,2009,(02):90-92. Diffie–Hellman key exchange. (2017, March 22). In Wikipedia, The Free Encyclopedia. Retrieved June 2, 2017, from https://en.wikipedia.org/wiki/Diffie–Hellman_key_exchange Station-to-Station protocol. (2017, March 28). In Wikipedia, The Free Encyclopedia. Retrieved 15:50, June 6, 2017 , from https://en.wikipedia.org/wiki/Station-to-Station_protocol Blakewilson, S., &amp; Menezes, A. (1999). Unknown Key-Share Attacks on the Station-to-Station (STS) Protocol. International Workshop on Practice and Theory in Public Key Cryptography (Vol.1560, pp.154-170). Springer-Verlag. Diffie, W., Oorschot, P. C. V., &amp; Wiener, M. J. (1992). Authentication and authenticated key exchanges. Designs, Codes and Cryptography, 2(2), 107-125.]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[密码学实验2]]></title>
      <url>%2Farticle%2F%E5%AF%86%E7%A0%81%E5%AD%A6%E5%AE%9E%E9%AA%8C2.html</url>
      <content type="text"><![CDATA[接口函数 AES: 12void Crypt_Enc_Block(unsigned char *input,int in_len,unsigned char*output,int *out_len, unsigned char *key,int keylen); AES加密过程涉及4种操作： 字节替代（SubBytes） 行移位（ShiftRows） 列混淆（MixColumns） 轮密钥加（AddRoundKey） 通用的序列密码RC4 12void RC4(unsigned char *input,int in_len,unsigned char *output,int*out_len,unsigned char *key,int keylen); RonRivest设计。密钥长度可变 状态:256个bytes – State[256]=[0,1,2,…,255] 密钥:40至2048比特可变 初始化算法和伪随机子密钥生成算法 RC4于1987年提出，和DES算法一样，是一种对称加密算法，也就是说使用的密钥为单钥（或称为私钥）。但不同于DES的是，RC4不是对明文进行分组处理，而是字节流的方式依次加密明文中的每一个字节，解密的时候也是依次对密文中的每一个字节进行解密。 RC4算法的特点是算法简单，运行速度快，而且密钥长度是可变的，可变范围为1-256字节(8-2048比特)，在如今技术支持的前提下，当密钥长度为128比特时，用暴力法搜索密钥已经不太可行，所以可以预见RC4的密钥范围任然可以在今后相当长的时间里抵御暴力搜索密钥的攻击。实际上，如今也没有找到对于128bit密钥长度的RC4加密算法的有效攻击方法。 在介绍RC4算法原理之前，先看看算法中的几个关键变量： 密钥流：RC4算法的关键是根据明文和密钥生成相应的密钥流，密钥流的长度和明文的长度是对应的，也就是说明文的长度是500字节，那么密钥流也是500字节。当然，加密生成的密文也是500字节，因为密文第i字节=明文第i字节^密钥流第i字节； 状态向量S：长度为256，S[0],S[1]…..S[255]。每个单元都是一个字节，算法运行的任何时候，S都包括0-255的8比特数的排列组合，只不过值的位置发生了变换； 临时向量T：长度也为256，每个单元也是一个字节。如果密钥的长度是256字节，就直接把密钥的值赋给T，否则，轮转地将密钥的每个字节赋给T； 密钥K：长度为1-256字节，注意密钥的长度keylen与明文长度、密钥流的长度没有必然关系，通常密钥的长度取为16字节（128比特）。 RC4的原理分为三步： 12345678910111213141516171、初始化S和Tfor i=0 to 255 do S[i]=i; T[i]=K[ i mod keylen ];2、初始排列Sj=0;for i=0 to 255 do j= ( j+S[i]+T[i])mod256; swap(S[i],S[j]);3、产生密钥流i,j=0;for r=0 to len do //r为明文长度，r字节 i=(i+1) mod 256; j=(j+S[i])mod 256; swap(S[i],S[j]); t=(S[i]+S[j])mod 256; k[r]=S[t]; SM3: 12void SM3(unsigned char *InMessage,int MessageLen,unsigned char *OutDigest,int *DigestLen); 寻找SM3的高56比特的碰撞.（提交源程序和碰撞消息对）]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[数值分析实验]]></title>
      <url>%2Farticle%2F%E6%95%B0%E5%80%BC%E5%88%86%E6%9E%90%E5%AE%9E%E9%AA%8C.html</url>
      <content type="text"><![CDATA[实验一 1.3 实验内容 编程观察无穷级数\[\sum_{n=1}^\infty\frac{1}{n}\]的求和运算. 采用IEEE单精度浮点数,观察当n为何值时求和结果不再变化,将它与理论分析的结论进行比较（注：在MATLAB中可用single命令将变量转成单精度浮点数）。 用IEEE双精度浮点数计算（1）中前n项的和，评估IEEE单精度浮点数计算结果的误差。 如果采用IEEE双精度浮点数，估计当n为何值时求和结果不再变化，这在当前做实验的计算机上大概需要多长的计算时间？ 实现思路： 第一问：采用IEEE单精度浮点数, 使用MATLAB的single命令即为IEEE单精度数值，使用single_sum和last_sum记录求和结果，当两个值相等时跳出while循环，保留n值，输出求和结果 不使用single即为Double型数据，对其循环1中的n次，记录求和结果 利用matlab的tic,toc组合命令记录双精度计算n次运行时间，进而推测求值不再变化时的运行时间 结果及分析 程序运行结果如下 1234双精度运算结果: 15.403683 n = 2097152时间已过 0.011240 秒。双精度运算结果: 15.133307单精度运算误差: -0.270376 数据类型 位数 符号位 指数位 尾数 有效位(10进制) float 32 1 8 23 7 double 64 1 11 52 16 1、 根据定理 1.6(即“大数吃小数”定理):\[|\frac{x_2}{x_1}|\le0.5\epsilon, x_1+x_2==x_1\]其中\(\epsilon\)为机器精度。则当前的 1/n 与当前求和值之比小于等于 \(0.5*\epsilon\)时退出循环。单精度浮点计数时的机器精度为\[2^{-24} \approx 5.960*10^{-8}，n -&gt; \infty\]时有\[\sum_{n=1}^\infty\frac{1}{n} = ln(n)\]从而当\[\frac{1}{nln(n)} \le 0.5 *5.960*10^{-8}\]时计算结果不再变化，估算该n值为2000000,这与程序所得结果2097152基本符合. 2、单精度计算前2097152项和结果与双精度计算结果绝对误差为0.270376。 3、单精度浮点计数时的机器精度为\[2^{-53} \approx 1.110*10^{-16}\]，有 \[\sum_{n=1}^\infty\frac{1}{n} = ln(n)\]从而当\[\frac{1}{nln(n)} \le 0.5 *1.110*10^{-16} = 5.55 * 10^{-17}\]时计算结果不再变化，估算该n值为\(n = 500000000000000 = 5.0*10^{14}\)， 计算2097152次双精度加法时耗时0.011240秒，则计算n次耗时\[0.011240 * \frac{n}{2097152} \approx 2679824.8291015625 s\] 估算结果 实验心得 初步接触了MATLAB的部分编程方法，对机器精度的理解更加深入了，对单精度和双精度的计算准确度也有了一个直观的了解。 实验二 2.2 实验内容 编程实现阻尼牛顿法。要求: 设定阻尼因子的初始值\(\lambda_0\)及解的误差阈值\(\epsilon\)； 阻尼因子\(\lambda\)用逐次折半法更新； 打印每个迭代步的最终值及近似解。 用所编程序求解： \(x^3 - x - 1 = 0\)，取\(x_0 = 0.6\) \(-x^3 + 5x = 0\), 取\(x_0 = 1.2\) 实现思路 阻尼牛顿法中迭代新解为 \[x_{k+1} = x_k - \lambda_i \frac{f(x_k)}{f^\prime(x_k)}\] 单调性要求为\[|f(x_{k+1})|&lt;|f(x_k)|, k = 0,1,2,\cdots\] 算法按照课本中算法2.5的伪代码实现 结果及分析 1234567891011121314151617181920212223242526272829303132333435363738394041========阻尼牛顿法========初始值x_0:0.6000000000 lambda:1.000000x_1: 1.1406250000 lambda:0.015625x_2: 1.3668136616 lambda:1.000000x_3: 1.3262798040 lambda:1.000000x_4: 1.3247202256 lambda:1.000000x_5: 1.3247179572 lambda:1.000000迭代解为: 1.32471795725 2.04494199352e-11初始值x_0:1.2000000000 lambda:1.000000x_1: -1.9411764706 lambda:0.250000x_2: -2.3204623232 lambda:1.000000x_3: -2.2404594360 lambda:1.000000x_4: -2.2360808552 lambda:1.000000x_5: -2.2360679776 lambda:1.000000迭代解为: -2.23606797761 1.1124381416e-09========牛顿法========初始值x_0:0.6000000000x_1: 17.9000000000x_2: 11.9468023286x_3: 7.9855203519x_4: 5.3569093148x_5: 3.6249960329x_6: 2.5055891901x_7: 1.8201294223x_8: 1.4610441099x_9: 1.3393232243x_10: 1.3249128677x_11: 1.3247179926迭代解为: 1.32471799264 1.50938453736e-07初始值x_0:1.2000000000x_1: -5.0823529412x_2: -3.6219359167x_3: -2.7660437838x_4: -2.3576006646x_5: -2.2448622370x_6: -2.2361193864x_7: -2.2360679793迭代解为: -2.23606797927 1.77279613212e-08 本实验中，\(\lambda\)均只在第一步迭代中有折半运算，其后都未进入折半循环，减少了总体计算次数，这是一个好结果。 实验三 3.6 实验内容 编程序生成Hilbert矩阵\(\mathbf H_n\)，以及n维向量\(b = \mathbf H_nx\)，其中x为所有分量都是1的向量。用Cholesky分解算法求解方程\(\mathbf H_nx = b\)，得到近似解\(\hat x\)，计算残差\(r = b - H_n\hat x\) 和误差 \(\Delta x = \hat x - x\) 的\(\infty-\)范数。 设n=10，计算\(\| r \|_\infty、 \| \Delta x \|_\infty\)。 在右端项上施加\(10^{-7}\)的扰动然后解方程组，观察残差和误差的变化情况。 改变n的值为8和12，求解相应方程，观察\(\| r \|_\infty、 \| \Delta x \|_\infty\)的变化情况。通过这个实验说明了什么问题？ 实现思路 根据Hilbert矩阵的结构，生成n阶矩阵\(\mathbf H\)，继而得到向量\(b = \mathbf H_nx\)； 用Cholesky分解算法求解方程\(\mathbf H_nx = b\)，得到近似解\(\hat x\)，具体过程为： 首先对\(\mathbf H_n\)进行Choklesky分解得到下三角矩阵\(L_n\) \(\mathbf H_nx = b, H_n = LL^T\), 则先计算\(LY = b\)得到Y, 然后计算\(L^Tx = Y\),得到\(x\) 计算残差和误差的\(\infty-\)范数。 结果及分析 123456789101112131415161718n = 8 扰动 = 0.0误差x无穷范数: 6.27198750047e-07残差r无穷范数: 2.22044604925e-16n = 8 扰动 = 1e-07误差x无穷范数: 0.0216211911554残差r无穷范数: 4.4408920985e-16n = 10 扰动 = 0.0误差x无穷范数: 0.000643781555903残差r无穷范数: 8.881784197e-16n = 10 扰动 = 1e-07误差x无穷范数: 0.699727076702残差r无穷范数: 4.4408920985e-16n = 12 扰动 = 0.0误差x无穷范数: 0.355545598229残差r无穷范数: 4.4408920985e-16n = 12 扰动 = 1e-07误差x无穷范数: 23.9972906485残差r无穷范数: 1.11022302463e-15 n = 10, 无扰动时 \[\| r \|_\infty = 8.881784197e-16\] \[\| \Delta x \|_\infty = 0.000643781555903\] n = 10, 扰动\(10^{-7}\)时 \[\| r \|_\infty = 4.4408920985e-16\] \[\| \Delta x \|_\infty = 0.699727076702\] n = 8时 \[\| r \|_\infty = 2.22044604925e-16\] \[\| \Delta x \|_\infty = 6.27198750047e-07\] n = 12时 \[\| r \|_\infty = 4.4408920985e-16\] \[\| \Delta x \|_\infty = 0.355545598229\] 进一步实验，统计实验结果 N \(\| \Delta x \|_\infty\) 引入\(10^{-7}\)扰动后 \(\| r \|_\infty\) 引入\(10^{-7}\)扰动后 8 6.27198750047e-07 0.0216211911554 2.22044604925e-16 4.4408920985e-16 10 0.000643781555903 0.699727076702 8.881784197e-16 4.4408920985e-16 12 0.355545598229 23.9972906485 4.4408920985e-16 1.11022302463e-15 由上述结果分析可以得出如下结论： 无论是施加扰动还是改变矩阵的阶数n, 对残差的无穷范数\(\| r \|_\infty\)影响不大，但误差的无穷范数\(\| \Delta x \|_\infty\)变化明显 矩阵的阶数n增大，误差的无穷范数\(\| \Delta x \|_\infty\)增加明显，问题的条件数 \[\begin{align} cond &amp;= \frac{\|\Delta x\|/\|x\|}{\|\Delta b\|/\|b\|} \\ &amp;= \frac{\|\Delta x\|/\|x\|}{\frac{\|\Delta b\|}{\|H\|}/\|x\|} \\ &amp;= \frac{\|\Delta x\|}{\|\Delta b\|}\|H\| \\ &amp;= \frac{\|\Delta x\|}{\|r\|} \|H\| \end{align}\] 希尔伯特矩阵是一个典型的病态矩阵，其条件数\(cond(H_n)_\infty &gt; 1\) 且随着矩阵的阶数n越大，其病态性越严重，因此本题中线性方程组求解问题也是敏感的。 实验四 4.1 实验内容 考虑10阶\(Hilbert\)矩阵作为系数阵的方程组\[Ax = b\] 其中，A的元素\(a_{ij}=\frac{1}{i+j-1}\), \(b = \begin{bmatrix}1&amp;\frac{1}{2}&amp;\cdots&amp;\frac{1}{10}\end{bmatrix}^T\).取初始解\(x^{(0)} = 0\),编写程序用\(Jacobi\)与\(SOR\)迭代法求解该方程组，将\(\|x^{(k+1)} - x^{(k)}\|_\infty &lt; 10^{-4}\)作为终止迭代的判据。 分别用\(Jacobi\)与\(SOR(w = 1.25)\)迭代法求解，观察收敛情况； 改变\(w\)的值，试验\(SOR\)迭代法的效果，考察解的准确度。 实现思路 根据教程中的\(Jacobi\)迭代算法和\(SOR\)迭代算法的伪代码完成编码 因为A是\(Hilbert\)矩阵，是实对称矩阵，其谱半径\(\rho(A) = \|A\|_2 &gt; 1\)，其\(Jacobi\)迭代法不收敛。 对于\(SOR\)迭代法的准确度，我们使用误差\(\Delta x = \hat x - x\)的无穷范数进行量化，易知线性方程组的正确解\(x=\begin{bmatrix}1&amp;0&amp;\cdots&amp;0\end{bmatrix}^T\) 结果及分析 1234567891011121314151617181920212223242526272829雅克比迭代法jacobi迭代不收敛w = 1.25迭代次数: 187迭代结果: [1.002022594409028, -0.021609987149109246, 0.05149585866739306, -0.028416470227697905, -0.006259608843587831, -0.006192548765741887, -0.0005373689012403876, 0.0018347675968743396, 0.003636620501503592, 0.004567285710065993]w = 1.6迭代次数: 493迭代结果: [1.0021469657102071, -0.0283570320963052, 0.08880081215699734, -0.09535773222087951, 0.04336165034147876, -0.0355509984828663, 0.02513571167799267, -0.010543151501989416, 0.013356482959673942, -0.0023146920643427986]w = 1.4迭代次数: 270迭代结果: [1.0022700223737249, -0.027450984607899714, 0.07485459487277796, -0.05746975449051693, 0.005911675449980705, -0.012463346972930323, 0.0036081977048833262, 0.0015499835906313381, 0.00513317184635749, 0.004901511994768535]w = 1.2迭代次数: 164迭代结果: [1.0019214723944498, -0.019188626689019883, 0.04270174478597167, -0.02008673411613389, -0.007120242694937775, -0.005157615844817916, -0.0009805683321413268, 0.0013857829440947498, 0.0029820023152041636, 0.0039040390400999197]w = 1.0迭代次数: 2迭代结果: [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]w = 0.8迭代次数: 178迭代结果: [0.9973420757985196, 0.022189810579507762, -0.03692216721730894, 0.0002172210934328124, 0.012508220958753955, 0.010340953138668948, 0.0047473091369648925, -0.00029255227286110113, -0.003947813463373296, -0.006348095947258598]w = 0.6迭代次数: 271迭代结果: [0.9955833379103364, 0.038088293718681644, -0.06191668688342534, -0.008191119893787442, 0.022657640869212896, 0.024947505162692972, 0.014412072529676016, 0.0013699258800427523, -0.009816061664535391, -0.018012206700100845]w = 0.4迭代次数: 397迭代结果: [0.9932920658677347, 0.05517010926044667, -0.08070115313478526, -0.023914639520845007, 0.024971861498459945, 0.03919911225602069, 0.029505998466182924, 0.008987379128371036, -0.013671954578940389, -0.03395765285066289]w = 0.2迭代次数: 657迭代结果: [0.9886002689546933, 0.081327354825667, -0.09358751789761771, -0.048223519275488166, 0.012311655912579827, 0.04317339038553021, 0.043997018466545454, 0.024122149074273676, -0.007160625602977868, -0.0428600869164585] w \(\| \Delta x \|_\infty\) 迭代次数 1.6 0.0953577322209 493 1.4 0.0748545948728 270 1.25 0.0514958586674 187 1.2 0.042701744786 164 1.0 0.0 2 0.8 0.0369221672173 178 0.6 0.0619166868834 271 0.4 0.0807011531348 397 0.2 0.0935875178976 657 \(Jacobi\)迭代法不收敛,\(SOR\)迭代法(\(w = 1.25\))迭代187次后收敛，得到近似解\(\hat x\) = [1.002022594409028, -0.021609987149109246, 0.05149585866739306, -0.028416470227697905, -0.006259608843587831, -0.006192548765741887, -0.0005373689012403876, 0.0018347675968743396, 0.003636620501503592, 0.004567285710065993], 误差的无穷范数为\(\| \Delta x \|_\infty = 0.0514958586674\) 选取不同的w值，得到的迭代次数和误差的无穷范数变化如上表所示，可以得到的结论是w值越接近1，其收敛速度越快，误差越小。 实验心得 对\(Jacobi\)迭代法和\(SOR\)迭代法的收敛条件有了更深入的认识，对\(SOR\)迭代法中松弛因子\(w\)的大小对迭代算法收敛速度及计算准确度的影响有了直观的感受，针对一类应用问题选择合适的松弛因子是一个重要问题。 实验五 5.1 实验内容 用幂法求下列矩阵按模最大特征值\(\lambda_1\)及其对应的特征向量\(x_1\)，使\(\|(\lambda_1)_{k+1} - (\lambda_1)_k\| &lt; 10^{-5}\)。 \(A = \begin{bmatrix}5&amp;-4&amp;1\\-4&amp;6&amp;-4\\1&amp;-4&amp;7\end{bmatrix}\) \(B = \begin{bmatrix}25&amp;-41&amp;10&amp;-6\\-41&amp;68&amp;-17&amp;10\\10&amp;-17&amp;5&amp;-3\\-6&amp;10&amp;-3&amp;2\end{bmatrix}\) 实现思路 按照课本中算法5.1实现算法 结果及分析 迭代次数 16 主特征向量: [ 0.67401996 -1. 0.8895594 ] 主特征值: 12.2543197032 迭代次数 6 主特征向量: [-0.60397234 1. -0.25113513 0.14895345] 主特征值: 98.5216977228 实验六 6.3 实验内容 对物理实验中所得的下列数据 \(t_i\) 1 1.5 2 2.5 3.0 3.5 4 4.5 \(y_i\) 33.40 79.50 122.65 159.05 189.15 214.15 238.65 252.2 \(t_i\) 5.0 5.5 6.0 6.5 7.0 7.5 8.0 \(y_i\) 267.55 280.50 296.65 301.65 310.40 318.15 325.15 用公式\(y = a + bt + ct^2\)做曲线拟合. 用指数函数\(y = ae^{bt}\)做曲线拟合. 比较上述两条拟合曲线，哪条更好？ 实现思路 参考课本算法6.2（用法方程方法求解曲线拟合的最小二乘问题）。 首先根据式（6.28）形成矩阵A，继而计算出\(G = A^TA\)和向量\(b = A^Tf\)，得到方程\(Gx = b\)。 解上述所得方程。可以采用\(Cholesky\)分解然后执行前代和回代过程,得到的向量即为多项式函数的各项系数（低次到高次）。 用指数函数拟合时，可以对函数式两边同时取对数，转化为一次多项式函数的曲线拟合问题。 结果及分析 拟合结果\(y=-45.29423077+94.19429218t-6.12682612t^2\)， 均方误差: 5.68393182348 拟合结果\(y=67.39379285e^{0.23898344t}\)， 均方误差: 63.1840873401 通过比较其均方误差可知：用2次多项式函数拟合曲线比指数函数拟合曲线好，其拟合图像如下所示 拟合图像]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[链接结构分析实验]]></title>
      <url>%2Farticle%2F%E9%93%BE%E6%8E%A5%E7%BB%93%E6%9E%84%E5%88%86%E6%9E%90%E5%AE%9E%E9%AA%8C.html</url>
      <content type="text"><![CDATA[实验内容 基于维基百科网站中各网页之间的链接关系图，计算该网站中各网页对应的PageRank值。 数据格式 12head:node1,node2,…,noden\n源网页:目标网页1,目标网页2,…,目标网页n\n 实现 按照课件中的PageRank算法完成实现即可, 其中跳出参数aplha = 0.15, 迭代次数TN = 30 链接结构文件在results.txt文件中 运行结果 PageRank算法结果分布情况，PageRank得分与相应条目语义内容的分析 PageRank值前10的页面为 网页名 ID PageRank OutDegree InDegree 箭头 25883781 0.0222736998255 5 45 &lt;- 27117327 0.0172915137241 2 228676 维基数据 26530938 0.00480022230925 14 196429 Unicode 27003594 0.00390634021739 170 945 符号 27013210 0.00383295720591 78 283 中国 27116231 0.00129406053337 2287 68798 美国 27116867 0.00115684939603 1741 70222 学名 27108158 0.00108490964783 20 68384 法国 27114727 0.000980044130108 629 54791 市镇 27079331 0.00097233210881 10 64201 由结果可以发现箭头和&lt;-是两个页面结果奇地高，分析这两个页面可以发现，“箭头”和“&lt;-”两个页面的出度都比较小，而‘&lt;-’入度特别高，这在一定程度上使得其PageRank值偏高，而’箭头’页面的入度实际上并不高，但其PageRank值却是最高的，查询对应网页内容，发现’&lt;-’页面其实是维基百科的重定向页面，有大量页面指向该页面，因此其PageRank值会偏高 &lt;-出度 箭头出度1 箭头出度2 ‘&lt;-’页面由于有很多页面指向，因此其PageRank值偏高是比较合理的，而“箭头”页面的入度实际上并不高，但是其PageRank值却是最高的，进一步分析“箭头页面”，查看“箭头”页面的出度，发现其5个出页面中有两个指向了自己，进一步查看‘&lt;-’页面的出度，发现其两个出度都是’25883781’,即“箭头页面”，由PageRank迭代过程中PageRank值更新过程（PageRank算法图所示），‘&lt;-’页面由于入度大故PageRank值较高，而该页面又全部指向了‘箭头’页面，由算法图中标记位置可知，“箭头”页面的PageRank值的确会特别高 由此可以得出结论，计算PageRank时一定要注意数据的清洗，清除一些不必要的页面，PageRank值排名第2和3的页面实际的重要程度并不高，但是由于其是分类或者重定向页面而显得比较高，这实际上影响了其页面重要性的真实度。 PageRank算法图 算法结果分布情况 维基百科语料库中入连接数/出连接数分布情况 维基百科语料库中入连接数/出连接数分布情况 PageRank与入链接数的关联分析 通过计算PageRank值和入连接数的相关系数，可知其相关性不大]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[集成学习知识点]]></title>
      <url>%2Farticle%2F%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E7%9F%A5%E8%AF%86%E7%82%B9.html</url>
      <content type="text"><![CDATA[两个概念 强学习器：高准确率 弱学习器：低准确率（比随机猜测略好） 基本思想 把几个弱分类器加权融合提升效果 每个算法池中的算法有自己的权值 当用于预测一个新的测例时，每个分类器给出自己的预测，总算法使用各自的权值加权得出结果 加权多数算法 加权多数算法 权值为0或1 二值输出，权重为0/1 算法流程，权值变化 每次失败除2， 避免特别小（直接置为0） Bagging算法 Bagging = Bootstrap aggregating 如果只有一个弱分类器，如何进行集成学习 自举法采样 给定一个包含m个训练集的数据集D 有放回采样取m个数据组成训练集\(D_i\) \(D_i\)可能存在重复，存在漏选 12345For t = 1, 2, …, T Docreate boostrap sample Dt from S train a classifier Ht on DtClassify new instance x∈X by majority vote of H t(equal weights) 可以预测连续输出 Bagging算法对结果提示是不稳定的 预测方法的稳定性问题 不稳定的算法：训练集的小变化会造成假设的大变化 如果扰动训练集可以造成预测器的显著变化那么Bagging算法可以提高其精度 特殊点 每个基础分类器在小数据集进行训练（只有63.2%的数据在所有的分数据集中） 但是最终的模型中几乎包含了所有的训练集（平均每个会被&gt;50%的bootstrap包含） 小结 加权多数算法： 同样的数据集，不同的学习算法，生成多个模型，加权预测 Bagging： 一个数据集，一个弱分类器， 生成多个训练集来训练多个模型，然后集成 Boosting算法 基本思想 从错误中学习 给每个example一个权值， AdaBoost get每一个样本一个等值权重\(\frac{1}{N}\) 对1~T,训练生成Ct,计算器错误率 实现参考 adaboosting AdaBoost.M1]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[SNMP知识点]]></title>
      <url>%2Farticle%2FUntitled.html</url>
      <content type="text"><![CDATA[SNMP: 简单网络管理协议 协议层， 规程（传输过程），格式, 服务的请求和提供者，提供什么服务，获取操作的信息 变量参数的定义，安全（认证AAA, 认证授权计费） FTP协议:文件系统 网络时间同步 硬件：原子钟，GPS，运营商 软件：NTP(网络时间协议) SNMP概述 一种网络管理体系结构 系列通信协议 网络管理信息定义和标准 体系结构 抽象模型，组成关系模型 OSI 七层模型， TCP/IP四层 三网融合 ISO ITU国际电信联盟 ：TMU IETF：民间组织 IEEE：底层技术标准 WIFI]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[机器学习实验二-集成学习]]></title>
      <url>%2Farticle%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E5%AE%9E%E9%AA%8C.html</url>
      <content type="text"><![CDATA[数据集 本实验采用的数据集是 WEBSPAM-UK2006 数据集, 数据集中包含1803个垃圾网站，4411个正常网站，每个网站有96个内容特征，有138个超链接标签。这138个超链接标签可以被分为五类：Degree-related features, PageRank-related features, TrustRank-related features, Truncated PageRank-related features, Supporter-related features 在本实验中，需要将数据集划分为4：1的训练集和测试集 实验任务 比较由不同的基本分类器组成的不同的集成学习算法的性能 至少需要包含Bagging 算法和 AdaBoost.M1算法 至少需要使用SVM和Decision Tree两种基本分类器算法 至少需要完成的算法组合为：Bagging + DTree, Bagging + SVM, AdaBoost.M1 + DTree, AdaBoost.M1 + SVM. 扩展任务 尝试其他的基本分类器（K-NN, Naive Bayes…） 分析不同的特征的影响（比如可以只使用内容特征，或者只使用链接特征，或者进行特征组合） 考虑特征缩放（归一化）来提高性能 数据集是不平衡的，如何克服 实验设计 基础分类器选取了SVM，Dtree, 和KNN 基础分类器采用了sklearn的包，由于KNN算法无法设置训练样本的权中，因此进行了如下组合： Bagging + DTree Bagging + SVM Bagging + KNN AdaBoost.M1 + DTree AdaBoost.M1 + SVM. 实验结果 针对Bagging和AdaBoost.M1集成学习算法 分别使用SVM,Dtree,KNN等基本分类器进行组合后，并且设置了不同的训练集大小，迭代次数（分类器个数），测试后的总体结果如下： 集成算法 基础算法 迭代次数 训练集大小 准确率 Bagging SVM 1 994 0.886565 Bagging Decision Tree 1 994 0.824618 Bagging KNN 1 994 0.849558 AdaBoost.M1 SVM 1 994 0.884956 AdaBoost.M1 Decision Tree 1 994 0.847949 Bagging SVM 21 994 0.881738 Bagging Decision Tree 21 994 0.913113 Bagging KNN 21 994 0.881738 AdaBoost.M1 SVM 21 994 0.862430 AdaBoost.M1 Decision Tree 21 994 0.881738 Bagging SVM 41 994 0.913113 Bagging Decision Tree 41 994 0.900241 Bagging KNN 41 994 0.884151 AdaBoost.M1 SVM 41 994 0.896219 AdaBoost.M1 Decision Tree 41 994 0.901046 Bagging SVM 21 1988 0.895414 Bagging Decision Tree 21 1988 0.902655 Bagging KNN 21 1988 0.885760 AdaBoost.M1 SVM 21 1988 0.873693 AdaBoost.M1 Decision Tree 21 1988 0.881738 Bagging SVM 21 3976 0.895414 Bagging Decision Tree 21 3976 0.901046 Bagging KNN 21 3976 0.897828 AdaBoost.M1 SVM 21 3976 0.901850 AdaBoost.M1 Decision Tree 21 3976 0.901046 由上表可以得出结论：对于本数据集，Bagging算法和AdaBoost算法的算法性能大致相同，组合基础算法时，KNN+性能最差，Dtree+性能最好，不过在迭代次数较少时SVM的性能较好，迭代次数增加、训练集增加时Dtree+的准确率上升较快 分析和讨论 数据预处理问题 在不对数据进行归一化处理的时候，SVM的Bagging算法表现极差，只能达到 对于SVM,Dtree, KNN三个基础分类器，进行特征归一化与否的分类效果如下所示： 是否归一化 分类器 准确率 否 SVM 0.735318 否 DTree 0.875302 否 KNN 0.781979 是 SVM 0.914722 是 DTree 0.876106 是 KNN 0.899437 可以得出结论:归一化操作对SVM,KNN等算法而言及其重要，这是因为这类分类器是利用两点间的距离计算两点间的差异，当各个特征的维度差异较大时对其分类效果影响较大，因此需要进行归一化，而这些也直接影响了集成学习算法的准确率，如下表所示 不进行归一化处理 集成算法 基础算法 迭代次数 训练集大小 准确率 Bagging SVM 21 994 0.714401 Bagging Decision Tree 21 994 0.896219 Bagging KNN 21 994 0.764280 AdaBoost.M1 Decision Tree 21 994 0.894610 （不归一化处理时，SVM在训练集上做测试时几乎会是完全正确，即出现了过拟合，这时AdaBoost算法无法进行下去，故无数据） 进行归一化处理 集成算法 基础算法 迭代次数 训练集大小 准确率 Bagging SVM 21 994 0.905873 Bagging Decision Tree 21 994 0.907482 Bagging KNN 21 994 0.893001 AdaBoost.M1 SVM 21 994 0.872888 AdaBoost.M1 Decision Tree 21 994 0.901046 可以发现，在不进行归一化处理时，对于基于SVM和KNN两种基本算法的Bagging和AdaBoost集成学习算法表现均较差，而进行了归一化处理之后，其算法准确率则大幅提高（注意，单个算法的准确率较高是因为训练时采用了全训练集，而集成学习的训练集只采用了20%的训练集，因此可能稍有偏差）而DTree的结果并无比较大的差异 由这个对比可以感受到特征归一化对部分分类器的重要性 SVM的参数设置 在实验过程中发现使用SVM算法 + AdaBoost.M1算法时会出现每轮迭代时会出现分类效果越来越差的情况，通过打印预测结果发现，其对训练集的预测会循环出现全0和全1，即这一轮迭代会将每一个测试数据分类为sapm,下一轮迭代时会将每一个测试数据分类为normal, 这样会导致最终的训练结果准确率为69%，反复调试发现是sklearn.svm.SVC参数设置问题 123456sklearn.svm.SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, decision_function_shape=&apos;ovr&apos;, degree=3, gamma=&apos;auto&apos;, kernel=&apos;rbf&apos;, max_iter=-1, probability=False, random_state=None, shrinking=True, tol=0.001, verbose=False) SVC的参数C代表了其惩罚参数，相当于惩罚松弛变量，松弛变量接近0时，即对误分类的惩罚增大，趋向于对训练集全分对的情况，这样对训练集测试时准确率很高，但泛化能力弱。默认为1时，训练时加上了各样本参数时则会对其训练效果产生极大影响，最终导致分类错误率增加，将其设置为300之后则效果变好。 算法表现差异分析 将上述测试结果按照准确率进行排序结果如下： 集成算法 基础算法 迭代次数 训练集大小 准确率 Bagging Decision Tree 1 994 0.824618 AdaBoost.M1 Decision Tree 1 994 0.847949 Bagging KNN 1 994 0.849558 AdaBoost.M1 SVM 21 994 0.86243 AdaBoost.M1 SVM 21 1988 0.873693 Bagging SVM 21 994 0.881738 Bagging KNN 21 994 0.881738 AdaBoost.M1 Decision Tree 21 994 0.881738 AdaBoost.M1 Decision Tree 21 1988 0.881738 Bagging KNN 41 994 0.884151 AdaBoost.M1 SVM 1 994 0.884956 Bagging KNN 21 1988 0.88576 Bagging SVM 1 994 0.886565 Bagging SVM 21 1988 0.895414 Bagging SVM 21 3976 0.895414 AdaBoost.M1 SVM 41 994 0.896219 Bagging KNN 21 3976 0.897828 Bagging Decision Tree 41 994 0.900241 AdaBoost.M1 Decision Tree 41 994 0.901046 Bagging Decision Tree 21 3976 0.901046 AdaBoost.M1 Decision Tree 21 3976 0.901046 AdaBoost.M1 SVM 21 3976 0.90185 Bagging Decision Tree 21 1988 0.902655 Bagging Decision Tree 21 994 0.913113 Bagging SVM 41 994 0.913113 可以发现，综合表现最好的是Bagging + Dtree算法，平均来看，Bagging集成方法优于AdaBoost算法，这与经验相悖，分析可能的原因是：实验时未考虑各特征的权重问题，每个特征的计算方法不同，归一化时并没有考虑这一点，这些可能使得当AdaBoost算法专注于那些分错的样本时其实违背了数据的特征规律。 Bagging算法和AdaBoost算法有什么区别 Bagging 算法流程： 给定一个大小为n的训练集D，Bagging算法从中均匀、有放回地选出T个大小为n1的子集Di，作为新的训练集。在这m个训练集上使用分类、回归等算法，则可得到m个模型，再通过取平均值、取多数票等方法，即可得到Bagging的结果 算法特点： Bagging要求基分类器的学习算法不稳定，也就是当数据发生小变化时，训练的分类器会产生很大不同，依次来增加基分类器的多样性，使得分类系统更加稳定，泛化能力更强。 AdaBoost 算法流程 AdaBoost方法是一种迭代算法，在每一轮中加入一个新的弱分类器，直到达到某个预定的足够小的错误率。 每一个训练样本都被赋予一个权重，表明它被某个分类器选入训练集的概率。 如果某个样本点已经被准确地分类，那么在构造下一个训练集中，它被选中的概率就被降低； 相反，如果某个样本点没有被准确地分类，那么它的权重就得到提高。 通过这样的方式，AdaBoost方法能“聚焦于”那些较难分（更富信息）的样本上。 在具体实现上，最初令每个样本的权重都相等， 对于第k次迭代操作，我们就根据这些权重来选取样本点，进而训练分类器Ck。 然后就根据这个分类器，来提高被它分错的的样本的权重，并降低被正确分类的样本权重。 然后，权重更新过的样本集被用于训练下一个分类器Ck[2]。整个训练过程如此迭代地进行下去。 算法特点 具有较低的泛化误差（low generalization） 不容易出现overfiting(过拟合)现象 算法差异 Bagging算法的训练集是从原数据集中有放回的抽样得到的（原数据集的一部分），每个基分类器是相互独立的，并列的。因为每个基分类器训练方法独立且相同，所以最后分类器等权重投票。 而在Boosting算法中，基分类器是依次训练的，因为分错的点在接下来的训练时会更加的被侧重，也就是说，每个基分类器的训练都是建立在之前基分类器的表现基础之上的。最后分类器加权投票。 通常来说，基分类器不稳定时，Bagging算法的表现会比较好，大多数情况下AdaBoost算法表现较好，但是也容易出现过拟合的情况，比如说本次实验中（【SVM的参数设置】一节所述），当基分类器为SVM时，若C参数设置为1时，则会出现最终分类准确率仅为69%的情况 什么样的组合最好，为什么 进行相应的参数调整之后，各个算法组合之间的差异并不是特别大，不过Bagging + Dtree的稳定性应该是最高的，其效果也最好，实际上Bagging + SVM的效果也比较好，此次测试SVM效果劣于Dtree可能是SVC的参数设置不够好 代码说明： ContentNewLinkAllSample.csv : 原始数据 README.md : 说明文档 base_cls.py : 基础分类器的分类效果测试, python base_cls.py即可运行 ensemble.py : 集成学习算法 main.py : 测试程序 share.py : 一些配置函数 参考资料 曾刚,李宏.一个基于现实世界的大型Web参照数据集——UK2006 Datasets的初步研究[J].企业技术开发（学术版）,2009,28(5):16-17,31. SVM: Weighted samples http://scikit-learn.org/stable/auto_examples/svm/plot_weighted_samples.html]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[实验 1 Cache 替换策略设计与分析]]></title>
      <url>%2Farticle%2FCRC_report.html</url>
      <content type="text"><![CDATA[实验目的 深入理解各种不同的 Cache 替换策略 理解学习不同替换策略对程序运行性能的影响 动手实现自己的 Cache 替换策略 实验要求 理解学习 LRU 及其它已经提出的 Cache 替换策略 在提供的模拟器上实现自己设计的 Cache 替换策略 通过 benchmark 测试比较不同的 Cache 替换策略 在实验报告中简要说明不同 Cache 替换策略的核心思想和算法 在实验报告中说明自己是怎样对不同的 Cache 替换策略进行测试的 在实验报告中分析不同替换策略下，程序的运行时间、Cache 命中率受到的影响 已有替换策略分析 常见的Cache替换策略有LRU，FIFO, LFU等，本次实验基于MK Qureshi于2007年的Adaptive insertion policies for high performance caching一文提出的LRU Insertion Policy算法进行的实现。 在该文中，作者指出，当程序重用工作集大于可用Cache时或者其存储访问具有低局部相关性时，Cache替换策略中最常用的LRU策略表现出十分低下的命中率。大量新替换入Cache的行对命中率的贡献为零，而原本可能命中的行则由于长期不被访问而替换出Cache。针对这种情况，该文提出了以下一些改进策略： LRU Insertion Policy (LIP) 。该策略将所有替换入Cache的行置于LRU端。相对于传统策略将所有替换入的行置于MRU端，LIP使一部分行得以驻留于Cache中，其驻留时间能比Cache本身容量更长。LIP策略能够很好地应对Thrashing，尤其对于循环访问内存的程序其性能近似于OPT策略。 Bimodal Insertion Policy (BIP)。BIP策略是对LIP的加强和改进，新换入的行会小概率地被置于MRU端。BIP可以很好地响应Working Set的切换，并保持一定的命中率。 Dynamic Insertion Policy (DIP) 能动态地在LRU和BIP间切换，选择其中命中率较高的策略执行后续指令。DIP对于LRU-friendly的程序块使用LRU策略，对于LRU-averse的程序块使用DIP策略，以求通用效率。对于1MB16路L2 Cache，DIP策略较LRU降低21%的失配率。 LIP和BIP两种策略对于LRU-friendly（高局部性）的程序，性能均不佳。 我实现的替换策略 考虑到实现难度，本次实验我实现了MK Qureshi提出的三个算法中的第一个LIP算法，具体的实现为： 当访存命中时，将命中的Cache块放到对应栈的LRU端，其他块依次移位 当Cache缺失时，替换策略与LRU一致，这时需要将新换入的Cache块放到对应栈的MRU端 在实际的实验过程中发现，LIP算法并没有很明显的改进LRU算法，约有\(\frac{1}{3}\)的测试样例中LIP算法表现更好，\(1/2\)的测试样例中LRU算法表现更好，由此，我采用了一个稍大概率值使得其一部分情况下采用LRU算法，一部分情况下采用LIP算法，考虑到LRU算法在大多数情况下表现较好，这里的概率值设置得稍大一些，置为了30%,即对于新换入的Cache块，有30%的概率会被放置在LRU端，有70%的概率会被置于MRU端 具体实现 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475// 这个函数是根据实现的Cache替换策略函数 INT32 CACHE_REPLACEMENT_STATE::Get_LIP_Victim( UINT32 setIndex )&#123; // Get pointer to replacement state of current set LINE_REPLACEMENT_STATE *replSet = repl[ setIndex ]; INT32 lipWay = 0; // Search for victim whose stack position is assoc-1 // 被替换的块策略与LRU一致，选择栈底被替换出去 for(UINT32 way=0; way&lt;assoc; way++) &#123; if( replSet[way].LRUstackposition == (assoc-1) ) &#123; lipWay = way; break; &#125; &#125; // return lru way return lipWay;&#125;// This function implements the LIP update routine for the traditional //// LIP replacement policy. The arguments to the function are the physical //// way and set index. void CACHE_REPLACEMENT_STATE::UpdateLIP( UINT32 setIndex, INT32 updateWayID, bool cacheHit )&#123; // Determine current LRU stack position UINT32 currLRUstackposition = repl[ setIndex ][ updateWayID ].LRUstackposition; // 如果hit // Update the stack position of all lines before the current line // Update implies incremeting their stack positions by one if (cacheHit) &#123; for(UINT32 way=0; way&lt;assoc; way++) &#123; if( repl[setIndex][way].LRUstackposition &lt; currLRUstackposition ) &#123; repl[setIndex][way].LRUstackposition++; &#125; &#125; // Set the LRU stack position of new line to be zero repl[ setIndex ][ updateWayID ].LRUstackposition = 0; &#125; else &#123; // Cache缺失时, 把新换入的Cache的行置于LRU端 repl[ setIndex ][ updateWayID ].LRUstackposition = assoc - 1; &#125; &#125;void CACHE_REPLACEMENT_STATE::UpdateBIP( UINT32 setIndex, INT32 updateWayID, bool cacheHit )&#123; // Determine current LRU stack position UINT32 currLRUstackposition = repl[ setIndex ][ updateWayID ].LRUstackposition; // 如果hit // Update the stack position of all lines before the current line // Update implies incremeting their stack positions by one if (cacheHit || (rand()%100 &lt; probability)) &#123; for(UINT32 way=0; way&lt;assoc; way++) &#123; if( repl[setIndex][way].LRUstackposition &lt; currLRUstackposition ) &#123; repl[setIndex][way].LRUstackposition++; &#125; &#125; // Set the LRU stack position of new line to be zero repl[ setIndex ][ updateWayID ].LRUstackposition = 0; &#125; else &#123; // Cache缺失时, 把新换入的Cache的行置于LRU端 repl[ setIndex ][ updateWayID ].LRUstackposition = assoc - 1; &#125; &#125; 替换策略测试 测试方法 本次实验中将29个测试程序分别采用LRU,LIP算法进行了测试，测试时的Cache设置为 UL3:1024:64:16 即 实验在第3级Cache上进行，统一存储指令和数据， Cache 大小为 1024KB Cache 行大小为 64B 16路组相连 在测试时我利用shell脚本自带的time命令输出其运行时间(取real time结果)以此代表程序运行时间，此外，我还采用了CPI,缺失率作为其评价指标 分析不同替换策略下，程序的运行时间、Cache 命中率受到的影响 LRU和LIP算法比较 程序 LRU_time LIP_time less_time LRU_CPI LIP_CPI Less_cpi LRU_miss LIP_miss less miss perlbench 17.070 16.420 LIP 0.642823 0.669326 LRU 40.7952 44.9668 LRU bzip2 123.840 120.870 LIP 0.554642 0.571873 LRU 61.7369 55.3261 LIP gcc 134.380 130.780 LIP 0.614864 0.575939 LIP 42.7521 38.5965 LIP bwaves 118.230 117.520 LIP 0.253088 0.253088 = 99.6634 99.6634 = gamess 133.080 135.110 LRU 0.30022 0.312251 LRU 26.3204 92.0923 LRU mcf 163.610 153.580 LIP 2.55358 2.04087 LIP 75.676 59.7033 LIP milc 148.670 144.780 LIP 1.14939 1.1807 LRU 77.0164 79.3471 LRU zeusmp 148.800 133.400 LIP 0.451633 0.452046 LRU 80.81 82.492 LRU gromacs 143.230 153.090 LRU 0.607769 0.664129 LRU 71.1979 89.1534 LRU cactusADM 133.370 130.880 LIP 0.44249 0.378492 LIP 76.1164 63.7024 LIP leslie3d 133.210 133.850 LRU 1.50626 1.4344 LIP 84.0284 79.7774 LIP namd 112.000 111.370 LIP 0.273795 0.273797 LRU 66.2603 66.2661 LRU gobmk 127.020 127.540 LRU 0.383876 0.388093 LRU 14.919 15.5215 LRU dealII 130.630 131.960 LRU 0.463916 0.446134 LIP 45.0892 47.2678 LRU soplex 72.310 75.080 LRU 0.356909 0.431378 LRU 15.2011 35.9993 LRU povray 134.860 134.040 LIP 0.360431 0.360387 LIP 39.9258 39.7901 LIP calculix 130.460 127.370 LIP 0.321346 0.359601 LRU 34.05 53.1214 LRU hmmer 133.820 135.260 LRU 0.258513 0.258513 = 71.4736 71.4868 LRU sjeng 129.020 134.210 LRU 0.323006 0.322841 LIP 93.5415 93.3249 LIP GemsFDTD 153.820 171.260 LRU 0.368143 0.368143 = 84.1525 84.1525 = libquantum 147.090 132.630 LIP 0.272749 0.272749 = 100.0 100.0 = h264ref 137.550 129.450 LIP 0.28787 0.292871 LRU 70.0086 72.4714 LRU tonto 122.590 122.090 LIP 0.347856 0.34803 LRU 78.8694 79.2837 LRU lbm 175.980 175.560 LIP 1.15379 1.15439 LRU 99.9725 99.7331 LIP omnetpp 156.400 150.580 LIP 0.41278 0.412729 LIP 59.4443 59.2859 LIP astar 161.890 152.050 LIP 0.267647 0.267647 = 5.84718 5.84718 = sphinx3 148.620 145.960 LIP 0.445644 0.445645 LRU 97.156 97.1648 LRU xalancbmk 163.490 155.190 LIP 0.414993 0.411322 LIP 66.2759 63.1589 LIP specrand 90.180 89.100 LIP 0.336211 0.336348 LRU 95.9373 97.6647 LRU LRU和BIP算法比较 程序 LRU_time BIP_time less_time LRU_CPI BIP_CPI Less_cpi LRU_miss BIP_miss less miss perlbench 17.070 17.490 LRU 0.642823 0.653642 LRU 40.7952 42.4235 LRU bzip2 123.840 144.210 LRU 0.554642 0.570891 LRU 61.7369 59.0281 BIP gcc 134.380 167.270 LRU 0.614864 0.575685 BIP 42.7521 37.0908 BIP bwaves 118.230 152.430 LRU 0.253088 0.253088 = 99.6634 99.6634 = gamess 133.080 159.270 LRU 0.30022 0.304011 LRU 26.3204 30.4442 LRU mcf 163.610 182.510 LRU 2.55358 2.27179 BIP 75.676 67.9679 BIP milc 148.670 137.590 BIP 1.14939 1.14853 BIP 77.0164 76.2688 BIP zeusmp 148.800 127.220 BIP 0.451633 0.451496 BIP 80.81 81.883 LRU gromacs 143.230 148.100 LRU 0.607769 0.625808 LRU 71.1979 74.1585 LRU cactusADM 133.370 141.210 LRU 0.44249 0.368551 BIP 76.1164 62.9769 BIP leslie3d 133.210 147.700 LRU 1.50626 1.44446 BIP 84.0284 73.9532 BIP namd 112.000 122.800 LRU 0.273795 0.273801 LRU 66.2603 66.2719 LRU gobmk 127.020 144.260 LRU 0.383876 0.384353 LRU 14.919 14.2733 BIP dealII 130.630 143.810 LRU 0.463916 0.42624 BIP 45.0892 35.1431 BIP soplex 72.310 79.220 LRU 0.356909 0.366633 LRU 15.2011 17.9177 LRU povray 134.860 165.980 LRU 0.360431 0.360417 BIP 39.9258 39.858 BIP calculix 130.460 145.710 LRU 0.321346 0.332007 LRU 34.05 39.1968 LRU hmmer 133.820 157.060 LRU 0.258513 0.258513 = 71.4736 71.4868 LRU sjeng 129.020 166.850 LRU 0.323006 0.322882 BIP 93.5415 93.359 BIP GemsFDTD 153.820 169.420 LRU 0.368143 0.368143 = 84.1525 84.1525 = libquantum 147.090 138.730 BIP 0.272749 0.272749 = 100.0 100.0 = h264ref 137.550 138.360 LRU 0.28787 0.289044 LRU 70.0086 70.8777 LRU tonto 122.590 143.870 LRU 0.347856 0.347883 LRU 78.8694 78.898 LRU lbm 175.980 149.190 BIP 1.15379 1.15361 BIP 99.9725 99.9688 BIP omnetpp 156.400 161.910 LRU 0.41278 0.412755 BIP 59.4443 59.3624 BIP astar 161.890 156.370 BIP 0.267647 0.267647 = 5.84718 5.84718 = sphinx3 148.620 136.150 BIP 0.445644 0.44566 LRU 97.156 97.2443 LRU xalancbmk 163.490 142.850 BIP 0.414993 0.421033 LRU 66.2759 67.5348 LRU specrand 90.180 85.870 BIP 0.336211 0.336235 LRU 95.9373 96.1932 LRU 算法表现统计 LRU与LIP算法 CPI值: LRU算法15次更小, LIP算法9次更小 运行时间:LRU算法9次更小, LIP算法20次更小 缺失率: LRU算法15次更小, LIP算法10次更小 LRU与BIP算法 CPI值: LRU算法13次更小, BIP算法11次更小 运行时间:LRU算法21次更小, BIP算法8次更小 缺失率: LRU算法13次更小, BIP算法12次更小 结果分析 由统计结果分析，大概一半的测试程序中，LRU的算法的缺失率和CPI指标比LIP算法更优，但是仍然有不少测试样例显示LIP算法表现更好，而这些程序即是前文所说的局部相关性不是特别高的情况，而LIP算法的运行时间明显更优。 新加入的第三种策略（类似于BIP算法）在缺失率上的表现较LIP稍好一些（运行时间偏慢应该是因为在其中做了取随机数和取模的运算操作），其表现与LRU表现不分上下，可以看做是LRU算法和LIP算法的一个综合，可以看出不同的算法对程序局部性的适应度是有区别的，而测试程序的局部性好坏也极大地影响着其缺失率，在实际的操作中综合LIP和LRU算法应该是比较好的。根据测试结果推测，当概率值设置为50%时BIP算法表现可能会优于LRU算法 由此可以分析，通常来说，程序的局部性更强时LRU算法表现会比较好，当程序的局部性表现有较大波动时，LIP一类改进算法会表现更好（即缺失率会降低），实际上，DIP算法的动态调节表现应该是最好的，不过出于实现难度本次实验没有进行实现。 附件： 123456789101112131415161718192021## 算法文件replacement_state.cppreplacement_state.h## 实验数据results/├── get_time.py // 通过运行数据提取运行结果表格记录├── time_bip.txt // 算法3的运行时间结果├── log_all.txt // 分别使用LRU算法和LIP算法运行29个测试程序的输出结果├── bip_log.txt // 算法3的运行记录├── name.txt├── run.sh // 运行脚本，在CRC/项目根目录下运行即可├── runs // 解压后的测试结果│ ├── LIP_GemsFDTD.stats│ ├── LIP_astar.stats│ ├── LIP_bwaves.stats│ ├── LIP_bzip2.stats| ......|└── time_log.txt // 每个程序的运行时间记录---- 参考文献 [1] Moinuddin K. Qureshi , Aamer Jaleel , Yale N. Patt , Simon C. Steely , Joel Emer, Adaptive insertion policies for high performance caching, Proceedings of the 34th annual international symposium on Computer architecture(ISCA), June 09-13, 2007, San Diego, California, USA]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[图片搜索实验]]></title>
      <url>%2Farticle%2F%E5%9B%BE%E7%89%87%E6%90%9C%E7%B4%A2%E5%AE%9E%E9%AA%8C.html</url>
      <content type="text"><![CDATA[基本任务 查找资料，跑通流程 【部署方法】 使用MyEclipse导入该工程，运行ImageIndexer建立索引生成forIndex文件夹 将forIndex文件夹拷贝到Tomcat的安装目录下（取决于具体运行环境，有的可能需要拷贝到Tomcat_ROOT/bin/目录下） 此外，由于我在ImageIndexer.java中修改了图片存放位置，因此一定要重新运行ImageIndexer.java生成forIndex文件并拷贝过去才行（也可直接拷贝提交的forIndex文件） 将图片解压至/Tomcat_ROOT/webapps/ROOT/pictures/下， 访问 http://localhost:8080/ImageSearch/imagesearch.jsp 即可进行搜索显示 根据ImageIndexer中对Sogou图片分类目录的索引，对Sogou图片搜索热门查询对应图片建立索引 首先运行 Encode_gbk_to_utf8.java将gbk编码的threeMonth.xml转换为utf-8编码格式， 1&lt;pic id=&quot;278535&quot; query=&quot;西藏&quot; total_click=&quot;6977&quot; click=&quot;14&quot; pic_url=&quot;http://travel.bjhotel.cn/tour/Article/UploadFiles/200803/2008030608595904.jpg&quot; page_url=&quot;http://travel.bjhotel.cn/tour/Article/fsyj/xz&quot; locate=&quot;pictures/threeMonth/西藏/274863.jpg&quot; /&gt; 索引结果 threeMonth.xml中对每个搜索词的描述有query，click，total_click，page_url等， 本次索引的field采取query(点击频度暂未找到合适的方法考虑到评分系统内，因此并未考虑该参数) 由于threeMonth图片集太大，而索引文件中本身有pic_url，因此,我直接采用的是将pic_url作为其picPath,同时将imageshow.jsp中的图片路径改为相对路径，确保图片正确显示（sougou图片集的ImageIndexer.java中需要在路径前加’/’） 1234567891011NamedNodeMap map=node.getAttributes();Node locate=map.getNamedItem(&quot;locate&quot;);Node url = map.getNamedItem(&quot;pic_url&quot;);// Node bigClass=map.getNamedItem(&quot;bigClass&quot;);// Node smallClass=map.getNamedItem(&quot;smallClass&quot;);Node query=map.getNamedItem(&quot;query&quot;);String absString=query.getNodeValue();Document document = new Document();// 采用网页链接作为图片地址Field PicPathField = new Field( &quot;picPath&quot; ,url.getNodeValue(),Field.Store.YES, Field.Index.NO);Field abstractField = new Field( &quot;abstract&quot; ,absString,Field.Store.YES, Field.Index.ANALYZED); 运行结果 12345678910111213141516171819202122232425262728293031323334353637process 0process 10000process 20000process 30000process 40000process 50000process 60000process 70000process 80000process 90000process 100000process 110000process 120000process 130000process 140000process 150000process 160000process 170000process 180000process 190000process 200000process 210000process 220000process 230000process 240000process 250000process 260000process 270000process 280000process 290000process 300000process 310000process 320000process 330000process 340000average length = 4.73518total 340518 documents 根据Simple实现的Lucene评分核心类，了解Lucene评分架构，在SimpleScorer.java中实现BM25模型算法（实现很简单） 给定一个查询 Q,包含了关键词 \(q_1, ..., q_n\), 利用BM25算法计算出的文档D的得分为: \[{\text{score}}(D,Q)=\sum _{i=1}^{n}{\text{IDF}}(q_{i})\cdot {\frac {f(q_{i},D)\cdot (k_{1}+1)}{f(q_{i},D)+k_{1}\cdot \left(1-b+b\cdot {\frac {|D|}{\text{avgdl}}}\right)}}\] 其中： \(f(q_i, D)\) 表示关键词\(q_i\)在文档D中的频率 |D| 是文档D的长度 avgdl 是平均文档长度 \(k_1\) 和 b 是调节因子, 通常\(k_1 \in [1.2,2.0]\) , b = 0.75 （本实验中\(k_i = 2.0\)） \(IDF(q_i)\) 是查询词\(q_i\)的IDF (inverse document frequency) 权重，是根据每个关键词在所有文档中出现的次数多少对该关键词的一个调整,其计算方式为：\[\text{IDF}(q_i) = \log \frac{N - n(q_i) + 0.5}{n(q_i) + 0.5}\] 其中N代表文档总数，\(n(q_i)\)是包含该关键词的文档个数 查阅Lucene可知，norm的计算为lengthNorm = 1.0 / Math.sqrt(numTerms) 具体实现为： 123456789// 文档termDocs中关键词qi的频率float f_qi_D = this.termDocs.freq();// doc对应的文档长度计算， norms[i] = 1/sqrt(length[i])float norm = Similarity.decodeNorm(this.norms[doc]);// 文档termDocs的长度float length = 1 / (norm * norm);// BM25算法return idf * f_qi_D * (this.K1 + 1)/ (f_qi_D + this.K1 *(1 - this.b + this.b * length/this.avgLength)); 利用Lucene提供的评分核心类实现VSM模型，并与BM25进行对比 原始评分算法即为VSM。 BM25和VSM算法的搜索结果没有明显差异，score值是明显不同的（这里的score值是开启了多关键词查询之后的结果） BM25算法 1234567891011=========下面是普通merge的结果==========doc=15007 score=8.551983 picPath= pictures/sogou/精美壁纸/人物壁纸/港台美女/0.jpg tag= 精美壁纸 人物壁纸 港台美女doc=15008 score=8.551983 picPath= pictures/sogou/精美壁纸/人物壁纸/港台美女/10.jpg tag= 精美壁纸 人物壁纸 港台美女doc=15009 score=8.551983 picPath= pictures/sogou/精美壁纸/人物壁纸/港台美女/11.jpg tag= 精美壁纸 人物壁纸 港台美女doc=15010 score=8.551983 picPath= pictures/sogou/精美壁纸/人物壁纸/港台美女/12.jpg tag= 精美壁纸 人物壁纸 港台美女doc=15011 score=8.551983 picPath= pictures/sogou/精美壁纸/人物壁纸/港台美女/13.jpg tag= 精美壁纸 人物壁纸 港台美女doc=15012 score=8.551983 picPath= pictures/sogou/精美壁纸/人物壁纸/港台美女/14.jpg tag= 精美壁纸 人物壁纸 港台美女doc=15013 score=8.551983 picPath= pictures/sogou/精美壁纸/人物壁纸/港台美女/15.jpg tag= 精美壁纸 人物壁纸 港台美女doc=15014 score=8.551983 picPath= pictures/sogou/精美壁纸/人物壁纸/港台美女/16.jpg tag= 精美壁纸 人物壁纸 港台美女doc=15015 score=8.551983 picPath= pictures/sogou/精美壁纸/人物壁纸/港台美女/17.jpg tag= 精美壁纸 人物壁纸 港台美女doc=15016 score=8.551983 picPath= pictures/sogou/精美壁纸/人物壁纸/港台美女/18.jpg tag= 精美壁纸 人物壁纸 港台美女 VSM算法 1234567891011=========下面是普通merge的结果==========doc=15007 score=5.85986 picPath= pictures/sogou/精美壁纸/人物壁纸/港台美女/0.jpg tag= 精美壁纸 人物壁纸 港台美女doc=15008 score=5.85986 picPath= pictures/sogou/精美壁纸/人物壁纸/港台美女/10.jpg tag= 精美壁纸 人物壁纸 港台美女doc=15009 score=5.85986 picPath= pictures/sogou/精美壁纸/人物壁纸/港台美女/11.jpg tag= 精美壁纸 人物壁纸 港台美女doc=15010 score=5.85986 picPath= pictures/sogou/精美壁纸/人物壁纸/港台美女/12.jpg tag= 精美壁纸 人物壁纸 港台美女doc=15011 score=5.85986 picPath= pictures/sogou/精美壁纸/人物壁纸/港台美女/13.jpg tag= 精美壁纸 人物壁纸 港台美女doc=15012 score=5.85986 picPath= pictures/sogou/精美壁纸/人物壁纸/港台美女/14.jpg tag= 精美壁纸 人物壁纸 港台美女doc=15013 score=5.85986 picPath= pictures/sogou/精美壁纸/人物壁纸/港台美女/15.jpg tag= 精美壁纸 人物壁纸 港台美女doc=15014 score=5.85986 picPath= pictures/sogou/精美壁纸/人物壁纸/港台美女/16.jpg tag= 精美壁纸 人物壁纸 港台美女doc=15015 score=5.85986 picPath= pictures/sogou/精美壁纸/人物壁纸/港台美女/17.jpg tag= 精美壁纸 人物壁纸 港台美女doc=15016 score=5.85986 picPath= pictures/sogou/精美壁纸/人物壁纸/港台美女/18.jpg tag= 精美壁纸 人物壁纸 港台美女 扩展任务 基于语料库所提供的图片所在网页内容，对图片描述文本进行扩充，实现基于图片描述文本的图像检索 基于BM25模型实现对于查询词的分词检索（参考Lucene的MultiTermQuery对应的核心评分类实现，也可参考网上的开源代码，最终大作业时需要） 分词工具采用IKAnalyzer, 具体实现参考了IKAnalyzer中文分词器[http://blog.sina.com.cn/s/blog_7663527601012vdg.html]一文。 一开始的实现是直接将查询词使用IKAnalyzer进行分词，然后将每个关键词分别作为搜索词去查询，然后将查询结果merge起来，按照得分高低排序。如下所示： 1234567891011121314151617181920212223242526272829303132333435363738394041424344//基于Lucene实现Analyzer analyzer = new IKAnalyzer(true);//true智能切分StringReader reader = new StringReader(queryString);TokenStream ts = analyzer.tokenStream("", reader);CharTermAttribute term = ts.getAttribute(CharTermAttribute.class);List&lt;String&gt; key_word = new ArrayList&lt;String&gt;(); while(ts.incrementToken())&#123; // System.out.print(term.toString()+"|"); key_word.add(term.toString());&#125;System.out.println(key_word);TopDocs[] resultsArray = new TopDocs[key_word.size()];for(int j = 0; j &lt; key_word.size(); ++j) &#123; // 针对每个搜索词去查询 resultsArray[j] = search.searchQuery(key_word.get(j), "abstract", 100);&#125;// TopDocs results=search.searchQuery(queryString, "abstract", 100);TopDocs results = TopDocs.merge(null, 100, resultsArray);System.out.println("=========下面是普通merge的结果==========");if (results != null) &#123; ScoreDoc[] hits = showList(results.scoreDocs, page); if (hits != null) &#123; // tags = new String[hits.length]; // paths = new String[hits.length]; for (int i = 0; i &lt; hits.length &amp;&amp; i &lt; PAGE_RESULT; i++) &#123; Document doc = search.getDoc(hits[i].doc); System.out.println("doc=" + hits[i].doc + " score=" + hits[i].score + " picPath= " + doc.get("picPath")+ " tag= "+doc.get("abstract")); // tags[i] = doc.get("abstract"); // paths[i] = picDir + doc.get("picPath"); &#125; &#125; else &#123; System.out.println("page null"); &#125;&#125;else&#123; System.out.println("result null");&#125; 但是在实际操作中发现，这样的查询操作是由问题的，表现极差，无法达到搜索者预期的多次查询需求，于是通过进一步的搜索和查询相关资料，最后决定采用 MultiFieldQueryParser的方式进行多关键词查询。实现参考博客- 关键词高亮（lucene笔记) http://www.jianshu.com/p/055ddb99819d 该函数可以实现多个关键词在不同的field中进行查询的功能，同时可以设定多个关键词之间的关系：and, not, or, 这里我们采用或的逻辑关系，同时设置field为‘abstract’，具体实现如下： 12345678910111213141516171819202122232425262728293031323334353637//待查找字符串对应的字段String [] to_query = &#123;queryString&#125;;String [] fields = &#123;"abstract"&#125;;//Occur.MUST表示对应字段必须有查询值， Occur.MUST_NOT 表示对应字段必须没有查询值， Occur.SHOULD(结果“或”)Occur[] occ=&#123;Occur.SHOULD&#125;;Query query = null;try &#123; query = MultiFieldQueryParser.parse(Version.LUCENE_35, to_query,fields,occ,analyzer); System.out.println(query);&#125; catch (ParseException e) &#123; // TODO Auto-generated catch block e.printStackTrace();&#125;results = search.searchQuery(query, "abstract", 100);System.out.println("============\n=========下面是MultiFieldQueryParser的结果==========");if (results != null) &#123; ScoreDoc[] hits = showList(results.scoreDocs, page); if (hits != null) &#123; tags = new String[hits.length]; paths = new String[hits.length]; for (int i = 0; i &lt; hits.length &amp;&amp; i &lt; PAGE_RESULT; i++) &#123; Document doc = search.getDoc(hits[i].doc); System.out.println("doc=" + hits[i].doc + " score=" + hits[i].score + " picPath= " + doc.get("picPath")+ " tag= "+doc.get("abstract")); tags[i] = doc.get("abstract"); paths[i] = picDir + doc.get("picPath"); &#125; &#125; else &#123; System.out.println("page null"); &#125;&#125;else&#123; System.out.println("result null");&#125; 实现结果对比： 搜索关键词为 港台女星 12345678910111213141516171819202122232425262728[港台, 女星]org.apache.lucene.search.TopDocs@23506dfborg.apache.lucene.search.TopDocs@75648bd9=========下面是普通merge的结果==========doc=15007 score=8.551983 picPath= pictures/sogou/精美壁纸/人物壁纸/港台美女/0.jpg tag= 精美壁纸 人物壁纸 港台美女doc=15008 score=8.551983 picPath= pictures/sogou/精美壁纸/人物壁纸/港台美女/10.jpg tag= 精美壁纸 人物壁纸 港台美女doc=15009 score=8.551983 picPath= pictures/sogou/精美壁纸/人物壁纸/港台美女/11.jpg tag= 精美壁纸 人物壁纸 港台美女doc=15010 score=8.551983 picPath= pictures/sogou/精美壁纸/人物壁纸/港台美女/12.jpg tag= 精美壁纸 人物壁纸 港台美女doc=15011 score=8.551983 picPath= pictures/sogou/精美壁纸/人物壁纸/港台美女/13.jpg tag= 精美壁纸 人物壁纸 港台美女doc=15012 score=8.551983 picPath= pictures/sogou/精美壁纸/人物壁纸/港台美女/14.jpg tag= 精美壁纸 人物壁纸 港台美女doc=15013 score=8.551983 picPath= pictures/sogou/精美壁纸/人物壁纸/港台美女/15.jpg tag= 精美壁纸 人物壁纸 港台美女doc=15014 score=8.551983 picPath= pictures/sogou/精美壁纸/人物壁纸/港台美女/16.jpg tag= 精美壁纸 人物壁纸 港台美女doc=15015 score=8.551983 picPath= pictures/sogou/精美壁纸/人物壁纸/港台美女/17.jpg tag= 精美壁纸 人物壁纸 港台美女doc=15016 score=8.551983 picPath= pictures/sogou/精美壁纸/人物壁纸/港台美女/18.jpg tag= 精美壁纸 人物壁纸 港台美女(abstract:港台 abstract:女星)org.apache.lucene.search.TopDocs@292b2c95=====================下面是MultiFieldQueryParser的结果==========doc=42133 score=20.643898 picPath= pictures/sogou/性感女星/港台女星/阿雅/0.jpg tag= 性感女星 港台女星 阿雅doc=42134 score=20.643898 picPath= pictures/sogou/性感女星/港台女星/阿雅/10.jpg tag= 性感女星 港台女星 阿雅doc=42135 score=20.643898 picPath= pictures/sogou/性感女星/港台女星/阿雅/11.jpg tag= 性感女星 港台女星 阿雅doc=42136 score=20.643898 picPath= pictures/sogou/性感女星/港台女星/阿雅/12.jpg tag= 性感女星 港台女星 阿雅doc=42137 score=20.643898 picPath= pictures/sogou/性感女星/港台女星/阿雅/13.jpg tag= 性感女星 港台女星 阿雅doc=42138 score=20.643898 picPath= pictures/sogou/性感女星/港台女星/阿雅/14.jpg tag= 性感女星 港台女星 阿雅doc=42139 score=20.643898 picPath= pictures/sogou/性感女星/港台女星/阿雅/15.jpg tag= 性感女星 港台女星 阿雅doc=42140 score=20.643898 picPath= pictures/sogou/性感女星/港台女星/阿雅/16.jpg tag= 性感女星 港台女星 阿雅doc=42141 score=20.643898 picPath= pictures/sogou/性感女星/港台女星/阿雅/17.jpg tag= 性感女星 港台女星 阿雅doc=42142 score=20.643898 picPath= pictures/sogou/性感女星/港台女星/阿雅/18.jpg tag= 性感女星 港台女星 阿雅 MultiFieldQueryParser查询方式的搜索结果如下： 多关键词查询的靠前结果同时满足两个关键词 港台 女星 第一页 从第6页开始出现了港台美女等相关搜索结果，能够实现关键词或的查询 1234567891011=========下面是MultiFieldQueryParser的结果==========doc=42163 score=17.694769 picPath= pictures/sogou/性感女星/港台女星/安雅/2.jpg tag= 性感女星 港台女星 安雅doc=42164 score=17.694769 picPath= pictures/sogou/性感女星/港台女星/安雅/3.jpg tag= 性感女星 港台女星 安雅doc=42165 score=17.694769 picPath= pictures/sogou/性感女星/港台女星/安雅/4.jpg tag= 性感女星 港台女星 安雅doc=42166 score=17.694769 picPath= pictures/sogou/性感女星/港台女星/安雅/5.jpg tag= 性感女星 港台女星 安雅doc=42167 score=17.694769 picPath= pictures/sogou/性感女星/港台女星/安雅/6.jpg tag= 性感女星 港台女星 安雅doc=42168 score=17.694769 picPath= pictures/sogou/性感女星/港台女星/安雅/7.jpg tag= 性感女星 港台女星 安雅doc=42169 score=17.694769 picPath= pictures/sogou/性感女星/港台女星/安雅/8.jpg tag= 性感女星 港台女星 安雅doc=42170 score=17.694769 picPath= pictures/sogou/性感女星/港台女星/安雅/9.jpg tag= 性感女星 港台女星 安雅doc=15007 score=15.022858 picPath= pictures/sogou/精美壁纸/人物壁纸/港台美女/0.jpg tag= 精美壁纸 人物壁纸 港台美女doc=15008 score=15.022858 picPath= pictures/sogou/精美壁纸/人物壁纸/港台美女/10.jpg tag= 精美壁纸 人物壁纸 港台美女 港台 女星 第6页 由此可以验证该多关键词查询实现是合理有效的。 注意 注意：在进行多关键词查询时需要修改ImageIndexer中的analyzer，将其设置为智能分词，不然会出现一些奇怪的查询结果： 搜索港台美女时第一页出现的是港台男星 将ImageIndexer的analyzer设置为智能分词即 analyzer = new IKAnalyzer(true);之后的搜索结果 修改后港台美女搜索结果第一页正常 第三页开始出现相关搜索结果 港台男星]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[安卓开发记录]]></title>
      <url>%2Farticle%2F%E5%AE%89%E5%8D%93%E5%BC%80%E5%8F%91%E8%AE%B0%E5%BD%95.html</url>
      <content type="text"><![CDATA[安卓中用户权限设置 位置：AndroidManifest.xml 在package之后， Application 之前 1234567&lt;!--用户权限--&gt;&lt;uses-permission android:name="android.permission.INTERNET" /&gt;&lt;uses-permission android:name="android.permission.WRITE_EXTERNAL_STORAGE" /&gt;&lt;uses-permission android:name="android.permission.ACCESS_WIFI_STATE" /&gt;&lt;uses-permission android:name="android.permission.ACCESS_NETWORK_STATE" /&gt;&lt;!--VPN 权限--&gt;&lt;uses-permission android:name="android.permission.BIND_VPN_SERVICE" /&gt; Android中Intent机制 作用 启动 Activity： Activity 表示应用中的一个屏幕。通过将 Intent 传递给 startActivity()，您可以启动新的 Activity 实例。Intent 描述了要启动的 Activity，并携带了任何必要的数据。 如果您希望在 Activity 完成后收到结果，请调用 startActivityForResult()。在 Activity 的 onActivityResult() 回调中，您的 Activity 将结果作为单独的 Intent 对象接收。如需了解详细信息，请参阅 Activity 指南。 启动服务： Service 是一个不使用用户界面而在后台执行操作的组件。通过将 Intent 传递给 startService()，您可以启动服务执行一次性操作（例如，下载文件）。Intent 描述了要启动的服务，并携带了任何必要的数据。 如果服务旨在使用客户端-服务器接口，则通过将 Intent 传递给 bindService()，您可以从其他组件绑定到此服务。如需了解详细信息，请参阅服务指南。 传递广播： 广播是任何应用均可接收的消息。系统将针对系统事件（例如：系统启动或设备开始充电时）传递各种广播。通过将 Intent 传递给 sendBroadcast()、sendOrderedBroadcast() 或 sendStickyBroadcast()，您可以将广播传递给其他应用。 Intent 结构 action – 想要实施的动作，例: ACTION_VIEW, ACTION_EDIT, ACTION_MAIN, etc. data – 具体的数据，一般由以Uri表示，例：通讯录中的某条记录，会以Uri来表示 category – 为实施的动作添加的额外信息，即Intent组件的种类信息，一个Intent对象可以有任意个category，例：CATEGORY_LAUNCHER 意味着，它应该在启动器中作为顶级应用而存在 type – 显示指定Intent的数据类型（MIME类型 - 多用途互联网邮件扩展，Multipurpose Internet Mail Extensions），例：一个组件是可以显示图片数据的而不能播放声音文件。很多情况下，data类型可在URI中找到，比如content:开头的URI，表明数据由设备上的content provider提供。但是通过设置这个属性，可以强制采用显式指定的类型而不再进行推导 MIME类型有两种：单个记录格式、多个记录格式 component – 指定Intent的目标组件的类名称。通常 Android会根据Intent 中包含的其它属性的信息，比如action、data/type、category进行查找，最终找到一个与之匹配的目标组件。但是，如果 component这个属性有指定的话，将直接使用它指定的组件，而不再执行上述查找过程。指定了这个属性以后，Intent的其它所有属性都是可选的，例如：Intent it = new Intent(Activity.Main.this, Activity2.class); startActivity(it); extras – 附加信息，例如：it.putExtras(bundle) - 使用Bundle来传递数据； Service机制 启动服务时依次执行onCreate，onStartCommand，onStart；如果在系统显示调用stopService和stopSelf之前终止服务，service再次重启，onStartCommand会被调用，重启服务时依次执行onStartCommand，onStart。无论何时，都会先调用onStartCommand()，在调用onStart()。 onStartCommand返回值 onStartComand使用时，返回的是一个(int)整形。 这个整形可以有四个返回值： 1start_sticky、start_no_sticky、START_REDELIVER_INTENT、START_STICKY_COMPATIBILITY 它们的含义分别是： START_STICKY：如果service进程被kill掉，保留service的状态为开始状态，但不保留递送的intent对象。随后系统会尝试重新创建service，由于服务状态为开始状态，所以创建服务后一定会调用onStartCommand(Intent,int,int)方法。如果在此期间没有任何启动命令被传递到service，那么参数Intent将为null。 START_NOT_STICKY：“非粘性的”。使用这个返回值时，如果在执行完onStartCommand后，服务被异常kill掉，系统不会自动重启该服务 START_REDELIVER_INTENT：重传Intent。使用这个返回值时，如果在执行完onStartCommand后，服务被异常kill掉，系统会自动重启该服务，并将Intent的值传入。 START_STICKY_COMPATIBILITY：START_STICKY的兼容版本，但不保证服务被kill后一定能重启。 onStartComand参数flags含义 flags表示启动服务的方式： Additional data about this start request. Currently either 0, START_FLAG_REDELIVERY, or START_FLAG_RETRY. START_FLAG_REDELIVERY：如果你实现onStartCommand()来安排异步工作或者在另一个线程中工作, 那么你可能需要使用START_FLAG_REDELIVERY来让系统重新发送一个intent。这样如果你的服务在处理它的时候被Kill掉, Intent不会丢失. START_FLAG_RETRY：表示服务之前被设为START_STICKY，则会被传入这个标记。 显示 隐藏（GONE） XML文件：android:visibility=“gone” Java代码：view.setVisibility(View.GONE); 错误 A/libc: Fatal signal 5 (SIGTRAP), code 1 in tid 24188 JNI 中 c 函数未返回]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[操作系统复习笔记]]></title>
      <url>%2Farticle%2F%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%2F%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0.html</url>
      <content type="text"><![CDATA[关于x86启动后执行的第一条指令及其地址 piazza 1、实模式下，地址为base+eip。第一条指令的地址是FFFFFFF0H = base(FFFF0000H) + EIP(0000FFF0H)。 2、争端出自于CS。可看看上面我高亮的normally和however部分。简单来讲就是，CS分为两部分，一部分为可见selector，另一部分为隐含base，实模式的一般情况下base = selector&lt;&lt;4，但是，机器刚启动的时候，selector=F000, base=FFFF0000！直到跳转指令以后才变成normally~ 例题：THU OS 2015 mid term 2 (启动)对于x86机器,加电后处于实模式下,并且cs寄存器的段选择子部分为0xf000, 隐藏的基址部分为0xffff0000,eip寄存器为0xfff0,则此时待执行的指令的物理地址为( 0xfffffff0) (16位实模式)执行完第一条 jmp指令后,cs寄存器的段选择子部分变为0xf000,eip变为 0xe05b,则此时待执行指令的物理地址为() 3） (进入32位保护模式)BIOS会将ucore bootloader 加载到网络地址0x7c00,且设置cs为0x0, eip为0x7c00。在boodoader中,使用 gdt加载了如下全局描述符表,然后进入32位保护模 式。此时为了正常工作。应该用 jmp将cs设置为(=3.1=),将ds/es/fs/gs/ss设置为(=3.2=) 此时若ep为0x7c6d,则实际被执行指令的物理地址为(一33二) 进程挂起相关 另一种解决方案是交换。包括把内存中某个进程的一部分或全部移到磁盘中。当内存中没有处于就绪状态的进程时，操作系统就把被阻塞的进程换出到磁盘中的”挂起队列“（suspend queue），即暂时保存从内存中”驱逐“出来的被挂器的进程队列。操作系统在此之后取出挂起队列中的另一个进程，或者接受一个新进程的请求，将其纳入内存运行。 “交换”（swapping）是一个I/O操作，因而可能使问题更恶化。但是由于磁盘I/O一般是系统中最快的I/O（相对于磁带或者打印机I/O），所以交换通常会提高性能。 阻塞-&gt;阻塞/挂起：如果没有就绪进程，则至少一个阻塞进程被换出，为另一个没有阻塞的进程让出空间。如果操作系统确定当前正在运行的进程，或者就绪进程为了维护基本的性能要求而需要更多的内存空间，那么，即使有可用的就绪态进程也可能出现这种转换。 阻塞挂起-&gt;就绪挂起：如果等待的事件发生了，则处于阻塞/挂起状态的进程可转换到就绪/挂起态。注意，这要求操作系统必须能够得到挂起进程的状态信息。 就绪/挂起-&gt;就绪：如果内存中没有就绪态进程，操作系统需要调入一个进程继续执行。此外，当处于就绪/挂起状态的进程比处于就绪态的任何进程的优先级都要高时，也可以进行这种转换。这种情况的产生是由于操作系统设计者规定，调入高优先级的进程比减少交换量更重要。 就绪-&gt;就绪/挂起：通常，操作系统更倾向于挂起阻塞态进程而不是就绪态进程，因为就绪态进程可以立即执行，而阻塞态进程占用了内存空间但不能执行。但如果释放内存以得到足够空间的唯一方法是挂起一个就绪态进程，那么这种转换也是必需的。并且，如果操作系统确信高优先级的阻塞态进程很快就会就绪，那么它可能选择挂起一个低优先级的就绪态进程，而不是一个高优先级的阻塞态进程。 通俗的说，就是挂起不挂起，不光要考虑为进程让出空间，不光要考虑是否就绪，还要考虑进程的优先级。 新建-&gt;就绪挂起及新建-&gt;就绪：当创建一个新进程时，该进程或者加入就绪队列，或者加入就绪/挂起队列。不论哪种情况，操作系统都必须建立一些表管理进程，并为进程分配地址空间。操作系统可能更倾向于在初期执行这些辅助工作，这使得它可以维护大量的未阻塞的进程。通过这一策略，内存中经常会没有足够的足够的空间分配给新进程。因此使用了（新建-&gt;就绪/挂起）转换。另一方面，我们可以证明创建进程时适时（just-in-time）原理，即尽可能推迟创建进程以减少操作系统的开销，并在系统被阻塞态进程阻塞时允许操作系统执行进程创建任务。 阻塞/挂起-&gt;阻塞：这种转化在设计中比较少见，如果一个进程没有准备好执行，并且不在内存中，调入它又有什么意义？但是考虑到下面的情况：一个进程终止，释放了一些内存空间，阻塞/挂起队列中有一个进程优先级比就绪/挂起队列中任何进程的优先级都要高，并且操作系统有理由相信阻塞进程的事件很快就会发射管，这时，把阻塞进程而不是就绪进程调入内存是合理的。 运行-&gt;就绪/挂起：通常当分配给一个运行进程的时间期满时，它将转换到就绪态。但是，如果由于位于阻塞/挂起队列中具有较高优先级的进程变得不再被阻塞，操作系统抢占这个进程，也可以直接把这个运行进程转换到就绪/挂起队列中，并释放一些内存空间。 各种状态/退出：在典型情况下，一个进程在运行时终止，或者是因为它已经完成，或者是因为出现了一些错误条件。但是，在某些操作系统中，一个进程可以被创建它的进程终止，或者当父进程终止时终止。如果允许这样，则进程在任何状态时都可以转换到退出态。 COW机制 COW机制简单来说就是fork的时候，子进程和父进程共用相同的内存，只有在修改的时候，拷贝原有内存当做一份私有的，在私有的基础上修改。 123456789101112131. fork的时候，进行如下处理。 1. VM_SHARE = 0，且页表P=1，将父进程的页表中的PTE_W记为0，这样这份内存只读不写。 如果考虑换入换出，应该注意，该页不能被换出，不然页表会不一致，所以我采取的办法是将该页设置为不可换出。 2. VM_SHARE = 0，且页表P=0，那么我们将页换入，然后按1和3做，或者也可以直接拷贝，并写入硬盘（比较麻烦）。 3. VM_SHARE = 1，不用cow机制。 段机制在这里不是特别好处理，我认为vma的管理就可以一定程度上取代段机制。2. 访问的时候如果发现pagefault，查看error_code和对应vmm的权限，如果发现 页存在，写出错，vmm可以写，那么我们进行处理。 1. VM_SHARE = 1，也就是说这块内存都是共享的，那么我们不应该对这块内存使用cow。 2. VM_SHARE = 0，新建对应的页表。 页存在，写出错，vmm不可以写，那就是错误异常了。 页不存在，写出错，vmm可以写 那么我们应该先把这一页换进来，再进行上面处理。 Page_fault Page_fault page_fault 造成page_fault的情况 造成page fault有两种情况 线性地址转换无效； 线性地址转换有效，但权限不合法。 线性地址转换无效的原因 页表项的P位为0，或保留位reserved为1。 error code的P位 如果页表项的P位为0，则error code的P位为0。 error code的P位 - P位为0时，表示page-fault缺页（准确来说，是entry的P位为0）。 - P位为1时，则不是因为缺页，而是因为page-level protection violation，即我们应该检测后面的标志位。 ucore的do_pgfault中 P为0的处理，即case 0和case 2，作为缺页处理，没问题。 P为1时，这时应该检测后面的标志位。先看case 1，ucore直接failed，也就是说，非缺页的读操作产生的page fault，ucore不做处理（后面的U/S、RSVD或I/D的错误），可以理解，没问题。再看看case 3，ucore先检测虚地址管理vma的写位，如果vma有写的权限，而发生了page fault，那么可能是entry没有写权限，因此可以尝试在entry添加上写权限。问题在于，如果不是因为entry的写权限造成的page fault的呢？此时ucore将陷入死循环！ 简单总结 ucore的page fault处理P=1,W/R=1的情况时，只处理了因为entry缺少写权限的情况，而没有考虑entry有写权限而由其它权限造成的异常，该处理可能导致ucore陷入死循环。赞同高思达同学对此处的质疑。 缺页中断 ①页表项全为0——虚拟地址与物理地址未建立映射关系或已被撤销。 ②物理页面不在内存中——需要进行换页机制。 ③访问权限不够——输出错误信息，并退出。 孤儿进程与僵尸进程 我们知道在unix/linux中，正常情况下，子进程是通过父进程创建的，子进程再创建新的进程。子进程的结束和父进程的运行是一个异步过程,即父进程永远无法预测子进程到底什么时候结束。 当一个 进程完成它的工作终止之后，它的父进程需要调用wait()或者waitpid()系统调用取得子进程的终止状态。 孤儿进程：一个父进程退出，而它的一个或多个子进程还在运行，那么那些子进程将成为孤儿进程。孤儿进程将被init进程(进程号为1)所收养，并由init进程对它们完成状态收集工作。 僵尸进程：一个进程使用fork创建子进程，如果子进程退出，而父进程并没有调用wait或waitpid获取子进程的状态信息，那么子进程的进程描述符仍然保存在系统中。这种进程称之为僵死进程。 子进程通过exit()退出时，父进程既没有结束，也没有通过wait()等待子进程结束，则子进程成为“僵尸进程(zombie) unix提供了一种机制可以保证只要父进程想知道子进程结束时的状态信息， 就可以得到。这种机制就是: 在每个进程退出的时候,内核释放该进程所有的资源,包括打开的文件,占用的内存等。 但是仍然为其保留一定的信息(包括进程号the process ID,退出状态the termination status of the process,运行时间the amount of CPU time taken by the process等)。直到父进程通过wait / waitpid来取时才释放。 但这样就导致了问题，如果进程不调用wait / waitpid的话， 那么保留的那段信息就不会释放，其进程号就会一直被占用，但是系统所能使用的进程号是有限的，如果大量的产生僵死进程，将因为没有可用的进程号而导致系统不能产生新的进程. 此即为僵尸进程的危害，应当避免。 孤儿进程是没有父进程的进程，孤儿进程这个重任就落到了init进程身上，init进程就好像是一个民政局，专门负责处理孤儿进程的善后工作。每当出现一个孤儿进程的时候，内核就把孤 儿进程的父进程设置为init，而init进程会循环地wait()它的已经退出的子进程。这样，当一个孤儿进程凄凉地结束了其生命周期的时候，init进程就会代表党和政府出面处理它的一切善后工作。因此孤儿进程并不会有什么危害。 任何一个子进程(init除外)在exit()之后，并非马上就消失掉，而是留下一个称为僵尸进程(Zombie)的数据结构，等待父进程处理。这是每个 子进程在结束时都要经过的阶段。如果子进程在exit()之后，父进程没有来得及处理，这时用ps命令就能看到子进程的状态是“Z”。如果父进程能及时 处理，可能用ps命令就来不及看到子进程的僵尸状态，但这并不等于子进程不经过僵尸状态。 如果父进程在子进程结束之前退出，则子进程将由init接管。init将会以父进程的身份对僵尸状态的子进程进行处理。 设置僵死状态的目的是维护子进程的信息，以便父进程在以后某个时候获取。这些信息包括了子进程的进程ID、终止状态以及资源利用信息（CPU时间、内存使用量等等）。如果一个进程终止（使其所有子进程变成孤儿进程），而该进程有子进程处于僵死状态，那么它的所有僵死子进程的父进程ID将被重置为1（init进程）。继承这些子进程的init进程将清理它们（也就是init进程将wait它们，从而除去它们的僵死状态）。 Belady现象 Clock算法有Belady现象 123412512345 实例 X86寄存器 通用寄存器 段寄存器 CS(代码段) DS(数据段) 指令寄存器 和标志寄存器 EIP：指令寄存器， EIP的低16位即8086的ip， 储存的是下一条要执行的指令的内存地址，在分段地址转换中，表示指令的段内便宜地址 EFLAGS: 标志寄存器， 中断允许标志位（IF）, CLI（禁止中断）、STI两个命令控制， CF,PF，ZF等等寄存器 用户态或内核态下的中断处理有什么区别？在trapframe中有什么体现 用户态进入中断时，由于涉及到从用户态进入内核态，需要从用户栈切换到内核栈，因此需要多保存ss（堆栈段）和esp（栈顶）两个寄存器，先在栈中压入这两个值。再压入error code，cs，eip，flags。 而内核态进入中断时，不需要设计栈的切换，因此只需要压入error code，cs，eip，flags。 在trapframe数据结构中： 12345678910111213141516171819202122struct trapframe &#123; struct pushregs tf_regs; uint16_t tf_gs; uint16_t tf_padding0; uint16_t tf_fs; uint16_t tf_padding1; uint16_t tf_es; uint16_t tf_padding2; uint16_t tf_ds; uint16_t tf_padding3; uint32_t tf_trapno; /* below here defined by x86 hardware */ uint32_t tf_err; uintptr_t tf_eip; uint16_t tf_cs; uint16_t tf_padding4; uint32_t tf_eflags; /* below here only when crossing rings, such as from user to kernel */ uintptr_t tf_esp; uint16_t tf_ss; uint16_t tf_padding5;&#125; __attribute__((packed)); 注释的那一行“below here only hwn crossing rings”，就表示ss和esp只需要在切换特权级（即从用户态到内核态）时需要保存。 x86特权级 linux操作系统CPL、DPL、RPL说明 Linux操作系统中特权级有3种：CPL,DPL和RPL，每个都是有4个等级。 我对他们的关系理解是这样：一般来说，CPL代表当前代码段的权限，如果它想要去访问一个段或门，首先要看看对方的权限如何，也就是检查对方的DPL，如果满足当前的权限比要访问的权限高，则有可能允许去访问，有些情况我们还要检查 选择子的权限，即RPL,因为我们通过选择子:偏移量的方式去访问一个段，这算是一个访问请求动作，因此 称为请求访问权限RPL(Requst Privilege Level)。当请求权限也满足条件，那么访问就被允许了。 CPL(Current Privilege Level) CPL是当前执行的任务的特权等级，它存储在CS和SS的第0位和第1位上。(两位表示0~3四个等级) 通常情况下，CPL等于代码所在段的特权等级，当程序转移到不同的代码段时，处理器将改变CPL。 注意:在遇到一致代码段时，情况特殊，一致代码段的特点是：可以被等级相同或者更低特权级的代码访问，当处理器访问一个与当前代码段CPL特权级不同的一致代码段时，CPL不会改变。 DPL(Descriptor Privilege Level) 表示门或者段的特权级，存储在门（中断描述符IDT）或者段的描述符（GDT）的DPL字段中。正如上面说的那样，当当前代码段试图访问一个段或者门时，其DPL将会和当前特权级CPL以及段或门的选择子比较，根据段或者门的类型不同，DPL的含义不同： 1.数据段的DPL：规定了访问此段的最低权限。比如一个数据段的DPL是1，那么只有运行在CPL为0或1的程序才可能访问它。为什么说可能呢？因为还有一个比较的因素是RPL。访问数据段要满足有效特权级别（上述）高于数据段的DPL. 2.非一致代码段的DPL(不使用调用门的情况)：DPL规定访问此段的特权，只有CPL与之相等才有可能访问。 3.调用门的DPL，规定了程序或任务访问该门的最低权限。与数据段同。 4.一致代码段和通过调用门访问的非一致代码段，DPL规定访问此段的最高权限。 比如一个段的DPL为2，那么CPL为0或者1的程序都无法访问。 5. TSS的DPL，同数据段。 RPL（Rquest Privilege Level） RPL是通过选择子的低两位来表现出来的(这么说来，CS和SS也是存放选择子的，同时CPL存放在CS和SS的低两位上，那么对CS和SS来说，选择子的RPL=当前段的CPL)。处理器通过检查RPL和CPL来确认一个访问是否合法。即提出访问的段除了有足够的特权级CPL，如果RPL不够也是不行的(有些情况会忽略RPL检查)。 为什么要有RPL？ 操作系统往往通过设置RPL的方法来避免低特权级的应用程序访问高特权级的内层数据。 例子情景：调用者调用操作系统的某过程去访问一个段。 当操作系统(被调用过程)从应用程序(调用者)接受一个选择子时，会把选择子的RPL设置称调用者的权限等级，于是操作系统用这个选择子去访问相应的段时(这时CPL为操作系统的等级,因为正在运行操作系统的代码)，处理器会使用调用者的特权级去进行特权级检查，而不是正在实施访问动作的操作系统的特权级(CPL)，这样操作系统就不用以自己的身份去访问(就防止了应用去访问需要高权限的内层数据,除非应用程序本身的权限就足够高)。 那么RPL的作用就比较明显了：因为同一时刻只能有一个CPL，而当低权限的应用去调用拥有至高权限的操作系统的功能来访问一个目标段时，进入操作系统代码段时CPL变成了操作系统的CPL，如果没有RPL，那么权限检查的时候就会用CPL，而这个CPL权限比应用程序高，也就可能去访问需要高权限才能访问的数据，这就不安全了。所以引入RPL，让它去代表访问权限，因此在检查CPL的同时，也会检查RPL.一般来说如果RPL的数字比CPL大(权限比CPL的低)，那么RPL会起决定性作用。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Lec12练习]]></title>
      <url>%2Farticle%2F%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%2FLec12%E7%BB%83%E4%B9%A0.html</url>
      <content type="text"><![CDATA[1（不用回答）理解 孤儿进程和僵死进程的含义 http://www.cnblogs.com/xiehongfeng100/p/4619913.html http://www.cnblogs.com/Anker/p/3271773.html https://piazza.com/class/i5j09fnsl7k5x0?cid=753 孤儿进程：一个父进程退出、但其仍在运行的孤儿进程。孤儿进程将被init进程（进程号为1）所收养，并由init进程对它们完成状态收集工作。 僵尸进程：一个已经终止、但是其父进程尚未对其进行善后处理（获取终止子进程的有关信息，释放它仍占用的资源）的进程。 5 请仔细阅读https://chyyuu.gitbooks.io/os_course_exercises/content/all/05-2-spoc-discussion.html. 中的小组思考题 设计一个简化的进程管理子系统，可以管理并调度如下简化进程.给出了参考代码（https://github.com/chyyuu/ucore_lab/blob/master/related_info/lab5/process-cpuio-homework.py），请理解代码，并完成＂YOUR CODE“部分的内容． 根据注释及gitbook中的提示完成实验即可，需要注意的是切换进程的时刻有:进程结束或进程发出yield请求 以及IO的时间设置问题，由输出的提示可知： System will switch when the current process is FINISHED or ISSUES AN YIELD or IO After IOs, the process issuing the IO will run LATER (when it is its turn) 因此，io完成时间应该是 clock_tick + self.io_length + 2,即需要两个切换时间 12# 设置IO完成时间self.io_finish_times[self.curr_proc].append(clock_tick + self.io_length + 2) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336#! /usr/bin/env python# coding:utf8import sysfrom optparse import OptionParserimport random# process switch behaviorSCHED_SWITCH_ON_IO = 'SWITCH_ON_IO'# io finished behaviorIO_RUN_LATER = 'IO_RUN_LATER'# process statesSTATE_RUNNING = 'RUNNING'STATE_READY = 'READY'STATE_DONE = 'DONE'STATE_WAIT = 'WAITING'# members of process structurePROC_CODE = 'code_'PROC_PC = 'pc_'PROC_ID = 'pid_'PROC_STATE = 'proc_state_'# things a process can doDO_COMPUTE = 'cpu'DO_YIELD = 'yld'DO_IO = 'io'class scheduler: def __init__(self, process_switch_behavior, io_done_behavior, io_length): # keep set of instructions for each of the processes self.proc_info = &#123;&#125; self.process_switch_behavior = process_switch_behavior self.io_done_behavior = io_done_behavior self.io_length = io_length return def new_process(self): proc_id = len(self.proc_info) self.proc_info[proc_id] = &#123;&#125; self.proc_info[proc_id][PROC_PC] = 0 self.proc_info[proc_id][PROC_ID] = proc_id self.proc_info[proc_id][PROC_CODE] = [] self.proc_info[proc_id][PROC_STATE] = STATE_READY return proc_id def load(self, program_description): proc_id = self.new_process() tmp = program_description.split(':') if len(tmp) != 3: print 'Bad description (%s): Must be number &lt;x:y:z&gt;' print ' where X is the number of instructions' print ' and Y is the percent change that an instruction is YIELD' print ' and Z is the percent change that an instruction is IO' exit(1) num_instructions, chance_yield, chance_io = int(tmp[0]), float( tmp[1]) / 100.0, float(tmp[2]) / 100.0 assert (chance_yield + chance_io &lt; 1) # print "proc %d, num_instr %d, change_cpu %f" % (proc_id,num_instructions, chance_cpu) for i in range(num_instructions): randnum = random.random(); if randnum &lt; (1.0 - chance_yield - chance_io): self.proc_info[proc_id][PROC_CODE].append(DO_COMPUTE) elif randnum &gt;= (1.0 - chance_yield - chance_io) and randnum &lt; ( 1.0 - chance_io): self.proc_info[proc_id][PROC_CODE].append(DO_YIELD) else: self.proc_info[proc_id][PROC_CODE].append(DO_IO) # print "proc %d, instr idx %d, instr cxt %s" % (proc_id, i, self.proc_info[proc_id][PROC_CODE][i]) return # change to WAIT STATE, the current proc's state should be expected def move_to_wait(self, expected): assert (self.proc_info[self.curr_proc][PROC_STATE] == expected) self.proc_info[self.curr_proc][PROC_STATE] = STATE_WAIT return # change to READY STATE, the current proc's state should be expected # if pid==-1, then pid=self.curr_proc def move_to_ready(self, expected, pid=-1): # YOUR CODE if pid is -1: pid = self.curr_proc assert (self.proc_info[pid][PROC_STATE] == expected) self.proc_info[pid][PROC_STATE] = STATE_READY return # change to RUNNING STATE, the current proc's state should be expected def move_to_running(self, expected): # YOUR CODE assert (self.proc_info[self.curr_proc][PROC_STATE] == expected) self.proc_info[self.curr_proc][PROC_STATE] = STATE_RUNNING return # change to DONE STATE, the current proc's state should be expected def move_to_done(self, expected): # YOUR CODE assert (self.proc_info[self.curr_proc][PROC_STATE] == expected) self.proc_info[self.curr_proc][PROC_STATE] = STATE_DONE return # choose next proc using FIFO/FCFS scheduling, If pid==-1, then pid=self.curr_proc def next_proc(self, pid=-1): # YOUR CODE """ 使用FIFO/FCFS：先来先服务, 只有进程done, yield, io时才会执行切换 先查找位于proc_info队列的curr_proc元素(当前进程)之后的进程(curr_proc+1..end)是否处于READY态， 再查找位于proc_info队列的curr_proc元素(当前进程)之前的进程(begin..curr_proc-1)是否处于READY态 如都没有，继续执行curr_proc直到结束 """ if pid == -1: pid = self.curr_proc # 使用FIFO/FCFS(先到先服务)调度算法选择下一个进程 for i in xrange(self.curr_proc + 1, len(self.proc_info)): if self.proc_info[i][PROC_STATE] == STATE_READY: self.curr_proc = i self.move_to_running(STATE_READY) return # break for i in xrange(0, self.curr_proc): if self.proc_info[i][PROC_STATE] == STATE_READY: self.curr_proc = i self.move_to_running(STATE_READY) return # break # 进程切换 if self.proc_info[self.curr_proc][PROC_STATE] == STATE_READY: self.move_to_running(STATE_READY) return def get_num_processes(self): return len(self.proc_info) def get_num_instructions(self, pid): return len(self.proc_info[pid][PROC_CODE]) def get_instruction(self, pid, index): return self.proc_info[pid][PROC_CODE][index] def get_num_active(self): num_active = 0 for pid in range(len(self.proc_info)): if self.proc_info[pid][PROC_STATE] != STATE_DONE: num_active += 1 return num_active def get_num_runnable(self): num_active = 0 for pid in range(len(self.proc_info)): if self.proc_info[pid][PROC_STATE] == STATE_READY or \ self.proc_info[pid][PROC_STATE] == STATE_RUNNING: num_active += 1 return num_active def get_ios_in_flight(self, current_time): num_in_flight = 0 for pid in range(len(self.proc_info)): for t in self.io_finish_times[pid]: if t &gt; current_time: num_in_flight += 1 return num_in_flight def space(self, num_columns): for i in range(num_columns): print '%10s' % ' ', def check_if_done(self): if len(self.proc_info[self.curr_proc][PROC_CODE]) == 0: if self.proc_info[self.curr_proc][PROC_STATE] == STATE_RUNNING: self.move_to_done(STATE_RUNNING) self.next_proc() return def run(self): clock_tick = 0 if len(self.proc_info) == 0: return # track outstanding IOs, per process self.io_finish_times = &#123;&#125; for pid in range(len(self.proc_info)): self.io_finish_times[pid] = [] # make first one active self.curr_proc = 0 self.move_to_running(STATE_READY) # OUTPUT: heade`[rs for each column print '%s' % 'Time', for pid in range(len(self.proc_info)): print '%10s' % ('PID:%2d' % (pid)), print '%10s' % 'CPU', print '%10s' % 'IOs', print '' # init statistics io_busy = 0 cpu_busy = 0 while self.get_num_active() &gt; 0: clock_tick += 1 # check for io finish io_done = False for pid in range(len(self.proc_info)): if clock_tick in self.io_finish_times[pid]: # if IO finished, the should do something for related process # YOUR CODE # IO完成，将pid进程转换为Ready状态 self.move_to_ready(STATE_WAIT, pid) if self.proc_info[self.curr_proc][ PROC_STATE] != STATE_RUNNING: self.next_proc(); io_done = True # pass #YOU should delete this # if current proc is RUNNING and has an instruction, execute it instruction_to_execute = '' if self.proc_info[self.curr_proc][PROC_STATE] == STATE_RUNNING and \ len(self.proc_info[self.curr_proc][PROC_CODE]) &gt; 0: # pop a instruction from proc_info[self.curr_proc][PROC_CODE]to instruction_to_execute # YOUR CODE instruction_to_execute = self.proc_info[self.curr_proc][ PROC_CODE].pop(0) # pass #YOU should delete this # OUTPUT: print what everyone is up to if io_done: print '%3d*' % clock_tick, else: print '%3d ' % clock_tick, for pid in range(len(self.proc_info)): if pid == self.curr_proc and instruction_to_execute != '': print '%10s' % ('RUN:' + instruction_to_execute), else: print '%10s' % (self.proc_info[pid][PROC_STATE]), if instruction_to_execute == '': print '%10s' % ' ', else: print '%10s' % 1, num_outstanding = self.get_ios_in_flight(clock_tick) if num_outstanding &gt; 0: print '%10s' % str(num_outstanding), io_busy += 1 else: print '%10s' % ' ', print '' # if this is an YIELD instruction, switch to ready state # and add an io completion in the future if instruction_to_execute == DO_YIELD: # YOUR CODE # 发出YIELD请求,放弃使用CPU, 进程切换 self.move_to_ready(STATE_RUNNING) self.next_proc() pass # YOU should delete this # if this is an IO instruction, switch to waiting state # and add an io completion in the future elif instruction_to_execute == DO_IO: # YOUR CODE # 发出I/O操作请求,放弃使用CPU self.move_to_wait(STATE_RUNNING) # 设置IO完成时间 self.io_finish_times[self.curr_proc].append( clock_tick + self.io_length + 2) # 切换进程 self.next_proc() pass # YOU should delete this else: cpu_busy += 1 # ENDCASE: check if currently running thing is out of instructions self.check_if_done() return (cpu_busy, io_busy, clock_tick)## PARSE ARGUMENTS#parser = OptionParser()parser.add_option('-s', '--seed', default=0, help='the random seed', action='store', type='int', dest='seed')parser.add_option('-l', '--processlist', default='', help='a comma-separated list of processes to run, in the form X1:Y1:Z1,X2:Y2:Z2,... where X is the number of instructions that process should run, and Y/Z the chances (from 0 to 100) issue an YIELD/IO', action='store', type='string', dest='process_list')parser.add_option('-L', '--iolength', default=3, help='how long an IO takes', action='store', type='int', dest='io_length')parser.add_option('-p', '--printstats', help='print statistics at end; only useful with -c flag (otherwise stats are not printed)', action='store_true', default=False, dest='print_stats')(options, args) = parser.parse_args()random.seed(options.seed)process_switch_behavior = SCHED_SWITCH_ON_IOio_done_behavior = IO_RUN_LATERio_length = options.io_lengths = scheduler(process_switch_behavior, io_done_behavior, io_length)# example process description (10:100,10:100)for p in options.process_list.split(','): s.load(p)print 'Produce a trace of what would happen when you run these processes:'for pid in range(s.get_num_processes()): print 'Process %d' % pid for inst in range(s.get_num_instructions(pid)): print ' %s' % s.get_instruction(pid, inst) print ''print 'Important behaviors:'print ' System will switch when',if process_switch_behavior == SCHED_SWITCH_ON_IO: print 'the current process is FINISHED or ISSUES AN YIELD or IO'else: print 'error in sched switch on iobehavior' exit(-1)print ' After IOs, the process issuing the IO will',if io_done_behavior == IO_RUN_LATER: print 'run LATER (when it is its turn)'else: print 'error in IO done behavior' exit(-1)print ''(cpu_busy, io_busy, clock_tick) = s.run()print ''print 'Stats: Total Time %d' % clock_tickprint 'Stats: CPU Busy %d (%.2f%%)' % (cpu_busy, 100.0 * float(cpu_busy) / clock_tick)print 'Stats: IO Busy %d (%.2f%%)' % (io_busy, 100.0 * float(io_busy) / clock_tick)print '']]></content>
    </entry>

    
    <entry>
      <title><![CDATA[服务器配置相关问题]]></title>
      <url>%2Farticle%2F%E6%9C%8D%E5%8A%A1%E5%99%A8%2F%E6%9C%8D%E5%8A%A1%E5%99%A8%E9%85%8D%E7%BD%AE.html</url>
      <content type="text"><![CDATA[服务器配置相关记录 Apache的反向代理 首先是库之类的环境配置， 如果是编译的，./configure附加–enable-proxy参数，把代理模块编译进来。 如果是安装好的，就在配置文件http.conf里启用相应的模块 LoadModule proxy_module modules/mod_proxy.so LoadModule proxy_http_module modules/mod_proxy_http.so 有的服务器该配置文件为, /etc/apache2/apache2.conf 12LoadModule proxy_module /usr/lib/apache2/modules/mod_proxy.soLoadModule proxy_http_module /usr/lib/apache2/modules/mod_proxy_http.so 123456789101112131415&lt;VirtualHost *:80&gt; ServerAlias wordpress.zhangshenghao.win ServerName wordpress.zhangshenghao.win ProxyPassReverse / http://pi.zhangshenghao.win/wordpress/ ProxyPass / http://pi.zhangshenghao.win/wordpress/ &lt;/VirtualHost&gt;&lt;VirtualHost *:80&gt; ServerAlias blog.zhangshenghao.win ServerName blog.zhangshenghao.win ProxyPassReverse / http://pi.zhangshenghao.win/qingfeng14.github.io/ ProxyPass / http://pi.zhangshenghao.win/qingfeng14.github.io/ &lt;/VirtualHost&gt; 123456789101112131415&lt;VirtualHost *:80&gt; ServerAlias code.zhangshenghao.win ServerName code.zhangshenghao.win # ProxyPassReverse / http://pi.zhangshenghao.win/wordpress/ DocumentRoot /Users/alexzhangch/Downloads/FTP_SERVER/ &lt;/VirtualHost&gt;&lt;Directory &quot;/Users/alexzhangch/Downloads/FTP_SERVER/&quot;&gt; Options FollowSymLinks Multiviews MultiviewsMatch Any AllowOverride None Require all granted&lt;/Directory&gt; 服务器重启 1sudo /etc/init.d/apache2 restart 开启HTTPS 代理 开启HTTPS步骤 更全 步骤1：生成密钥 1openssl genrsa 1024 &gt; server.key 说明：这是用128位rsa算法生成密钥，得到server.key文件 步骤2: 生成证书请求文件 1openssl req -new -key server.key &gt; server.csr 说明：这是用步骤1的密钥生成证书请求文件server.csr, 这一步提很多问题，一一输入 1234567891011Country Name (2 letter code) [AU]:CN ← 国家代号，中国输入CN State or Province Name (full name) [Some-State]:BeiJing ← 省的全名，拼音 Locality Name (eg, city) []:BeiJing ← 市的全名，拼音 Organization Name (eg, company) [Internet Widgits Pty Ltd]:MyCompany Corp. ← 公司英文名 Organizational Unit Name (eg, section) []: ← 可以不输入 Common Name (eg, YOUR name) []: ← 此时不输入 Email Address []:admin@mycompany.com ← 电子邮箱，可随意填Please enter the following ‘extra’ attributes to be sent with your certificate request A challenge password []: ← 可以不输入 An optional company name []: ← 可以不输入 步骤3: 生成证书 1openssl req -x509 -days 3650 -key server.key -in server.csr &gt; server.crt 说明：这是用步骤1,2的的密钥和证书请求生成证书server.crt，-days参数指明证书有效期，单位为天 1sudo ln -s /etc/apache2/sites-available/default-ssl.conf /etc/apache2/sites-enabled/001-ssl.conf let’s encrypt 12Generating key (2048 bits): /etc/letsencrypt/keys/0001_key-certbot.pemCreating CSR: /etc/letsencrypt/csr/0001_csr-certbot.pem 12# 泛域名更新certbot certonly -d *.2simple.top --manual --preferred-challenges dns --server https://acme-v02.api.letsencrypt.org/directory 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071&lt;VirtualHost _default_:443&gt; ServerAdmin zhangshenghao1995@163.com DocumentRoot /var/www/html/qingfeng14.github.io/ ErrorLog $&#123;APACHE_LOG_DIR&#125;/error.log CustomLog $&#123;APACHE_LOG_DIR&#125;/access.log combined # SSL Engine Switch: # Enable/Disable SSL for this virtual host. SSLEngine on # A self-signed (snakeoil) certificate can be created by installing # the ssl-cert package. See # /usr/share/doc/apache2/README.Debian.gz for more info. # If both key and certificate are stored in the same file, only the # SSLCertificateFile directive is needed. SSLCertificateFile /etc/letsencrypt/live/zhangshenghao.win/fullchain.pem SSLCertificateKeyFile /etc/letsencrypt/live/zhangshenghao.win/privkey.pem &lt;FilesMatch &quot;\.(cgi|shtml|phtml|php)$&quot;&gt; SSLOptions +StdEnvVars &lt;/FilesMatch&gt; &lt;Directory /usr/lib/cgi-bin&gt; SSLOptions +StdEnvVars &lt;/Directory&gt; BrowserMatch &quot;MSIE [2-6]&quot; \ nokeepalive ssl-unclean-shutdown \ downgrade-1.0 force-response-1.0 # MSIE 7 and newer should be able to use keepalive BrowserMatch &quot;MSIE [17-9]&quot; ssl-unclean-shutdown ServerName zhangshenghao.win &lt;/VirtualHost&gt; &lt;VirtualHost _default_:443&gt; ServerAdmin zhangshenghao1995@163.com DocumentRoot /var/www/html/qingfeng14.github.io/ ErrorLog $&#123;APACHE_LOG_DIR&#125;/error.log CustomLog $&#123;APACHE_LOG_DIR&#125;/access.log combined # SSL Engine Switch: # Enable/Disable SSL for this virtual host. SSLEngine on # A self-signed (snakeoil) certificate can be created by installing # the ssl-cert package. See # /usr/share/doc/apache2/README.Debian.gz for more info. # If both key and certificate are stored in the same file, only the # SSLCertificateFile directive is needed. SSLCertificateFile /etc/letsencrypt/live/blog.zhangshenghao.win/fullchain.pem SSLCertificateKeyFile /etc/letsencrypt/live/blog.zhangshenghao.win/privkey.pem &lt;FilesMatch &quot;\.(cgi|shtml|phtml|php)$&quot;&gt; SSLOptions +StdEnvVars &lt;/FilesMatch&gt; &lt;Directory /usr/lib/cgi-bin&gt; SSLOptions +StdEnvVars &lt;/Directory&gt; BrowserMatch &quot;MSIE [2-6]&quot; \ nokeepalive ssl-unclean-shutdown \ downgrade-1.0 force-response-1.0 # MSIE 7 and newer should be able to use keepalive BrowserMatch &quot;MSIE [17-9]&quot; ssl-unclean-shutdown ServerName blog.zhangshenghao.win &lt;/VirtualHost&gt; http 重定向至 http 12RewriteEngine OnRewriteRule ^ https://%&#123;SERVER_NAME&#125;%&#123;REQUEST_URI&#125; [END,NE,R=permanent] squid 代理服务器]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[lec10_lec11练习题]]></title>
      <url>%2Farticle%2F%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%2Flec10-lec11.html</url>
      <content type="text"><![CDATA[1 以lab3为例，说明虚拟页与磁盘后备页面的对应关系是什么？ 物理内存+磁盘空间 构成了虚拟存储通过虚拟页式存储机制提供给用户更大的逻辑空间，虚拟内存页有可能在物理内存中，也可能在磁盘后备页面中，这种调整是通过页面置换算法进行对应的换入换出的。 4 由于何种原因，可出现进程的何种状态转到退出状态？ 正常退出：进程的应用程序逻辑流程执行结束，进程会由运行态转为退出状态 错误退出：程序运行过程中发现错误（异常输入等等）主动退出（运行状态到退出状态） 致命错误：程序运行出现差错无法继续执行而退出（运行到退出） 被其他进程所杀，其他进程向该进程发送信号使之退出（此前可能是就绪状态，运行状态，等待状态） 5 请设计一个简化的进程管理子系统，可以管理并调度如下简化进程.给出了参考代码（https://github.com/chyyuu/ucore_lab/blob/master/related_info/lab4/process-concept-homework.py），请理解代码，并完成＂YOUR CODE“部分的内容。 该调度算法模拟了先到先服务调度（FCFS）算法，根据注释填入代码即可 需要注意的是在进行FCFS算法时，需要注意在还有其他进程处于Ready状态时不要让当前进程继续运行,如下所示： 12345678910111213def next_proc(self, pid=-1): #YOUR CODE if pid==-1: pid = self.curr_proc # 使用FIFO/FCFS(先到先服务)调度算法选择下一个进程 for i in xrange(0, len(self.proc_info)): if self.proc_info[i][PROC_STATE] == STATE_READY and i != pid: # 避免选择本进程再次调度 self.curr_proc = i break if self.proc_info[self.curr_proc][PROC_STATE] == STATE_READY: self.move_to_running(STATE_READY) return]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[[计算机系统结构]层次存储]]></title>
      <url>%2Farticle%2F%E7%B3%BB%E7%BB%9F%E7%BB%93%E6%9E%84%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F%E7%BB%93%E6%9E%84-%E5%B1%82%E6%AC%A1%E5%AD%98%E5%82%A8.html</url>
      <content type="text"><![CDATA[程序访问的局部性原理：时间局部性，空间局部性 存储系统的多级层次结构 CPU-&gt;M1-&gt;M2–。。。。。。.&gt;Mn 在存储层次中，各存储器中一般满足包含关系,即任何一层存储器中的内容都是其下一层存储（离CPU更远一级）中内容的子集 CPU与M1一般以字为单位传递，M1以外一般以块/页为单位传递数据 存储系统性能参数 存储容量S 一般来说，整个存储系统的存储容量即为最后一级存储器的容量，eg:cache + 主存的存储系统容量为主存容量 存储系统的平均每位价格C 命中率H CPU 访问该存储系统时，在M1中找到所需信息的概率 平均访存时间\(T_A\) 命中时间 不命中开销：访一级时间 + 数据传送时间 三级存储系统：Cache + 主存 + 磁盘 cache-主存层次：为了弥补主存速度的不足，一般是硬件完成的，对应用程序员和系统程序员是透明的 主存-辅存层次：为了弥补主存容量的不足，常被用于实现虚拟存储器，向编程人员提供用不完的程序空间，主要由软件完成 存储层次的四个问题 映像规则 查找算法 替换算法 写策略 Cache cache 基本结构及原理 cache是按块管理的，cache和主存均被分成大小相同的块，信息以块为单位调入cache中 主存地址： 块地址（块号） | 块内偏移 主存地址寄存器–》主存-Cache地址转换部件 –》 命中，Cache块地址，根据偏移在Cache存储体中查到对应的数据指令送给CPU -》 未命中，访主存储器，调入Cache（是否需要替换），送给CPU，或者直接送给CPU提高效率 Cache 映像规则 全相联映射 主存中的人意块可以放到Cache中的任意一个位置 ### 直接映射 主存中每一块只能放到Cache中的唯一一个位置，对应关系依次循环分配 主存的第i块（块地址为i）映射到Cache的第j块 \[j = i mod M\] M为Cache的快数， 若\(M = 2^m\)，则j实际上是i的低m位（注意是块地址的低m位），可用这m位去进行选择（索引） 组相连映像 Cache被分为若干组，每个组由若干块组成，主存中每一个块可以放到Cache中唯一一个组的任意一个位置。组的选择通常采用位选择方法 对于主存中的第i个块，若它映射到Cache中的组号为j，则 \[j = i mdo G\] G为Cache的组数，当\(G=2^g\)时，k为块号i的低g位，这里的低g位称为索引 如果每组有n 个块，则称该映像规则为n路组相联 n值越大，Cache的空间利用率就越高，块冲突的概率就越低 查找方法 Cache中设置有一个目录表，每一个Cache块在该表中均有唯一的一项，用于指出当前该块存放的信息是哪个主存块的（一般有多个主存块映射到该Cache块，它实际上记录了该主存块的块地址的高位部分，称为标识），每个主存块能唯一地由其标识来确定 12主存地址： 标识 | 索引 | 块内偏移 ____块地址__ 经典的CPU性能公式 现在我们可以用指令数、CPI和时钟周期时间来写出基本的性能公式： CPU时间=指令数×CPI×时钟周期时间 CPI（clock cycles per instruction）：每条指令的时钟周期数，表示执行某个程序或者程序片段时每条指令所需的时钟周期平均数。 指令数（instruction count）：执行某程序所需的总指令数量。 或 CPU时间=指令数×CPI/时钟频率 永远记住，唯一能够被完全可靠测量的计算机性能指标是时间。例如，对指令集减少指令数目的改进可能降低时钟周期时间或提高CPI，从而抵消了改进的效果。类似地，CPI与执行的指令类型相关，执行指令数最少的代码其执行速度未必是最快的。 写分配法 张晨曦 按写分配法：写失效时，先把所写单元所在的块调入 Cache，然后再进行写入。这与读失效类似。这种方法也称为写时取方法。 不按写分配法：写失效时，直接写入下一级存储器而不将相应的块调入 Cache。这种方法也称为绕写法。 写策略 写Cache时何时更新主存中的内容 写直达法： 执行写操作时，不仅把数据写入Cache中对应的块，还把数据写入下一级存储器 写回法： 拷回法，只把数据写回cache，Cache块被替换时写回下一级存储器]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[操作系统-页面置换算法]]></title>
      <url>%2Farticle%2F%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%2F%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F-%E9%A1%B5%E9%9D%A2%E7%BD%AE%E6%8D%A2%E7%AE%97%E6%B3%95.html</url>
      <content type="text"><![CDATA[置换算法的功能和目标 功能 当出现缺页异常，需调入新页面而内存已满时，置换算法选择被置换的物理页面 设计目标 尽可能减少页面的调入调出次数 把未来不再访问或短期内不访问的页面调出 页面锁定(frame locking) 描述必须常驻内存的逻辑页面 操作系统的关键部分 要求响应速度的代码和数据 页表中的锁定标志位(lock bit) 置换算法的评价方法 记录进程访问内存的页面轨迹 评价：模拟页面置换算法，记录缺页的次数，更少的缺页意味着更好的性能 页面置换算法分类 局部页面置换算法：着眼于当前进程 最优算法，先进先出算法，最近最久未使用算法，时钟算法，最不常用算法 全局页面置换算法：着眼于所有可换出的物理页面 工作集算法，缺页率算法 局部页面置换算法 最优页面置换算法 OPT 置换未来最长时间不访问的页面 实际系统中无法实现 FIFO算法 思路： 选择在内存中滞留时间最长的页面进行置换 实现： 维护一个记录所有位于内存中的逻辑页面链表 链表元素按驻留内存时间排序，链首最长，链尾最短 出现缺页时，选择链首进行置换 特征： 实现简单 性能较差 Belady现象 时钟置换算法 思路：对页面的访问情况进行大致统计 实现： 在页表项增加访问位， 各页面组织成环形链表 指针指向最先调入的页面 算法： 访问页面时，在页表项记录页面访问情况 缺页时，从指针处开始顺序查找未被访问的页面进行置换 特征：是LRU和FIFO的折中 【Note】:访问位的标记是硬件自动完成的 具体流程 页面装入内存时，访问位初始化为0 访问页面（读/写)时，访问位置1 缺页时，从指针当前位置顺序检查环形链表 访问位为0，则置换该页 访问位为1，则访问位置0，并指针移动到下一个页面， 直到找到可置换的页面 全局页面置换算法 工作集页置换算法 工作集 \(W(t, \Delta)\) t是当前的执行时刻 \(\Delta\) 是工作集窗口，即一个定长的页面访问时间窗口 \(W(t,\Delta)\)表示在当前时刻t前的\(\Delta\)时间窗口中的所有访问页面所组成的集合 \(|W(t,\Delta)|\)指工作集的大小，即页面数目 工作集页置换算法思路：换出不在工作集中的页面 窗口大小\(\tau\):当前时刻前\(\tau\)个内存访问的页引用是工作集，\(\tau\)被称为窗口大小 实现方法： 访存链表：维护窗口内的访存页面链表 访存时，换出不在工作集的页面，更新访存链表 缺页时，换出页面，更新访存链表]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[数值分析]]></title>
      <url>%2Farticle%2F%E6%95%B0%E5%80%BC%E5%88%86%E6%9E%90.html</url>
      <content type="text"><![CDATA[设𝑨,𝑩,𝑪均为𝑛×𝑛矩阵，且𝑩、𝑪非奇异，𝒃是𝑛维向量，要计算\(𝒙 = B^{-1} (2A+I)(C^{-1}+A)b\), 请给出一个合理、高效率的算法流程. \[y1 = C^{-1}b\] \[y2 = y1 + Ab\] \[y_3 = 2(Ay_2) + y_2\] \[ x = B^{-1}y_3\] 如果A的每个对角元的绝对值都比所在行的非对角元的绝对值的和要大,即 \[|a_{ii}|&gt;sum{j!=i}|a_{ij}|\] 对所有的i成立,那么称A是（行）严格对角占优阵. 如果A’是行严格对角占优阵,那么称A是列严格对角占优阵. 习惯上如果不指明哪种类型的话就认为是行对角占优. 矩阵特征值的几何意义 矩阵乘法对应了一个变换，是把任意一个向量变成另一个方向或长度都大多不同的新向量。在这个变换的过程中，原向量主要发生旋转、伸缩的变化。如果矩阵对某一个向量或某些向量只发生伸缩变换，不对这些向量产生旋转的效果，那么这些向量就称为这个矩阵的特征向量，伸缩的比例就是特征值。 实际上，上述的一段话既讲了矩阵变换特征值及特征向量的几何意义（图形变换）也讲了其物理含义。物理的含义就是运动的图景：特征向量在一个矩阵的作用下作伸缩运动，伸缩的幅度由特征值确定。特征值大于1，所有属于此特征值的特征向量身形暴长；特征值大于0小于1，特征向量身形猛缩；特征值小于0，特征向量缩过了界，反方向到0点那边去了。 特征值及特征向量定义 定义 设A是n阶方阵，如果数λ和n维非零列向量x使关系式 Ax=λx (1) 成立，那么这样的数λ称为矩阵A特征值，非零向量x称为A的对应于特征值λ的特征向量．（1）式也可写成， ( A-λE)X=0 (2) 这是n个未知数n个方程的齐次线性方程组，它有非零解的充分必要条件是系数行列式 |A-λE|=0 , (3) 特征值 来自维基百科的解释 在数学上，特别是线性代数中，对于一个给定的线性变换\(A\)，它的特征向量（eigenvector，也译固有向量或本征向量）\(v\) 经过这个线性变换之后，得到的新向量仍然与原来的\(v\)保持在同一条直线上，但其长度或方向也许会改变。即 \[ Av=\lambda v\] \(\lambda\) 为标量，即特征向量的长度在该线性变换下缩放的比例，称 \(\lambda\) 为其特征值（本征值）。如果特征值为正，则表示\(v\)在经过线性变换的作用后方向也不变；如果特征值为负，说明方向会反转；如果特征值为0，则是表示缩回零点。但无论怎样，仍在同一条直线上。图1给出了一个以著名油画《蒙娜丽莎》为题材的例子。在一定条件下（如其矩阵形式为实对称矩阵的线性变换），一个变换可以由其特征值和特征向量完全表述，也就是说：所有的特征向量组成了这向量空间的一组基底。一个特征空间(eigenspace)是具有相同特征值的特征向量与一个同维数的零向量的集合，可以证明该集合是一个线性子空间，比如 \[ E_{\lambda}=\{u \in V\mid Au=\lambda u\}\] 即为线性变换 A中以λ为特征值的特征空间。 这些概念在纯数学和应用数学的众多领域中都有重要的应用。在线性代数和泛函分析之外，甚至在一些非线性的情况下，这些概念都是十分重要的。 \(\mathbf{A}\)的特征向量\(\mathbf {x}\) ，按照定义，是在变换 \(\mathbf{A}\)的作用下会得到 \(\mathbf {x}\) 自身的若干倍的非零向量。假设在 \(\mathbf{A}\)的作用下 \(\mathbf {x}\) 变成了自身的 \(\lambda\) 倍，也就是 \(\mathbf{A} \mathbf{x} = \lambda \mathbf{x}\) 在等式两边的左侧乘以单位矩阵I，得到 \[\mathbf{IA} \mathbf{x} =\mathbf{I} \cdot \lambda \mathbf{x} \] \(\mathbf{A}\mathbf{x} = (\lambda I)\mathbf{x}\) 因此 \((\mathbf{A}-\lambda \mathbf{I}) \mathbf{x}=0\) 根据线性方程组理论，为了使这个方程有非零解，矩阵 \(\mathbf{A}-\lambda \mathbf{I}\)的行列式必须是零： \[\det(\mathbf{A}-\lambda \mathbf{I}) = 0\] 按照行列式的展开定义，上面式子的左端是一个关于 \(\lambda\) 的多项式，称为特征多项式。这个多项式的系数只和 \(\mathbf{A}\)有关。在这个例子中，可以计算这个特征多项式： \[\det\!\left(\begin{bmatrix}1 &amp; 0\\ -\frac{1}{2} &amp; 1\end{bmatrix} - \lambda\begin{bmatrix}1 &amp; 0\\ 0 &amp; 1\end{bmatrix} \right)=(1-\lambda)^2\] 在这种情况下特征多项式的方程变成 \[(1-\lambda)^2 = 0\]它的唯一的解是： \(\lambda=1\)。这就是矩阵 \(\mathbf{A}\)的特征值。 找到特征值 \(\lambda=1\)后，就可以找出 \[(\mathbf{A}-\lambda \mathbf{I}) \mathbf{x}=0\] 的非零解，也就是特征向量了。在例子中： \[\begin{bmatrix}1-\lambda &amp; 0\\ -\frac{1}{2} &amp; 1-\lambda \end{bmatrix}\begin{bmatrix}x_1\\ x_2\end{bmatrix}=0\] 将\(\lambda=1\)代入，就有 \[\begin{bmatrix}0 &amp; 0\\ -\frac{1}{2} &amp; 0 \end{bmatrix}\begin{bmatrix}x_1\\ x_2\end{bmatrix}=0\] 解这个新矩阵方程，得到如下形式的解： \[\mathbf{x} = \begin{bmatrix}0\\ c\end{bmatrix}\] 这里的c是任意非零常量。因此，矩阵\(\mathbf{A}\)的特征向量就是所有竖直方向的向量（比如图中红色箭头代表的向量）。 正定矩阵的判定 判定定理1：对称阵A为正定的充分必要条件是：A的特征值全为正。 判定定理2：对称阵A为正定的充分必要条件是：A的各阶顺序主子式都为正。 判定定理3：任意阵A为正定的充分必要条件是：A合同于单位阵。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[机器学习-朴素贝叶斯]]></title>
      <url>%2Farticle%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0-%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF.html</url>
      <content type="text"><![CDATA[朴素贝叶斯原理 youryion数据挖掘指导 朴素贝叶斯法属于一种分类方法，基于特征条件独立假设学习输入输出的联合概率分布，以此为模型，对于给定的输入x，利用贝叶斯定理求出后验概率最大的输出y。 简单有效，是一种常用的机器学习方法. 设输入空间 \(X \subseteq R^n\)为n维向量的集合 输出空间为类标记集合 \(Y=c_1, c_2,\cdots,c_k\) X是定义在输入空间上的随机变量 Y是定义在输出空间上的随机变量 P(X,Y)是X和Y的联合概率分布 \(T = (x_1, y_1), (x_2, y_2), \cdots, (x_N, y_N)\)是由P(X,Y)独立同分布产生的训练集 贝叶斯法则 条件概率 条件概率表示为P（A|B），读作“在B条件下A的概率”。 若只有两个事件A，B，那么 \[P(A|B) = \frac{P(AB)}{P(B)}\] 贝叶斯法则 引入独立性假设 得到朴素贝叶斯分类器 拉普拉斯平滑]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[机器学习实验-朴素贝叶斯分类器]]></title>
      <url>%2Farticle%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%AE%9E%E9%AA%8C-%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8.html</url>
      <content type="text"><![CDATA[提示：个人联系方式及代码简要说明、运行方式在README.md文件中 实验目的 在真实数据集上实现朴素贝叶斯分类器，并验证其分类效果 了解如何在测试数据集上实现一个机器学习算法 了解如何评价分类效果 了解如果分析实验结果 算法实现原理 假设各条件相互间独立，即\[P(y|x_1,\cdots,x_n)P(y) \propto \prod_{i=1}^{n}P(x_i|y)\] 在训练时训练 \(P(y)\) 以及\(P(x_i|y)\) 测试时输出 \[\hat y = argmax_yP(y)\prod_{i=1}^{n}P(x_i|y)\] 数据集处理 数据集描述 实验给定了Adult数据集，其中adult.train为训练集（32561条数据），adult.test为测试集(16281条数据)，每行数据代表一个人，共有15个维度的特征，最后一个特征为该人的收入是否超过了50K。 数据集中部分特征是连续数据，部分数据可能未知(用?表示) 分类器的性能评价指标： 本次实验中，我采用了准确率作为朴素贝叶斯分类器性能的评价指标，计算方法为： \[Accuracy = \frac{number\ of\ correctly\ classified\ records}{number\ test\ records}\] 数据集的变量及其含义 变量名 意义 数据特征 处理方式 age 年龄 连续数据 分段离散 work_class 职业类型 离散数据 fnlwgt 最终重量(?) 连续数据 无意义、忽略 education 学历等级 离散数据 education_num 学历的数字等级 连续数据 重复、忽略 marital-status 婚姻状况 离散数据 occupation 职业 离散数据 relationship 家庭关系 离散数据 race 人种 离散数据 sex 性别 离散数据 capital_gain 资本利得 连续数据 离散化 capital_loss 资本损失 连续数据 离散化 hours_per_week 每周工作时长 连续数据 离散化 native-country 出生国 离散 income 收入 离散 特殊数据的处理方式 未知数据？的处理 数据集中有未知的数据(?), 我的处理方式是将这类数据直接忽略掉 数据合并 使用R语言对数据进行统计，发现，Never-worked和Without-pay可以合并为Without-pay字段。 数据的离散化 数据规律探索 考虑到R语言对数据处理的优越性，因此采用R语言对数据规律进行探索，利用R语言读入测试集和训练集 123456# 读取测试集，已清除？test = read.csv("after.test", sep=",", header=F, col.names=c("age", "work_class", "fnlwgt", "education", "education_num", "marital-status", "occupation", "relationship", "race", "sex", "capital_gain", "capital_loss", "hourr_per_week","native-country", "income"), fill = FALSE, strip.white = T) 然后对连续数据做统计之后得到如下结果 Age的规律 12345678&gt; table(train$age) 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 328 447 594 629 621 674 824 752 799 745 789 808 774 813 851 789 837 836 828 852 828 791 786 765 769 741 743 704 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 706 711 683 523 555 575 571 455 448 394 386 343 337 344 332 276 259 213 186 173 136 110 111 90 80 64 54 40 73 74 75 76 77 78 79 80 81 82 83 84 85 86 88 90 49 38 34 29 20 14 15 16 13 7 5 8 3 1 3 35 可知年龄范围为17~90，我们设置其分割粒度为5 资本收益 资本收益 可以从统计结果看出，投资收益相互间差别很大，直接以固定颗粒度分割并不适合，考虑到投资和收入之间存在一定的关系，而且可以很明显的看出收益为0的占了所有数据的大部分，我们将其分为三个类别，没有资本收益，资本收益较少，资本收益较大，。 除去数据中为0的值之后，求得其平均值，中位数，方差如下： 123456789&gt; # 资本收益平均值&gt; mean(train$capital_gain[train$capital_gain!=0])[1] 12977.6&gt; # 资本收益中位数&gt; median(train$capital_gain[train$capital_gain!=0])[1] 7298&gt; # 资本收益方差&gt; sd(train$capital_gain[train$capital_gain!=0])[1] 22311.91 可以看出其方差较大，以平均值作为界点不合适，我们取其中位数作为资本收益高低的界点(训练集与测试集这个数据差别不大) 同理，资本损失的数据也有这样的规律，我们也用这样的方法对其进行处理。 123456789&gt; # 资本损失平均值&gt; mean(train$capital_loss[train$capital_loss!=0])[1] 1867.898&gt; # 资本损失中位数&gt; median(train$capital_loss[train$capital_loss!=0])[1] 1887&gt; # 资本损失收益方差&gt; sd(train$capital_loss[train$capital_loss!=0])[1] 361.8574 资本损失的差异值不大，我们直接取中位数作为分界点 每周工作时间 每周工作时间 工作时间为1~99， 分割粒度设置为5 各连续变量的范围及对应的分割粒度如下： 变量名 范围 分割粒度 age 17~90 5 capital_gain 0 ~ 99999 0，0~7298，7298+ capital_loss 0 ~ 4356 0，0~1887， 1887+ hours_per_week 1~99 5 实验结果的分析 训练集规模的影响 问题：训练集的规模对分类效果有什么影响？ 选取5%， 50%， 100%的训练集数据训练分类模型 测试数据的选择，经过多方面的对比测试，发现选取的特征值为 1年龄 工作类别 学历等级 婚姻状况 职业 投资利得 投资损失 收入 时，训练的模型分类效果最好。以此为基础，分别选取 5%， 50%， 100%的训练集数据训练分类模型时的训练结果对比如下： 比例 训练集数目 准确率 5% 1494 83.71% 50% 15075 83.99% 100% 30162 84.12% 由这个表格可以看出，随着数据集的增加，分类器的分类效果越来越好。而实际上，即使只取了5%（1494条训练数据），训练出的分类器分类效果仍然挺好的，可以由此体会到贝叶斯分类的高效性和实用性。 重复随机抽取样本实验（5次），记录最小，最大，平均准确率 随机比例 训练集数目 准确率 70.13% 21131 83.99% 47.92% 14458 84.01% 94.40% 28472 84.04% 58.65% 17687 84.14% 11.83% 3566 83.95% 最小值 最大值 平均值 83.95% 84.14% 84.03% 综合两个测试结果可以得出如下结论：数据集的规模会对分类效果产生一定影响，但这种影响并不是绝对的，当抽取的训练集具有随机性时，小训练集也有可能会有特别好的分类效果，不过整体来看，训练集规模越大，分类效果越好。 0概率的处理 当测试集中某个数据的某条特征值取了某个值\(x_i\)，但训练集中该特征值并没有取过该值\(x_i\)，则在训练时\(P(x_i|y) = 0\)，由此在做测试时计算其概率\[\hat y = argmax_yP(y)\prod_{i=1}^{n}P(x_i|y)\]时会得到概率为0。 解决方法：通常我们会进行拉普拉斯平滑处理，即在计算条件概率时对每个\(x_i\) 做\(+\lambda\)处理, 对应的总数也需要做\(+M\lambda\)，经过测试可以发现，在未做拉普拉斯平滑时，训练集取50%时，分类准确率为83.95%， 加上拉普拉斯平滑处理之后，分类准确率为83.99%， 准确率有一定提升。 连续特征以及未知特征的处理 连续数据如何进行处理 说明：【数据的分析及离散化方案】见前面【3.4.3数据的离散化】小节。由于后期经过测试发现特征选取年龄 工作类别 学历等级 婚姻状况 职业 投资利得 投资损失 收入这8个特征时分类效果最好，故前面测试样例的特征选取均选择了这8个，为了测试连续数据分割方案对分类效果的影响，将每周工作时间这一连续特征加入分类的特征中。 测试时，训练集取100%， 拉普拉斯平滑处理的\(\lambda = 1\) 各连续特征值分割粒度与其分类效果对比 特征 分割粒度 准确率 age 不分割 83.816% age 3 83.831% age 5 83.751% age 10 83.784% 特征 分割粒度 准确率 hour 不分割 83.845% hour 3 83.845% hour 5 83.752% hour 10 83.804% 特征 分割粒度 准确率 投资收益与损失 不分割 【85.20%】 投资收益与损失 1000 83.62% 投资收益与损失 无，低，高 83.75% （注意：测试某一特征时，其余特征分割情况默认为： 年龄分割粒度 5， 每周工作时间分割粒度为5， 投资收益和损失分割为无， 收益/损失低, 收益/损失高） 结果分析：这里发现了一些很尴尬的结果，对这三个连续特征值进行不同粒度离散之后发现，【不进行离散】，直接以每一个数据单独作为一个类别进行分类时准确率反而比对其进行不同间距离散之后分类【准确率高】，特别是【投资收益与损失】的两个特征值，不进行离散时其准确率甚至高达 【85.2%】，而进行了等间距离散或者以无，低、高， 结合前面对各个特征的分布情况的统计可以进行如下猜测： 结合前面age, work hour, capital_gain, capital_loss几个特征值的取值特征统计结果， 可以发现，age和work hour在各个取值中虽然较为分散，但是也有小范围集中，简单的等间距离散，对其分类的优化效果不大，甚至于有可能因为粒度过大而使分类效果显著下降， 当分割粒度恰好使得集中数据分在了一个category时，其准确率会略高一点，而如果恰好使之分散开，可能会对分类准确率有反作用，如work hour特征分割粒度为5和10的对比。 对于投资收益和损失这两个特征值，分析其数据特征可以发现，有超过70%的数据是0，而其他数据就较为零散并且差异值极大，简单地等间距分割，或者以中位数，平均数作为临界点进行分割都是不太合理的，因此这种情况下不进行离散化分类效果反而会更好，（不知道高斯分布处理会不会优化其分类效果，由于能力和时间限制，没来得及进行测试） 未知数据如何处理 对于未知数据的处理，我做了两种情况的对比，一种是直接忽略掉这些数据，另一种是将‘？’也视为一种数据和特征 处理方案 准确率 忽略未知数据 84.12% 视为新类型 【84.45%】 可以看出，将未知数据视为一种特殊的新类型时其分类准确率有较大提高，可知这些数据某种程度上也能够反映出其收入的高低 选取的特征值对比 选取的特征值 准确率 年龄 工作类别 重量 学历等级 教育年限 婚姻状况 职业 家庭关系 人种 性别 投资利得 投资损失 每周工作时间 出生国 收入 79.77% 年龄 工作类别 学历等级 婚姻状况 职业 家庭关系 人种 性别 投资利得 投资损失 每周工作时间 出生国 收入 81.85% 年龄 工作类别 学历等级 婚姻状况 职业 家庭关系 性别 投资利得 投资损失 每周工作时间 出生国 收入 81.83% 年龄 工作类别 学历等级 婚姻状况 职业 人种 投资利得 投资损失 每周工作时间 收入 83.74% 年龄 工作类别 学历等级 婚姻状况 职业 投资利得 投资损失 每周工作时间 收入 83.75% 年龄 工作类别 学历等级 婚姻状况 职业 投资利得 投资损失 收入 84.12% 年龄 工作类别 教育年限 婚姻状况 职业 收入 81.79% 由这个表的对比分析可以明显地感受到特征值的选取对分类效果的影响,【年龄,工作类别,学历等级,婚姻状况,职业,投资利得,投资损失】这几个特征值能够很大程度上反应出其收入的高低，特别是投资收入和损失，这个特征与收入有较大的相关性，这也是为什么在【5.3.1 连续数据如何进行处理】一节中提到对这两个特征不进行分割时其分类效果甚至可以高达85.20%的一个原因。 这也提醒我们在选取训练数据时注意对特征值的选取，有代表性的特征对其分类准确率有促进作用，而一些无关的特征则会对分类效果有负作用。 交叉验证 选取不同比例的测试集数据用作训练，观察其对分类效果的影响 测试集比例 训练集+测试集 准确率 0% 15075 + 0 83.99% 5% 15075 + 751 84.02% 50% 15075 + 7525 84.23% 100% 15075 + 15060 84.26% [注] 训练集选取的是50%训练集数据 由这个数据对比可以看出，训练模型时加入部分测试集数据，对分类效果有提升作用。 实验总结及结论 本次实验实现了贝叶斯分类算法，并探讨了数据规模、特征值的选择对分类效果的影响，以及连续值，未知值的不同离散方案和处理方案对分类效果的影响，并探讨了拉普拉斯平滑对分类效果的影响。 通过多方面的对比，主要得出了以下一些结论: 特征值的选取对分类效果影响较大，可以根据特征值与类别的相关性大小判断其对分类是有帮助的还是有干扰的。 特征值个数也对分类效果有一定影响，选择合适的，足够的特征作为分类依据是较好的，特征值的选择和个数的选择可通过参数的调整进行尝试后得出最优方案 数据规模对分类效果有一定影响，但是只要特征值选取较好，训练数据较少情况下训练出的模型其分类效果也较好。 连续值的处理方式对分类效果影响较大，对于一些本身比较离散并且该特征值对类型影响较大情况下，分割粒度越小，分类效果会更好，尤其是本实验中投资收入和投资损失两个特征值，不进行分割时其准确率甚至高达85.2%，可明显体会到数据离散粒度对分类效果的影响。 未知数据的处理，未知数据视为一种特殊类型也是一种比较有效的处理方式，背后原因可能是这些没统计到数据的人群可能具有某方面的共性，其收入也会受到这方面的影响。 本次实验中自己收获很大，特别是学会了从不同的方面去评价一个算法的性能的方式的分析方法。此外，对编程也有一些新的启发，在编程时需要注意面向对象编程，考虑到各种可能的变化，本次实验中的贝叶斯分类器的实现方法我参考了【参考资料1 使用python编写朴素贝叶斯分类器】一文的实现方式，这种实现可以达到一种自适应的状态，不论特征值有多少个，是什么类型的数据，只要给出参数，均可对其进行训练和分类，这就方便了我们进行不同的特征值对分类效果的影响分析时的测试，只需要对测试文件和训练文件做对应修改即可。此外，在实现过程中设置了很多参数，可以方便进行训练数据比例，是否随机选取，特征值分割粒度，是否进行拉普拉斯平滑处理等进行设置，极大地方便了测试和分析。 参考资料 使用Python编写朴素贝叶斯分类器 https://dataminingguide.books.yourtion.com/chapter-6/chapter-6-6.html Dataset: Adult (R) http://scg.sdsu.edu/dataset-adult_r/]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[搜索引擎性能评价实验]]></title>
      <url>%2Farticle%2F%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%2F%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%E6%80%A7%E8%83%BD%E8%AF%84%E4%BB%B7%E5%AE%9E%E9%AA%8C.html</url>
      <content type="text"><![CDATA[实验步骤 自由分组， 至多2人一组 构建查询样例集合：利用网络资源(http://top.baidu.com/; http://top.sogou.com/等) 和个人使用经验构建查询样例集合，查询样例集合需覆盖不同查询热门程度（ 冷门/热门） 和各种类型的用户查询需求（ 导航类/信息类/事务类），样例集合的规模为10个查询，各类比例为2:5:3， 并根据个人经验，撰写每个查询样例的信息需求内容。 构建Pooling：学生根据其构建的查询样例集合，抓取常用的三个中文搜索引擎(百度、360好搜、 搜狗)对这部分查询词的查询结果，每个搜索引擎抓取查询结果的前十位结果，并利用这些结果 构建Pooling。 构建相关性标注集合：根据步骤2中撰写好的信息需求，对Pooling里的结果进行标注，标注为 “ 答案” 和“ 非答案” 两类即可。 根据标注结果，依据MAP，P@10，MRR等评价指标对各个搜索引擎的查询性能进行评价，并对 搜索引擎满足不同信息需求的情况加以比较，每人各自撰写实验报告。 查询样例集合创建 导航类 丝芙兰官网（热门） 河海大学主页（冷门） 信息类 萨德(热门), 美对朝忍耐到尽头(热门)，红薯 地瓜（冷门），青铜器制作流程（冷门）, 烟草学专业(冷门) 事务类 NBA直播(热门)， 金刚狼3下载（热门），张家界联想笔记本维修（冷门） 查询样例信息需求 丝芙兰官网（热门）：找到丝芙兰官网，查看产品信息，功效，购买方式等等 河海大学主页（冷门）：找到河海大学的校园主页 萨德：关于萨德的最新新闻，以及事件经历 美对朝忍耐到尽头：相关的新闻 红薯 地瓜：红薯与地瓜是否是同一种事物，各地有什么区别 青铜器制作流程：直接给出青铜器的制作方法 烟草学专业：查询专业简介，需要给出专业学习内容，就业方向，相关学校 NBA直播：最新NBA赛事直播，比分情况，赛况解说等 金刚狼3下载：给出电影的下载链接或者在线观看链接 张家界联想笔记本维修：给出维修点，联系方式等 构建pooling 针对三个搜索引擎分别抓取对应搜索数据，并进行标记，抓取结果及Pooling标记结果见【统计数据.xls】 性能指标计算 首先需要明确性能指标的计算方法 平均准确率（AP）： \[AP=\frac{1}{N}\sum_{i=1}^NPrecision(i)\] MAP MAP方法是Mean Average Precison，即平均准确率法的简称。其定义是求每个相关文档检索出后的准确率的平均值（即Average Precision）的算术平均值（Mean），即 \[MAP = mean(AP)\] RR 首位相关结果倒数RR,即出现第一个相关性标注的排序的倒数 \[RR = \frac{1}{Rank(1)}\] MRR MRR是平均排序倒数（Mean Reciprocal Rank）的简称，MRR方法主要用于寻址类检索（Navigational Search）或问答类检索（Question Answering）,MRR方法首先计算每一个查询的第一个相关文档位置的倒数，然后将所有倒数值求平均。 P@N P@N本身是Precision@N的简称，指的是对特定的查询，考虑位置因素，检测前N条结果的准确率 基于此，计算各词条的RR,P@10，AP，以及对搜索引擎的MRR,MAP，P@10结果如下： 分词条结果 进一步统计各个搜索引擎对不同类型的关键词的搜索结果性能： 各搜索引擎不同类别的统计结果 实验结论 按照统计结果做出各项指标的柱状图如下： MAP MP@10 MRR 总体的数据 由统计结果分析，从总体来看，在各项指标中，百度是三个搜索引擎中表现最好的,360的性能次之，而搜狗的结果则稍差一些。 导航类搜索词 对于导航类搜索关键词，RR一般用作评价导航类的查询需求，用于表示用户在知道目标前需要浏览的结果数目，可以看到，三大搜索引擎的导航类关键词的MRR指标均为1，可以发现，当用户想要搜索的信息为已知资源，主页，资源等信息时，搜索引擎可能会更倾向于返回给用户一些官方的主页信息，以使用户能够尽快找到目标，对于导航类信息的其他指标，相差也不大，但是P@10的指标值相差比较大，百度的P@10值是较好的，而360和搜狗的结果则稍差，查看原始搜索结果标记，三大搜索引擎都加入了对应的百科，问答平台，而搜狗和360的结果还夹杂了不少“同名的广告”，以“河海大学主页”词条为例，360和搜狗的结果中有不少标题虽是“河海大学招生网”等信息，但实际是一些培训机构的页面，两家的搜索引擎并没有做这方面的剔除，使得结果首页多了不少奇怪的“广告”，影响了搜索体验。另一个比较有趣的现象是，河海大学离退休工作处官网的名称是“河海大学主页”，这个页面在三大搜索引擎的结果中排第2、3位，可见搜索引擎背后会根据用户的点击数据调整结果的显示顺序。 信息类搜索词 信息类数据是用户搜索需求中占比最大的，用户的关注点在于结果的全面和权威性，对于这类搜索词，搜索引擎多数会给出其问答平台的结果，相关新闻结果，或者百科结果。对于信息类关键词，P@10是评价其搜索性能的较好指标，百度的数据在70%左右，而360和搜狗在60%左右，可见在中文搜索中，百度的确做得比较好，对于大多数信息类搜索词，百度的结果足够全面。对于新闻类的信息，三大搜索引擎结果差别并不是特别大，但是对于一些知识类信息，或者生活类信息的搜索，360和搜狗的表现则差强人意，以“红薯 地瓜”关键词为例，用户的搜索需求是查询红薯地瓜的区别，百度的结果大体上与之相符，而搜狗和360除了少数两三条结果与之相关，多数结果只与红薯有关，可以推测是由于搜索引擎的分词和联合搜索系统的处理方式的差异。 事务类搜索词 事物类搜索词中，百度的结果优势不是那么明显，甚至略差，360的结果则稍微更好一些， 这里差异较大的词条是金刚狼3下载这个搜索词条，其实这个词条是一个坑，一般来说这类资源可能在互联网上很少甚至不存在，因此很多数据可能其实是广告或者一些死链接，这时可能更需要搜索引擎去剔除一些不必要的结果以帮助用户完成其任务需求，360的结果大多数是迅雷的链接，而百度的结果则包含了各种不同的站点，这些站点大多数是广告等非用户目标站点，可能是出于广告费等方面的考虑吧，使得其结果表现并不好。 冷热门 对于热门数据，三大搜索引擎的表现都比较好，冷门数据百度表现依然较好，而360和搜狗的性能则有所下降，一方面可能是由于百度的市场占有率更大，用户更多，能够获取到的用户数据也更多更全面，即使是冷门搜索词由于有较大的用户基数也能得到较好地反馈结果，另一方面，百度的数据抓取可能更全面，对于不同类别的搜索词，百度的P@10指标均能达到近70%，可见其数据是比较齐全的，这也给其冷门搜索词的搜索提供的数据。 总结 从各方面的分析可以看出，百度的性能的确是最好的，分析推测其原因如下：百度的实力更强，硬件资源，软件资源均遥遥领先于其他两家搜索引擎，这就使得百度可以拿到更多的数据，拿到更全面的数据，这会对搜索引擎性能有较大的影响；此外，百度的用户群体更大，丰富的用户数据可以帮助百度动态优化其搜索结果的排序，进而提升用户体验；百度的分词和检索算法可能更优，正如前面提到的“地瓜 红薯”词条，百度的结果是两个词的联合结果，而360和搜狗的结果可能只与其中一个有关。对于搜狗来说，其搜索结果中有时总是会有微信或者知乎的结果，当用户的意图并不在此时，可能会极大地影响其体验。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[IPv6协议转发实验]]></title>
      <url>%2Farticle%2F%E7%BD%91%E7%BB%9C%2FIPv6%E5%8D%8F%E8%AE%AE%E8%BD%AC%E5%8F%91%E5%AE%9E%E9%AA%8C.html</url>
      <content type="text"><![CDATA[p{ text-indent: 2em; } 实验目的 通过前面的实验，我们已经深入了解了 IPv6 协议的分组接收和发送处理流程。本实验需要将实验模块的角色定位从通信两端的主机转移到作为中间节点的路由器上，在 IPv6 分组收发处理的基础上，实现分组的路由转发功能。 网络层协议最为关注的是如何将 IPv6 分组从源主机通过网络送达目的主机，这个任务就是由路由器中的 IPv6协议模块所承担。路由器根据自身所获得的路由信息，将收到的 IPv6分组转发给正确的下一跳路由器。如此逐跳地对分组进行转发，直至该分组抵达目的主机。 IPv6 分组转发是路由器最为重要的功能。 本实验设计实现路由器中的 IPv6 协议，可以在原有 IPv6 分组收发实验的基础上，增加 IPv6 分组的转发功能。对网络的观察视角由主机转移到路由器中，了解路由器是如何为分组选择路由，并逐跳地将分组发送到目的端的。 大家在本实验中也会初步接触路由表这一重要的数据结构，认识路由器是如何根据路由表对分组进行转发的。 实验具体任务 对于每一个到达本机的 IPv6 分组，根据其目的 IPv6 地址查找本机的路由表，对该分组进行如下的几类操作： 丢弃查不到路由的分组； 向上层协议上交目的地址为本机地址的分组； 根据路由查找结果，向相应接口转发其余的分组。 实验内容 实验内容主要包括： 设计路由表数据结构：设计路由表所采用的数据结构。要求能够根据 IPv6 地址来确定分组处理行为（丢弃、 上交或转发），转发情况下需获得下一跳的 IPv6 地址。路由表的数据结构和查找算法会极大的影响路由器的转发性能，有兴趣的同学可以深入思考和探索。 IPv6 分组的接收和发送：对前面实验中所完成的代码进行修改，在路由器协议栈的 IPv6 模块中能够正确完成分组的接收和发送处理。具体要求不做改变，参见IPv6 分组收发实验。 IPv6 分组的转发：对于需要转发的分组进行处理，获得下一跳的 IP 地址，然后调用发送接口函数进一步处理。 实验过程： 程序整体设计流程 程序运行流程 实验流程如图1所示，在下层接收接口函数stud_ipv6_fwd_deal( )中(图1接口函数1)，实现 分组接收处理。其主要功能是根据分组中目的 IPv6 地址查找路由表，根据路由表查找结果进行后续处理。 分组需要上交，则调用接口函数ipv6_fwd_LocalRcv( )(图 5.1 中接口函数2); 需要丢弃，则调用函数 ipv6_fwd_DiscardPkt( )(图 5.1 中函数5); 需要转发，则进行转发操作。转发操作的实现要点包括: Hop Limit 值减 1，然后调用发送接口函数 ipv6_fwd_SendtoLower( )(图 5.1 中接口函数 4)将分组发送出去。 接口函数ipv6_fwd_SendtoLower( )比前面实验增加了一个参数nexthop，要求 在调用时传入下一跳的 IPv6 地址，此地址是通过查找路由表得到的。 另外，本实验增加了一个路由表配置的接口(图1中函数6)，要求能够根据系统所给信息来设定本机路由表。实验中只需要简单地设置静态路由信息，以作为分组接收和发送处理的判断依据。 路由表数据结构设计及路由查找过程 在本实验的实现过程中，我的路由表数据结构是采用链表进行实现的，路由查找时顺序遍历链表比对掩码和目的地址，并记录当前最长匹配结果。路由的掩码匹配做法是先将IPv6地址转换为128bit的位数组，然后比对掩码长度位，具体如下： 转换为bitset&lt;128&gt; 位数组 123456789bitset&lt;128&gt; trans_bit_addr(ipv6_addr addr) &#123; bitset&lt;128&gt; bit_ipv6; for (int i = 0; i &lt; 4; ++i) &#123; bit_ipv6 = bit_ipv6 | (bitset&lt;128&gt; (addr.dwAddr[3-i]) ) &lt;&lt; (32 * i); // 按位或将ipv6_addr的四个long型数据转换为对应的位数组，左移96，64，32，0位 &#125; return bit_ipv6;&#125; 路由查找 1234567891011121314151617181920212223242526272829303132333435list&lt;stud_ipv6_route_msg&gt;::iterator router;bool match = false;// 是否找到匹配项int MAX_LENGTH = 0;// 标记当前最长匹配// 利用bitset将目的ip地址转换为128位数组bitset&lt;128&gt; dest_ipv6 = trans_bit_addr(*dest_addr);ipv6_addr next;for(router = router_tables.begin(); router != router_tables.end(); ++router) &#123; // 遍历路由表 if (router -&gt; masklen &lt; MAX_LENGTH) &#123; // 短于当前已匹配路由表 continue; &#125; // 将路由表中目的ip地址换算为bit表示的数组 bitset&lt;128&gt; router_dest = trans_bit_addr(router -&gt; dest); bool local_match = true; for(int i = 0; i &lt; router -&gt; masklen; ++i) &#123; if (router_dest[i] != dest_ipv6[i]) &#123; local_match = false; break; &#125; &#125; if (local_match) &#123; match = true; // 匹配则更新masklen, 更新路由地址 next = router -&gt; nexthop; MAX_LENGTH = router -&gt; masklen; &#125;&#125; 优化措施 这样的设计其实效率是比较低的，可以进一步完成的优化是：在插入新的路由信息至路由链表时，按照掩码从大到小的顺序插入，这样在遍历查找路由信息时，查找到第一个匹配的记录即可停止遍历。 更进一步的优化是采用二叉树的形式进行构建。 下层分组接收函数 stud_ipv6_fwd_deal( ) 本函数是 IPv6 协议接收流程的下层接口函数，实验系统从网络中接收到分组后 会调用本函数。调用该函数之前已完成 IPv6 分组的合法性检查，因此本函数应该考虑实现 如下功能: 判定是否为发给本机的分组，如果是，则调用 ipv6_fwd_LocalRcv(). 1234567891011121314151617181920// a. 查找是否是发向本机// 先获取目的地址，第192bit起为目的ip，故偏移24Byteipv6_addr *dest_addr = (ipv6_addr*) (pBuffer + 24);ipv6_addr *local_addr = new ipv6_addr();// 本地IPv6地址getIpv6Address(local_addr);bool is_lcoal = true;for(int i = 0; i &lt;16; ++i) &#123; if(dest_addr-&gt;bAddr[i] != local_addr-&gt;bAddr[i]) &#123; // 分组不是本地 is_lcoal = false; break; &#125;&#125;if(is_lcoal) &#123; // 是本机接收分组，直接上交 ipv6_fwd_LocalRcv(pBuffer, length); return 0;&#125; 按照最长匹配原则查找路由表，获取下一跳 IPv6 地址。查找失败，则调用 ipv6_fwd_DiscardPkt(). 123456789101112131415161718192021222324252627282930313233343536// b. 按照最长匹配查找路由表获取下一跳，查找失败则调用ipv6_fwd_DiscardPkt( )；∂list&lt;stud_ipv6_route_msg&gt;::iterator router;bool match = false;// 是否找到匹配项int MAX_LENGTH = 0;// 标记当前最长匹配// 利用bitset将目的ip地址转换为128位数组bitset&lt;128&gt; dest_ipv6 = trans_bit_addr(*dest_addr);ipv6_addr next;for(router = router_tables.begin(); router != router_tables.end(); ++router) &#123; // 遍历路由表 if (router -&gt; masklen &lt; MAX_LENGTH) &#123; // 短于当前已匹配路由表 continue; &#125; // 将路由表中目的ip地址换算为bit表示的数组 bitset&lt;128&gt; router_dest = trans_bit_addr(router -&gt; dest); bool local_match = true; for(int i = 0; i &lt; router -&gt; masklen; ++i) &#123; if (router_dest[i] != dest_ipv6[i]) &#123; local_match = false; break; &#125; &#125; if (local_match) &#123; match = true; // 匹配则更新masklen, 更新路由地址 next = router -&gt; nexthop; MAX_LENGTH = router -&gt; masklen; &#125;&#125; 查找成功，则调用 ipv6_fwd_SendtoLower()，完成分组发送。 转发过程中注意对 Hop Limit 的处理。 123456789101112131415161718// c. 需要转发，hop limit - 1 ,调用 ipv6_fwd_SendtoLower( )完成报文发送if(match) &#123; // 获取hop limit，注意转换为本地字节序 char HopLimit = *(char *)(pBuffer + 7); HopLimit -= 1; memcpy(pBuffer + 7, &amp;(HopLimit), sizeof(char)); if(HopLimit == 1) &#123; // 直接发向目的地址 ipv6_fwd_SendtoLower(pBuffer, length, dest_addr); return 0; &#125; if(HopLimit &gt; 1) &#123; ipv6_fwd_SendtoLower(pBuffer, length, &amp;next); return 0; &#125;&#125; 实验结果分析 分析传输的报文，处理正确 实验结果 遇到的问题及解决 掩码的比对问题，由于IPV6地址是128位长，其实际存储结构为 123456typedef union&#123; char bAddr[16]; unsigned short wAddr[8]; long dwAddr[4];&#125;ipv6_addr; 其掩码比对无法像IPV4地址那样直接简单地进行移位操作，因此，我想到的一个解决方法是将其转换为长度为128的位数组，进一步依次进行比较。 处理IPV6数据包时的网络字节序和本机字节序问题，网络字节序是大端模式，而本机字节序一般是小端模式，因此需要注意其字节序的转换，在本实验的实现过程中，IPv6地址的比较是采用按位比对的模式，由于计算机底层硬件bit序并不受解析模式的影响，因此不需要考虑字节序转换问题，而解析hop limit时，由于是直接截取的char类型(8 bit),也不会受字节序的影响，也没有必要进行字节序的转换。 思考题：实验指导书的思考题（2） IPv6和IPv4地址长度存在显著差别，考虑路由查找过程中各自采用什么样的数据结构和算法有助于提高查找效率 答： IPV4的地址为32位长，不考虑空间占用的话，实际上最快的一种数据结构应该是二叉树，即根据目的地址的比特串构建二叉树即可，这样搜索时最多需要32步，可以说是可以极大的减小查询时间，当然这样的空间占用会比较高，通常会做出压缩的优化以及其他优化，可以说IPv4的路由查询可以做到非常高效的查询。 而IPv6地址长度为128位，是不可能像IPv4那样建一个二叉比特树的，查阅相关资料，有如下解决方案： 已有IPv6路由表的统计结果和地址分配策略表明，长度大于48比特的前缀比例很低，仅为5%左右，因此可对IPv6地址的高48位采用多分支trie进行查找，剩余的16比特直接采用hash查找。根据IPv6地址前缀的分布特点，构造查找步宽为24-8-8-8-16的五层多分支TrieC树，限制最坏情况下路由查找的访存次数。 通过这样的方式可以极大地加快IPV6的路由查询。 总结 本次实验实现了IPV6分组的转发，通过本次实验，我更加清晰地认识了网络原理课程中的IPv6数据 包转发的过程，对最长匹配原则也有了一个清晰的理解，对网络字节序和本机字节序有了更深入的理解。感谢助教的热心帮助。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Linux/Mac 常用命令]]></title>
      <url>%2Farticle%2Flinux-Mac-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4.html</url>
      <content type="text"><![CDATA[使用pandoc将markdown输出为pdf 安装配置及常用命令 pandoc的安装与配置 1234567891011121314151617181920212223242526pandoc hello-world.md -o out.pdf --latex-engine=xelatexpandoc -D latex &gt; mytemplate.tex# 如果提示pandoc无法找到插件xelatex（支持输出中文），则下载Tex，BasicTexpandoc hello-world.md -o out.pdf --latex-engine=/Library/TeX/texbin/xelatex# tips：可以将该路径加入$PATH# 指定中英文字体pandoc -N -s --toc --smart --latex-engine=xelatex -V CJKmainfont='Kai' -V mainfont='Monaco' -V geometry:margin=1in hello-world.md -o output.pdfpandoc -N -s --toc --smart -V CJKmainfont='Songti SC' -V mainfont='Monaco' --variable sansfont="Monaco" --variable monofont="Monaco" --variable fontsize=26pt --latex-engine=xelatex -V geometry:margin=1in IPv6协议转发实验.md -o example14.pdfpandoc -s -N --smart --template default.tex -V CJKmainfont='Songti SC' -V mainfont='Monaco' --variable sansfont="Monaco" --variable monofont="Monaco" --latex-engine=xelatex -V geometry:margin=1in --listings -H code.tex Massive-Data-Process-Homework-4.md -o 计41-张盛豪-2014011450-作业4.pdf &amp;&amp; open 计41-张盛豪-2014011450-作业4.pdf &amp;&amp; cp 计41-张盛豪-2014011450-作业4.pdf /Users/alexzhangch/Documents/jianguoyun/2017_summer/pandoc -s -N --smart --template default.tex -V CJKmainfont='Songti SC' -V mainfont='Monaco' --variable sansfont="Monaco" --variable monofont="Monaco" --latex-engine=xelatex -V geometry:margin=1in --listings -H code.tex CCF2016.md -o CCF2016.pdfpandoc -s -N --toc --smart --template default.tex -V CJKmainfont='Songti SC' -V mainfont='Monaco' --variable sansfont="Monaco" --variable monofont="Monaco" --latex-engine=xelatex -V geometry:margin=1in --listings -H code.tex 大规模数据处理大作业报告.md -o FinalProjectReport.tex &amp;&amp; open FinalProjectReport.pdf &amp;&amp; cp FinalProjectReport.pdf /Users/alexzhangch/Documents/jianguoyun/2017_summer/- --toc: 生成带目录的pdf文档pandoc -s -N --smart --template /Users/alexzhangch/Documents/git_project/blog/qingfeng14.github.io/source/_posts/default.tex -V CJKmainfont='Songti SC' -V mainfont='Monaco' --variable sansfont="Monaco" --variable monofont="Monaco" --latex-engine=xelatex -V geometry:margin=1in --listings -H /Users/alexzhangch/Documents/git_project/blog/qingfeng14.github.io/source/_posts/code.tex 密码学-Exercise1.md -o out.pdfpandoc -s -N --toc --smart --template /Users/alexzhangch/Documents/git_project/blog/qingfeng14.github.io/source/_posts/default.tex -V CJKmainfont='Songti SC' -V mainfont='Monaco' --variable sansfont="Monaco" --variable monofont="Monaco" --latex-engine=xelatex -V geometry:margin=1in --listings -H /Users/alexzhangch/Documents/git_project/blog/qingfeng14.github.io/source/_posts/code.tex 4over6实验报告.md -o IPv4_over_IPv6 实验报告.pdf &amp;&amp; open IPv4_over_IPv6实验报告.pdf &amp;&amp; cp IPv4_over_IPv6实验报告.pdf /Users/alexzhangch/Documents/git_project/4over6/IPv4_over_IPv6实验客户端报告.pdf &amp;&amp; cp 4over6实验报告.md client_report.md Mac 下的中文字体集 12345678910宋体仿宋 STFangsong宋体黑体 STHei宋体简体 Songti SC PingFang SC LingWai SC LiSong Pro LiHei Pro Libian SC Lantinghei TC Kaiti SC 1\documentclass[$if(fontsize)$$fontsize$,$endif$$if(lang)$$babel-lang$,$endif$$if(papersize)$$papersize$paper,$endif$$for(classoption)$$classoption$$sep$,$endfor$]&#123;$documentclass$&#125; pandoc 导出pdf 图片位置固定 Unix/Linux 系统下修改环境变量 123sudo vi .bash_profile# 添加需要增加的环境变量PATH=$PATH:/Library/TeX/texbin/ 然后重新启动terminal即可 sublime 快捷键插入自定义字符串 Preference-&gt; Key Bingding-&gt; 修改user settings 在其中加入自定义快捷键 12345678910111213141516171819202122232425262728293031[ &#123; "keys": ["ctrl+r"], "command": "insert_snippet", "args": &#123; "contents": "&lt;font color=red&gt;$&#123;1:&#125;$SELECTION&lt;/font&gt;$&#123;0&#125;" &#125; &#125;, &#123; "keys": ["super+b"], "command": "insert_snippet", "args": &#123; "contents": "**$&#123;1:&#125;$SELECTION**$&#123;0&#125;" &#125; &#125;, &#123; "keys": ["ctrl+shift+j"], "command": "insert_snippet", "args": &#123; "contents": "&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;$&#123;1:&#125;$SELECTION$&#123;0&#125;" &#125; &#125;, &#123; "keys": ["ctrl+b"], "command": "insert_snippet", "args": &#123; "contents": "&lt;font color=blue&gt;$&#123;1:&#125;$SELECTION&lt;/font&gt;$&#123;0&#125;" &#125; &#125;,] 其中光标所在位置为${1:}$SELECTION所在地 树莓派开机自启动脚本 12345678910111213141516171819202122232425#!/bin/sh### BEGIN INIT INFO# Provides: auto_start# Required-Start: $remote_fs# Required-Stop: $remote_fs# Default-Start: 2 3 4 5# Default-Stop: 0 1 6# Short-Description: Start or stop the HTTP Proxy.### END INIT INFOcase $1 in start) # svnserve -d -r /home/pi/svn_repository python /home/pi/Documents/tips/auto_login.py python /home/pi/Documents/tips/start_note.py python /home/pi/Documents/tips/baidu_weather.py php5 /home/pi/Documents/tips/bind_dns/bind.php pi php5 /home/pi/Documents/tips/bind_dns/bind.php survey ;; stop) killall svnserve ;;*)echo "Usage: $0 (start|stop)";;esac 注：本意是想写一个可以开机自启动的脚本，脚本的目的是登录校园网，然后将本机的IP地址更新到自己的域名中，向linux机器添加开机自启动脚本的方法为： 123456789# 在/etc/init.d/文件夹下添加新的启动脚本，格式参考上面，注意需要加入BEGIN INIT INFO那一段注释，否则在加入启动项时会出现LSB不存在的errorsudo chmod +x 775 name# 给脚本足够运行权限sudo update-rc.d name defaults 95# 将自启动脚本name添加到自启动表项中，启动顺序为95，注意，需要使用网络的脚本顺序要大于90sudo update-rc.d -f name remove# 移除自启动项name 这样既可开机自启动该脚本了 但是在实际运行中发现并不能实现自动的登录以及域名解析的更新，分析了之后发现该脚本启动时间太早，机器还没有分到ip地址，无法完成网络请求，于是将该脚本加入到了/etc/rc.local自启动脚本中，linux系统的脚本启动顺序为： 123456/etc/init.d/ # init.d目录包含许多系统各种服务的启动和停止脚本。/etc/rc.local# 脚本是在系统初始化级别脚本运行之后再执行的，# 因此可以安全地在里面添加你想在系统启动之后执行的脚本。 在rc.local文件中，exit 0之前加入我们的启动脚本 为了保证可以联网，我们在脚本中设置了一个死循环，重复登录直到登录成功 12345678910111213141516while True: try: f = urllib2.urlopen(url, post_data) # `这里完成了向登录页面发送登录POST请求的操作，只有这一步成功完成之后才能进行下一步的绑定域名脚本的执行，如果尚未分配到ip则会进入异常处理等待5秒继续尝试登录` content = f.read() # print content result=urllib.urlopen('http://baidu.com').read() print result print os.popen('sh /home/pi/Documents/bind.sh').read() # 在python中执行shell脚本的方法，这个函数可以输出脚本的输出信息 print "Network is Ready!" break except Exception , e: print e print "Network is not ready,Sleep 5s..." time.sleep(5) python 调用命令行 os.popen这种调用方法可以输出脚本的输出信息,推荐使用 12import osos.popen('sh /home/pi/Documents/bind.sh').read() os.system 会输出一些奇怪的东西 12import osos.system("echo \"Hello World\"") 查看应用安装位置 12345# 查看运行文件所在处which hexo# 查看应用安装位置whereis hexo git 相关 git status 中文乱码问题: git config --global core.quotepath false git gitignore 不起作用: 1234git rm -r --cached .git add .git commit -m 'update .gitignore'git push -u origin master Mac 下鼠标设置 defaults read -g com.apple.mouse.scaling defaults write -g com.apple.mouse.scaling 7 mac GCC 问题 12ld: symbol(s) not found for architecture i386clang: error: linker command failed with exit code 1 (use -v to see invocation) 使用标准c库 1gcc -o run hw3a.cpp -lstdc++ Ubuntu 内存查看及释放 实时查看内存信息 123watch -n 1 free -mwatch -n 1 cat /proc/meminfo 释放内存 1234sudo sysctl -w vm.drop_caches=3sudo sync &amp;&amp; echo 3 | sudo tee /proc/sys/vm/drop_caches 123456789101112131415# 1. Freeing Up the Page Cacheecho 1 &gt; /proc/sys/vm/drop_caches# 2. Freeing Up the Dentries and Inodesecho 2 &gt; /proc/sys/vm/drop_caches# 3. Freeing Up the Page Cache, Dentries and Inodesecho 3 &gt; /proc/sys/vm/drop_caches# 4. Flushing the File System Bufferssync 树莓派查看ARM GPU内存分配 1vcgencmd get_mem arm &amp;&amp; vcgencmd get_mem gpu mysql 123456mkdir /var/run/mysqld/chown mysql: /var/run/mysqld/# Unknown/unsupported storage engine: InnoDB# 这条命令是关键sudo -u mysql mysqld --skip-innodb --default-storage-engine=myisam cat 输出某几行 【一】从第3000行开始，显示1000行。即显示3000~3999行 cat filename | tail -n +3000 | head -n 1000 【二】显示1000行到3000行 cat filename| head -n 3000 | tail -n +1000 *注意两种方法的顺序 分解： tail -n 1000：显示最后1000行 tail -n +1000：从1000行开始显示，显示1000行以后的 head -n 1000：显示前面1000行 查看linux CPU核数 1234567891011# 总核数 = 物理CPU个数 X 每颗物理CPU的核数 # 总逻辑CPU数 = 物理CPU个数 X 每颗物理CPU的核数 X 超线程数# 查看物理CPU个数cat /proc/cpuinfo| grep &quot;physical id&quot;| sort| uniq| wc -l# 查看每个物理CPU中core的个数(即核数)cat /proc/cpuinfo| grep &quot;cpu cores&quot;| uniq# 查看逻辑CPU的个数cat /proc/cpuinfo| grep &quot;processor&quot;| wc -l AWK命令 shell 统计文本行数 1awk &quot;END &#123;print NR&#125;&quot; Clean_2015.csv shell 等步长读取文本 1awk &quot;NR % 10000 == 0&quot; Clean_2015.csv &gt; sample.csv OpenVpn 配置 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303################################################## Sample OpenVPN 2.0 config file for ## multi-client server. ## ## This file is for the server side ## of a many-clients &lt;-&gt; one-server ## OpenVPN configuration. ## ## OpenVPN also supports ## single-machine &lt;-&gt; single-machine ## configurations (See the Examples page ## on the web site for more info). ## ## This config should work on Windows ## or Linux/BSD systems. Remember on ## Windows to quote pathnames and use ## double backslashes, e.g.: ## &quot;C:\\Program Files\\OpenVPN\\config\\foo.key&quot; ## ## Comments are preceded with &apos;#&apos; or &apos;;&apos; ################################################### Which local IP address should OpenVPN# listen on? (optional);local a.b.c.d# Which TCP/UDP port should OpenVPN listen on?# If you want to run multiple OpenVPN instances# on the same machine, use a different port# number for each one. You will need to# open up this port on your firewall.port 1995# TCP or UDP server?# 如果使用的是ipv6进行连接的话（比如教育网），需要将下面所示的udp修改为udp6。如果使用ipv4进行连接的话，则不用修改此处。;proto tcpproto udp# &quot;dev tun&quot; will create a routed IP tunnel,# &quot;dev tap&quot; will create an ethernet tunnel.# Use &quot;dev tap0&quot; if you are ethernet bridging# and have precreated a tap0 virtual interface# and bridged it with your ethernet interface.# If you want to control access policies# over the VPN, you must create firewall# rules for the the TUN/TAP interface.# On non-Windows systems, you can give# an explicit unit number, such as tun0.# On Windows, use &quot;dev-node&quot; for this.# On most systems, the VPN will not function# unless you partially or fully disable# the firewall for the TUN/TAP interface.;dev tapdev tun# Windows needs the TAP-Win32 adapter name# from the Network Connections panel if you# have more than one. On XP SP2 or higher,# you may need to selectively disable the# Windows firewall for the TAP adapter.# Non-Windows systems usually don&apos;t need this.;dev-node MyTap# SSL/TLS root certificate (ca), certificate# (cert), and private key (key). Each client# and the server must have their own cert and# key file. The server and all clients will# use the same ca file.## See the &quot;easy-rsa&quot; directory for a series# of scripts for generating RSA certificates# and private keys. Remember to use# a unique Common Name for the server# and each of the client certificates.## Any X509 key management system can be used.# OpenVPN can also use a PKCS #12 formatted key file# (see &quot;pkcs12&quot; directive in man page).ca ca.crtcert server.crtkey server.key # This file should be kept secret# Diffie hellman parameters.# Generate your own with:# openssl dhparam -out dh1024.pem 1024# Substitute 2048 for 1024 if you are using# 2048 bit keys.# 加密长度dh dh2048.pem# Configure server mode and supply a VPN subnet# for OpenVPN to draw client addresses from.# The server will take 10.8.0.1 for itself,# the rest will be made available to clients.# Each client will be able to reach the server# on 10.8.0.1. Comment this line out if you are# ethernet bridging. See the man page for more info.server 10.8.0.0 255.255.255.0# Maintain a record of client &lt;-&gt; virtual IP address# associations in this file. If OpenVPN goes down or# is restarted, reconnecting clients can be assigned# the same virtual IP address from the pool that was# previously assigned.ifconfig-pool-persist ipp.txt# Configure server mode for ethernet bridging.# You must first use your OS&apos;s bridging capability# to bridge the TAP interface with the ethernet# NIC interface. Then you must manually set the# IP/netmask on the bridge interface, here we# assume 10.8.0.4/255.255.255.0. Finally we# must set aside an IP range in this subnet# (start=10.8.0.50 end=10.8.0.100) to allocate# to connecting clients. Leave this line commented# out unless you are ethernet bridging.;server-bridge 10.8.0.4 255.255.255.0 10.8.0.50 10.8.0.100# Configure server mode for ethernet bridging# using a DHCP-proxy, where clients talk# to the OpenVPN server-side DHCP server# to receive their IP address allocation# and DNS server addresses. You must first use# your OS&apos;s bridging capability to bridge the TAP# interface with the ethernet NIC interface.# Note: this mode only works on clients (such as# Windows), where the client-side TAP adapter is# bound to a DHCP client.;server-bridge# Push routes to the client to allow it# to reach other private subnets behind# the server. Remember that these# private subnets will also need# to know to route the OpenVPN client# address pool (10.8.0.0/255.255.255.0)# back to the OpenVPN server.;push &quot;route 192.168.10.0 255.255.255.0&quot;;push &quot;route 192.168.20.0 255.255.255.0&quot;# To assign specific IP addresses to specific# clients or if a connecting client has a private# subnet behind it that should also have VPN access,# use the subdirectory &quot;ccd&quot; for client-specific# configuration files (see man page for more info).# EXAMPLE: Suppose the client# having the certificate common name &quot;Thelonious&quot;# also has a small subnet behind his connecting# machine, such as 192.168.40.128/255.255.255.248.# First, uncomment out these lines:;client-config-dir ccd;route 192.168.40.128 255.255.255.248# Then create a file ccd/Thelonious with this line:# iroute 192.168.40.128 255.255.255.248# This will allow Thelonious&apos; private subnet to# access the VPN. This example will only work# if you are routing, not bridging, i.e. you are# using &quot;dev tun&quot; and &quot;server&quot; directives.# EXAMPLE: Suppose you want to give# Thelonious a fixed VPN IP address of 10.9.0.1.# First uncomment out these lines:;client-config-dir ccd;route 10.9.0.0 255.255.255.252# Then add this line to ccd/Thelonious:# ifconfig-push 10.9.0.1 10.9.0.2# Suppose that you want to enable different# firewall access policies for different groups# of clients. There are two methods:# (1) Run multiple OpenVPN daemons, one for each# group, and firewall the TUN/TAP interface# for each group/daemon appropriately.# (2) (Advanced) Create a script to dynamically# modify the firewall in response to access# from different clients. See man# page for more info on learn-address script.;learn-address ./script# If enabled, this directive will configure# all clients to redirect their default# network gateway through the VPN, causing# all IP traffic such as web browsing and# and DNS lookups to go through the VPN# (The OpenVPN server machine may need to NAT# or bridge the TUN/TAP interface to the internet# in order for this to work properly).push &quot;redirect-gateway def1 bypass-dhcp&quot;# Certain Windows-specific network settings# can be pushed to clients, such as DNS# or WINS server addresses. CAVEAT:# http://openvpn.net/faq.html#dhcpcaveats# The addresses below refer to the public# DNS servers provided by opendns.com.push &quot;dhcp-option DNS 208.67.222.222&quot;push &quot;dhcp-option DNS 208.67.220.220&quot;# Uncomment this directive to allow different# clients to be able to &quot;see&quot; each other.# By default, clients will only see the server.# To force clients to only see the server, you# will also need to appropriately firewall the# server&apos;s TUN/TAP interface.;client-to-client# Uncomment this directive if multiple clients# might connect with the same certificate/key# files or common names. This is recommended# only for testing purposes. For production use,# each client should have its own certificate/key# pair.## IF YOU HAVE NOT GENERATED INDIVIDUAL# CERTIFICATE/KEY PAIRS FOR EACH CLIENT,# EACH HAVING ITS OWN UNIQUE &quot;COMMON NAME&quot;,# UNCOMMENT THIS LINE OUT.;duplicate-cn# The keepalive directive causes ping-like# messages to be sent back and forth over# the link so that each side knows when# the other side has gone down.# Ping every 10 seconds, assume that remote# peer is down if no ping received during# a 120 second time period.keepalive 10 120# For extra security beyond that provided# by SSL/TLS, create an &quot;HMAC firewall&quot;# to help block DoS attacks and UDP port flooding.## Generate with:# openvpn --genkey --secret ta.key## The server and each client must have# a copy of this key.# The second parameter should be &apos;0&apos;# on the server and &apos;1&apos; on the clients.;tls-auth ta.key 0 # This file is secret# Select a cryptographic cipher.# This config item must be copied to# the client config file as well.;cipher BF-CBC # Blowfish (default)# 选择使用AES进行加密cipher AES-128-CBC # AES;cipher DES-EDE3-CBC # Triple-DES# Enable compression on the VPN link.# If you enable it here, you must also# enable it in the client config file.comp-lzo# The maximum number of concurrently connected# clients we want to allow.;max-clients 100# It&apos;s a good idea to reduce the OpenVPN# daemon&apos;s privileges after initialization.## You can uncomment this out on# non-Windows systems.# 会将其权限限制在用户nobody和用户组nogroup的等级上，而其没有特权，所以能保证安全。user nobodygroup nogroup# The persist options will try to avoid# accessing certain resources on restart# that may no longer be accessible because# of the privilege downgrade.persist-keypersist-tun# Output a short status file showing# current connections, truncated# and rewritten every minute.status openvpn-status.log# By default, log messages will go to the syslog (or# on Windows, if running as a service, they will go to# the &quot;\Program Files\OpenVPN\log&quot; directory).# Use log or log-append to override this default.# &quot;log&quot; will truncate the log file on OpenVPN startup,# while &quot;log-append&quot; will append to it. Use one# or the other (but not both).;log openvpn.log;log-append openvpn.log# Set the appropriate level of log# file verbosity.## 0 is silent, except for fatal errors# 4 is reasonable for general usage# 5 and 6 can help to debug connection problems# 9 is extremely verboseverb 3# Silence repeating messages. At most 20# sequential messages of the same message# category will be output to the log.;mute 20]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[[计算机系统结构]指令系统]]></title>
      <url>%2Farticle%2F%E7%B3%BB%E7%BB%9F%E7%BB%93%E6%9E%84%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F%E7%BB%93%E6%9E%84-%E6%8C%87%E4%BB%A4%E7%B3%BB%E7%BB%9F.html</url>
      <content type="text"><![CDATA[指令系统，寻址方式，内存映射，大端小端，哈夫曼编码，单地址指令，两地址指令 寄存器寻址、立即值寻址、偏移寻址、寄存器间接寻址、索引寻址、直接寻址或绝对对寻址、存储器间接寻址、自增寻址、自减寻址、缩放寻址、 寻址技术 编址方式 编址方式: 对各种存储设备进行编码的方法 编址单位: 字编址，字节编址，位编址，块编址 寻址方式 寻址方式：指令系统中如何形成所要访问的数据的地址。 寻址方式可以指明指令中的操作数是一个常数、一个寄存器操作数或者是一个存储器操作数。 对于存储器操作数来说，由寻址方式确定的存储器地址称为有效地址。 &lt;-：赋值操作 Mem：存储器 Regs：寄存器组 方括号：表示内容 Mem[ ]：存储器的内容 Regs[ ]：寄存器的内容 Mem[Regs[R1]]：以寄存器R1中的内容作为地址的存储器单元中的内容 张晨曦 计算机系统结构教程 操作数寻址方式 定位方式 程序的主存物理地址方式什么时候确定，采用什么方法实现 主要的定位方式： 直接定位方式：程序装入主存储器之前，指令和数据的主存物理地址就已经确定了 静态定位：程序装入主存过程中进行地址变换 动态定位：程序执行过程中，访问到对应指令或数据才进行地址变换 指令格式的优化设计 目标： 节省程序的存储空间，指令格式尽量规整，便于译码 指令组成 操作码和地址码 地址码包含： 地址：地址码，立即数，寄存器 地址的附加信息：偏移量，块长度 寻址方式 操作码的优化设计：节省程序存储空间 优化方式:固定长度、Huffman编码、扩展编码 固定长度操作码 规整，译码简单， 但浪费信息量 Huffman编码 操作码的最短平均长度 \[H = -\sum_{i=1}^np_i * \log_2p_i\] 其中\(p_i\)表示第\(i\)种操作码在程序中出现的频率 地址码的优化设计 指令格式优化设计 计算机系统的性能评价 时钟频率 指令执行速度 一种经典的表示运算速度的方法 MIPS(Million Instructions Per Second), GIPS, TIPS \[MIPS = \frac{指令条数}{执行时间\times 10^6} = \frac{Fz}{CPI} = IPC \times Fz\] 其中: Fz为处理机的工作主频 CPI(Cycles Per Instruction)为每条指令所需的平均时钟周期数 IPC(Instruction Per Cycle)为每个时钟周期平均执行的指令条数 平均速度 核心程序法 峰值速度]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[[搜索引擎]性能评价]]></title>
      <url>%2Farticle%2F%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E%2F%E6%90%9C%E7%B4%A2%E5%BC%95%E6%93%8E-%E6%80%A7%E8%83%BD%E8%AF%84%E4%BB%B7.html</url>
      <content type="text"><![CDATA[搜索引擎性能评价的目的 对用户而言： 信息获取渠道 对搜索引擎广告商而言 对搜索技术研究人员而言 对搜索引擎服务提供者 从较差查询样例中学习 链接锚本重定向问题 清华美院 团、派 搜索引擎性能评价流程 1.1 搜索引擎性能评价的对象 网络服务提供商的属性 市场占用率、 网络信息检索工具属性 网络数据环境、用户群体 搜索引擎运行效果 响应时间 耗费的硬件资源 是否满足用户的信息需求 用户获取信息耗费的时间成本？ 根据用户的需求调整响应时间和查询质量？ 1.2 搜索引擎性能评价的Cranfield体系 黑箱评价方式：给定标准输入情况下，看系统输出与标准输出的差异 输入:语料库，查询样例、相关性标注 优势：复用性（一次标注，多次使用） 语料库采集 信息检索系统：提供固定的语料库集合，集合规模适当，数据质量可靠 商业搜索引擎：不提供固定的语料库集合，评价数据抓取子系统性能 查询样例集合构建 用户查询规模庞大 核心问题：如何采样：真实性，精确性，全面性 查询采样的真实性：反映用户实际需求 来源：用户查询日志行为（日志隐私保护）、公开数据资源 查询采样的精确性：减少数据标注困难 查询采样的全面性：综合评价各方面性能 少量查询样例代表大多数需求 采样依据：内容类别、热门程度、需求类型 查询信息需求决定了用户使用搜索引擎的满意度，数据环境复杂用户意图难以琢磨 搜索引擎用户信息需求分布： 导航类：查询某个已知存在的资源，页面 主页，考试资源等等 信息类：查询与某个主题相关的关键信息资源 获取相关信息，没有确定查询目标， 香港股市，，， 事务类：查找与完成某个特定任务相关的资源 垂直搜索引擎服务对象 如：xxx下载， 保证采样全面性：查询热门程度（冷热保证） 查询信息需求（2：5：3） 结果相关性标注 内容相关≠关键资源 相关性结果的共性要求： 结果提供的学习时新、真实可靠 结果的标题和摘要应当方便阅读并有效引导用户阅读 以信息需求类别为指导 Pooling 方法 文本语料=》查询样例集 》 手工相关性标注 准确率/召回率 准确率：找到的是否准确 召回率：找到的是否全面 信息检索强调序列的关系 搜索引擎性能指标设计 AP：平均准确率 搜索引擎用户行为的特殊性 前10个结果（85%） 搜索引擎用户信息需求的差异性 • 导航类信息需求的用户仅关注特定检索目标 • 信息类信息需求的用户关注全面而权威的信息 • 事务类信息需求的用户关注自己的任务是否可以顺利完成。 指标 适用 描述 RR 导航类 用户在找到搜索目标前需要浏览多少结果 P@N 信息类 结果列表中多大比例信息能够满足用户需求？ MAP 反映系统在全部相关文档上性能的单值指标。系统检索出来的相关文档越靠前(rank 越高)，MAP就应该越高 前n位成功率 事务类 用户是否能够利用给出的结果完成自己所关注的事务 首位相关结果倒数(Reciprocal Rank) \[RR = \frac{1}{Rank(1)}\] 适用于导航类型的查询信息需求（ 用户在找到搜索目标前需要浏览多少结果？ ） 前N位准确率(Precision@N) 适用于信息类型的查询信息需求（ 结果列表中多大比例信息能够满足用户需求？ ） 前N位成功率(Success@N) 适用于事务类型的查询信息需求（ 用户是否能够利用给出的结果完成自己所关注的事务？ ） 其他 NDCG@N: • Normalized Discounted Cumulative Gain • 对结果进行多级相关性标注时适用 • ERR: Expected Reciprocal Rank • Supported by Yahoo! and Google • 可以获取用户行为数据时适用 • B-pref: Binary preference • 相关性标注不完整时适用 P@N的计算方法 P@N本身是Precision@N的简称，指的是对特定的查询，考虑位置因素，检测前N条结果的准确率。例如对单次搜索的结果中前5篇，如果有4篇为相关文档，则P@5 = 4/5 = 0.8 。 测试通常会使用一个查询集合（按照前文所述方法构造），包含若干条不同的查询词，在实际使用P@N进行评估时，通常使用所有查询的P@N数据，计算算术平均值，用来评判该系统的整体搜索结果质量。 N的选取 对用户来说，通常只关注搜索结果最前若干条结果，因此通常搜索引擎的效果评估只关注前5、或者前3结果，所以我们常用的N取值为P@3或P@5等。 对一些特定类型的查询应用，如寻址类的查询（Navigational Search），由于目标结果极为明确，因此在评估时，会选择N=1（即使用P@1）。举个例子来说，搜索“新浪网”、或“新浪首页”，如果首条结果不是 新浪网（url：www.sina.com.cn），则直接判该次查询精度不满足需求，即P@1=0 MRR 平均倒数排序 MRR是平均排序倒数（Mean Reciprocal Rank）的简称，MRR方法主要用于寻址类检索（Navigational Search）或问答类检索（Question Answering），这些检索方法只需要一个相关文档，对召回率不敏感，而是更关注搜索引擎检索到的相关文档是否排在结果列表的前面。 MRR方法首先计算每一个查询的第一个相关文档位置的倒数，然后将所有倒数值求平均。 MRR 上述的P@N方法，易于计算和理解。但细心的读者一定会发现问题，就是在前N结果中，排序第1位和第N位的结果，对准确率的影响是一样的。但实际情况是，搜索引擎的评价是和排序位置极为相关的。即排第一的结果错误，和第10位的结果错误，其严重程度有天壤之别。因此在评价系统中，需要引入位置这个因素。 MRR是平均排序倒数（Mean Reciprocal Rank）的简称，MRR方法主要用于寻址类检索（Navigational Search）或问答类检索（Question Answering），这些检索方法只需要一个相关文档，对召回率不敏感，而是更关注搜索引擎检索到的相关文档是否排在结果列表的前面。MRR方法首先计算每一个查询的第一个相关文档位置的倒数，然后将所有倒数值求平均。例如一个包含三个查询词的测试集，前5结果分别为： 查询一结果：1.AN 2.AR 3.AN 4.AN 5.AR 查询二结果：1.AN 2.AR 3.AR 4.AR 5.AN 查询三结果：1.AR 2.AN 3.AN 4.AN 5.AR 其中AN表示不相关结果，AR表示相关结果。那么第一个查询的排序倒数（Reciprocal Rank）RR1 = 1/2=0.5 ；第二个结果RR2 = 1/2 = 0.5 ； 注意倒数的值不变，即使查询二获得的相关结果更多。同理，RR3= 1/1 = 1。 对于这个测试集合，最终MRR=（RR1+RR2+RR3）/ 3 = 0.67 然而对大部分检索应用来说，只有一条结果无法满足需求，对这种情况，需要更合适的方法来计算效果，其中最常用的是下述MAP方法。 MAP MAP方法是Mean Average Precison，即平均准确率法的简称。其定义是求每个相关文档检索出后的准确率的平均值（即Average Precision）的算术平均值（Mean）。这里对准确率求了两次平均，因此称为Mean Average Precision。（注：没叫Average Average Precision一是因为难听，二是因为无法区分两次平均的意义） MAP 是反映系统在全部相关文档上性能的单值指标。系统检索出来的相关文档越靠前(rank 越高)，MAP就应该越高。如果系统没有返回相关文档，则准确率默认为0。 MAP:全称mean average precision(平均准确率)。mAP是为解决P，R，F-measure的单点值局限性的，同时考虑了检索效果的排名情况。 计算如下： 假设有两个主题，主题1有4个相关网页，主题2有5个相关网页。某系统对于主题1检索出4个相关网页，其rank分别为1, 2, 4, 7；对于主题2检索出3个相关网页，其rank分别为1,3,5。对于主题1，平均准确率为(1/1+2/2+3/4+4/7)/4=0.83。对于主题 2，平均准确率为(1/1+2/3+3/5+0+0)/5=0.45。则MAP=(0.83+0.45)/2=0.64。”]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Hello Hexo]]></title>
      <url>%2Farticle%2Fhello-world.html</url>
      <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick Start Create a new post 1$ hexo new "My New Post" More info: Writing Run server 1$ hexo server More info: Server Generate static files 1$ hexo generate More info: Generating Deploy to remote sites 1$ hexo deploy 草稿 草稿相当于很多博客都有的“私密文章”功能。 1$ hexo new draft "new draft" 会在source/_drafts目录下生成一个new-draft.md文件。但是这个文件不被显示在页面上，链接也访问不到。也就是说如果你想把某一篇文章移除显示，又不舍得删除，可以把它移动到_drafts目录之中。 如果你希望强行预览草稿，更改配置文件： 1render_drafts: true 或者，如下方式启动server： 1$ hexo server --drafts 下面这条命令可以把草稿变成文章，或者页面： 1$ hexo publish [layout] &lt;filename&gt; More info: Deployment MathJax MathJax 常用命令 mathjax 常用 推荐 手写识别 修复公式不正常 在blog文件夹中执行： 1$ hexo math install 在_config.yml文件中添加： 12plugins:- hexo-math 使用 1Simple inline $a = b + c$. 效果： Simple inline \(a = b + c\). MathJax Block: 1234$$\frac&#123;\partial u&#125;&#123;\partial t&#125;= h^2 \left( \frac&#123;\partial^2 u&#125;&#123;\partial x^2&#125; +\frac&#123;\partial^2 u&#125;&#123;\partial y^2&#125; +\frac&#123;\partial^2 u&#125;&#123;\partial z^2&#125;\right)$$ 效果： \[\frac{\partial u}{\partial t} = h^2 \left( \frac{\partial^2 u}{\partial x^2} + \frac{\partial^2 u}{\partial y^2} + \frac{\partial^2 u}{\partial z^2}\right)\] 为Hexo Next博客添加关键词功能 修改文件：themes\next\layout_partials\head.swig 1234567&#123;% if page.keywords %&#125; &lt;meta name="keywords" content="&#123;&#123; page.keywords &#125;&#125;" /&gt;&#123;% elif page.tags and page.tags.length %&#125; &lt;meta name="keywords" content="&#123;% for tag in page.tags %&#125;&#123;&#123; tag.name &#125;&#125;,&#123;% endfor %&#125;" /&gt;&#123;% elif theme.keywords %&#125; &lt;meta name="keywords" content="&#123;&#123; theme.keywords &#125;&#125;" /&gt;&#123;% endif %&#125; 修改内容：35行左右，将原来的设置ketwords的代码覆盖即可 1234567&#123;% if page.keywords and page.keywords.length %&#125; &lt;meta name="keywords" content="&#123;% for key in page.keywords %&#125;&#123;&#123; key &#125;&#125;,&#123;% endfor %&#125;" /&gt;&#123;% elif page.tags and page.tags.length %&#125; &lt;meta name="keywords" content="&#123;% for tag in page.tags %&#125;&#123;&#123; tag.name &#125;&#125;,&#123;% endfor %&#125;" /&gt;&#123;% elif theme.keywords %&#125; &lt;meta name="keywords" content="&#123;&#123; theme.keywords &#125;&#125;" /&gt;&#123;% endif %&#125; hexo 中设置访问密码 在themes-&gt;next-&gt;layout-&gt;_partials-&gt;head.swig中的meta标签之后添加js代码 123456789101112131415161718192021222324252627282930313233&lt;script src=&quot;https://cdn.bootcss.com/js-sha1/0.4.1/sha1.js&quot;&gt;&lt;/script&gt;&lt;script src=&quot;http://cdn.bootcss.com/blueimp-md5/1.1.0/js/md5.js&quot;&gt;&lt;/script&gt; &lt;script type=&quot;text/javascript&quot;&gt; loopy() function loopy() &#123; if(&apos;&#123;&#123; page.password &#125;&#125;&apos;)&#123; while(true) &#123; var pass = prompt(&quot;输入正确密码才能查看!&quot;); if(pass) &#123; // 点击的确定 var ss = sha1(pass) // alert(ss); if(ss == &apos;&#123;&#123; page.password &#125;&#125;&apos;) &#123; alert(&quot;欢迎进入我的博客！Contact me at zhangshenghao1995@163.com&quot;); break; &#125; else &#123; alert(&quot;密码错误！&quot;); &#125; &#125; else if(pass == &quot;&quot;) &#123; // 用户没有输入 &#125; else &#123; // 点击取消 history.back() &#125; &#125; &#125;&#125;&lt;/script&gt; 安装扩展包 1sudo npm install hexo-math --save Pandoc 的问题 Error: spawn pandoc ENOENT 12345678910events.js:183 throw er; // Unhandled 'error' event ^Error: spawn pandoc ENOENT at _errnoException (util.js:1022:11) at Process.ChildProcess._handle.onexit (internal/child_process.js:190:19) at onErrorNT (internal/child_process.js:372:16) at _combinedTickCallback (internal/process/next_tick.js:138:11) at process._tickCallback (internal/process/next_tick.js:180:9) 解决方案: 安装 pandoc Error: -smart/-S removed 123456789Error: pandoc exited with code 2: --smart/-S has been removed. Use +smart or -smart extension instead.For example: pandoc -f markdown+smart -t markdown-smart.Try pandoc --help for more information. at ChildProcess.&lt;anonymous&gt; (/home/arxan/go_project/src/blog_hexo/node_modules/hexo-renderer-pandoc/index.js:73:20) at emitTwo (events.js:126:13) at ChildProcess.emit (events.js:214:7) at maybeClose (internal/child_process.js:925:16) at Process.ChildProcess._handle.onexit (internal/child_process.js:209:5)]]></content>
    </entry>

    
  
  
</search>
